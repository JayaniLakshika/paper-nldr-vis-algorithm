---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: https://www.michaellydeamore.com/
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-linear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data (\pD{}) by applying a non-linear transformation. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of \pD{} distributions. The NLDR methods and (hyper)parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the \pD{} data, we have developed an algorithm to show the \gD{} NLDR model in the \pD{} space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks. 
  
keywords: [high-dimensional data vizualization, non-linear dimension reduction, tour]
keywords-formatted: [high-dimensional data vizualization, non-linear dimension reduction, tour]

bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
# remotes::install_github("JayaniLakshika/quollr")
library(quollr)
library(tibble)
library(dplyr)
# remotes::install_github("jlmelville/snedata")
# library(snedata)
# library(ggflowchart)
#library(purrr) ## map function
#library(gridExtra) ## for grid.arrange
# library(rsample)
# library(DT)
#library(ggbeeswarm)
library(ggplot2)
library(readr)
library(tidyr)

# library(Rtsne)
# library(uwot)
# library(phateR)
library(patchwork)
library(png)
#library(langevitour)
library(colorspace)
library(kableExtra)
#library(grid)
library(conflicted)
conflicts_prefer(dplyr::filter)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label, position = c(0.92, 0.92), cex = 1, col="grey70") {
  annotation_custom(grid::textGrob(label = label,
      x = unit(position[1], "npc"), y = unit(position[2], "npc"),
      gp = grid::gpar(cex = cex, col=col)))
}
```

```{r}
#| label: scale-highd-data
# Center the data by subtracting the mean of each column
center_data <- function(data) {
  apply(data, 2, function(col) col - mean(col))
}

# Function to scale data manually
scale_data_manual <- function(data, type_col) {
  # Step 1: Center the data (mean 0)
  data_centered <- center_data(data |> select(-all_of(type_col)))
  
  # Step 2: Calculate the standard deviation of each dimension
  sds <- apply(data_centered, 2, sd)
  
  # Step 3: Scale each dimension to have the range [0, 1]
  data_scaled <- apply(data_centered, 2, function(col) col / max(abs(col)))
  
  # Step 4: Scale dimensions according to their variation
  # The dimension with the highest standard deviation is scaled to [-1, 1]
  # Other dimensions are scaled to smaller ranges based on their standard deviations
  max_sd <- max(sds)
  
  # Normalize the standard deviations to get scaling factors
  scaling_factors <- sds / max_sd
  
  for (i in seq_along(scaling_factors)) {
    data_scaled[, i] <- data_scaled[, i] * scaling_factors[i]
  }
  
  # Combine the scaled data with the 'type' column and return as a tibble
  data_scaled <- as_tibble(data_scaled) %>% 
    mutate(!!type_col := data[[type_col]])
  
  return(data_scaled)
}
```

```{r}
#| label: gen-axes

gen_axes <- function(proj, limits = 1, axis_pos_x = NULL, axis_pos_y = NULL, axis_labels, threshold) {
  
  axis_scale <- limits/6
  
  if (is.null(axis_pos_x)) {
    
    axis_pos_x <- -2/3 * limits
    
  }
  
  if (is.null(axis_pos_y)) {
    
    axis_pos_y <- -2/3 * limits
    
  }
  
  adj <- function(x, axis_pos) axis_pos + x * axis_scale
  axes <- data.frame(x1 = adj(0, axis_pos_x), 
                     y1 = adj(0, axis_pos_y), 
                     x2 = adj(proj[, 1], axis_pos_x), 
                     y2 = adj(proj[, 2], axis_pos_y))
  
  rownames(axes) <- axis_labels
  
  ## To remove axes
  axes <- axes |>
    mutate(distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)) |>
    filter(distance >= threshold)
  
  theta <- seq(0, 2 * pi, length = 50)
  circle <- data.frame(c1 = adj(cos(theta), axis_pos_x), 
                       c2 = adj(sin(theta), axis_pos_y))
  
  return(list(axes = axes, circle = circle))
  
}
```

```{r}
#| label: solve-quad-fun

quad <- function(a = 3, b = 2 * a2, c = -(a2^2 + a1^2))
{
  a <- as.complex(a)
  answer <- c((-b + sqrt(b^2 - 4 * a * c)) / (2 * a),
              (-b - sqrt(b^2 - 4 * a * c)) / (2 * a))
  if(all(Im(answer) == 0)) answer <- Re(answer)
  if(answer[1] == answer[2]) return(answer[1])
  answer[answer>0] ## only positive 
}

```

```{r}
#| label: code-setup
set.seed(20240110)
#source("nldr_code.R", local = TRUE)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{1.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data ($k < p$). Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2022], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
#| label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a")

nldr1c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
nldr2 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b")

nldr2c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("b")


umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c")

nldr3c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#f781bf') +
  interior_annotation("c")


#umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_15_min_dist_0.99.rds")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d")

nldr4c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("d")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e")

nldr5c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("e")

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f")

nldr6c <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("f")

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g")

nldr7c <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("g")

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h")

nldr8c <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("h")
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Eight different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 30), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The paper is organized as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD{}   representations of \pD{}   data have been computed using multidimensional scaling (MDS) [@borg2005], which includes principal components analysis (PCA) [@jolliffe2011] as a special case.  The \kD{}   representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{}   layout is constructed by minimizing a stress function that differences distances between points in \pD{}   with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}   minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is \gD{}, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in \gD{}    represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@cfa69] and multidimensional scaling (MDS) [@borg2005]. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)
<!--### Notation -->

<!-- @tbl-notation summarises the notation used to explain the new methodology. The observed data is denoted as $\mathbfit{x}_{n \times p}$ where $x_{ij}$ would indicate the $i^{th}$ observation on the $j^{th}$ variable sampled from a population $\mathbfit{X}$. To refer to variable $j$, we would use $X_j$.--> 

<!--
$X_{n \times p} = \begin{bmatrix} \textbf{x} _{1} & \textbf{x}_ {2} & \cdots & \textbf{x}_{n} \\  \end{bmatrix}^\top$

$Y_{n \times d} = \begin{bmatrix} \textbf{y} _{1} & \textbf{y}_ {2} & \cdots & \textbf{y}_{n} \\  \end{bmatrix}^\top$

$C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$

$C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$ $p$-D mappings of 2D hexagon bin centroids of the $k^{th}$ hexagon

-->

```{r}
#| label: tbl-notation
#| tbl-cap: "Summary of notation for describing new methodology."
# Notation used in the paper

notation_df <- read_csv("misc/notation.csv")

# Create the table
kable(notation_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("3cm", "12cm"))
```

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with (hyper-)parameters $\mathbfit{\theta}$. The parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. <!--Any input requirements for the data (such as normalization, or preprocessing through the use of PCA or similar) is considered part of the function $g$.-->

With our goal being to make a representation of this \gD{} layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbour information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. We recommend using a hexagon grid because it better reflects the data distribution and has less artifacts than a rectangular grid. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-two-curvy, and documented below.

```{r}
#| label: two-curvy-training
training_data_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_data.rds")
```

```{r}
#| label: two-curvy-true-proj-data

true_model_df <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_true_model.rds")
wireframe_true_model <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_true_model_connections.rds")

data_two_curvy <- training_data_two_curvy |>
  mutate(type = "data")

true_model_two_curvy <- true_model_df |> 
    select(-ID) |> 
  mutate(type = "true model")
```

<!--UMAP applied for Two curvy data-->
```{r}
#| label: umap-two-curvy
umap_two_curvy <- read_rds(file = "data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_n-neigbors_15_min-dist_0.1.rds") 

two_curvy_scaled_obj <- gen_scaled_data(
  data = umap_two_curvy)

umap_two_curvy_scaled <- two_curvy_scaled_obj$scaled_nldr |>
  mutate(ID = row_number())
lim1 <- two_curvy_scaled_obj$lim1
lim2 <- two_curvy_scaled_obj$lim2
r2 <- diff(lim2)/diff(lim1)

sc_ltr_pos <- c(0.08, 0.9)
# sc_xlims <- c(-0.5, 1.47)
# sc_ylims <- c(-0.32, 2.1)
sc_xlims <- c(-0.27, 1.2)
sc_ylims <- c(-0.2, 0.8)

nldr_two_curvy <- umap_two_curvy_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, colour="#636363", size = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

nldr_two_curvy2 <- umap_two_curvy_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, colour="#636363", size = 0.5) +
  interior_annotation("a", sc_ltr_pos)

nldr_two_curvy_original <- umap_two_curvy_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, color = "#636363", size = 0.5) +
  theme(aspect.ratio = 1) +
  #xlim(c(-0.01, 1.1)) + ylim(c(-0.08, 0.57)) +
  interior_annotation("a", sc_ltr_pos)

nldr_two_curvy_original2 <- umap_two_curvy_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, color = "#636363", size = 0.5) +
  theme(aspect.ratio = 1) +
  #xlim(c(-0.01, 1.1)) + ylim(c(-0.08, 0.57)) +
  interior_annotation("a1", sc_ltr_pos)
```

<!--Full hexagon grid with UMAP data-->

```{r}
#| label: hexbin-two-curvy
## Compute hexbin parameters
num_bins_x_two_curvy <- 12

## hexagon binning to have regular hexagons
hb_obj_two_curvy <- hex_binning(
  data = umap_two_curvy_scaled, 
  bin1 = num_bins_x_two_curvy, 
  r2 = r2,
  q = 0.1)

## Data set with all centroids
all_centroids_df <- hb_obj_two_curvy$centroids

a1_1 <- hb_obj_two_curvy$a1

## Generate all coordinates of hexagons
hex_grid <- hb_obj_two_curvy$hex_poly

## To obtain the standardise counts within hexbins
counts_df <- hb_obj_two_curvy$std_cts
df_bin_centroids_two_curvy <- extract_hexbin_centroids(
  centroids_df = all_centroids_df, 
  counts_df = counts_df) |>
  filter(drop_empty == FALSE)

umap_data_two_curvy_with_hb_id <- hb_obj_two_curvy$data_hb_id
df_all_two_curvy <- dplyr::bind_cols(training_data_two_curvy, umap_data_two_curvy_with_hb_id)
df_bin_two_curvy <- avg_highd_data(data = df_all_two_curvy, col_start = "x")

hex_grid_with_counts <- 
  left_join(hex_grid,
            counts_df, 
            by = c("hex_poly_id" = "hb_id"))

hex_grid_two_curvy <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(color = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = umap_two_curvy_scaled, 
             aes(x = UMAP1, y = UMAP2), 
             alpha = 0.5, size = 0.5, color = "#636363") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos)

hex_grid_coloured_two_curvy1 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts, 
    aes(x = x, y = y,
      group = hex_poly_id, 
      fill = std_counts), color = "grey70", linewidth=0.2) +
  geom_point(data = umap_two_curvy_scaled,
             aes(x = UMAP1, y = UMAP2),
             alpha = 0.3,
             size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "E") +
  interior_annotation("a", sc_ltr_pos) + 
  xlim(c(-0.3, 1.2)) + ylim(c(-0.17, 0.78)) 


```

<!--Non-empty bins with bin centroids-->

```{r}
#| label: empty-bin-two-curvy
hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids_two_curvy$hexID)

hex_grid_nonempty_two_curvy <- ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(color = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids_two_curvy, 
             aes(x = c_x, y = c_y), 
             color = "#000000",
             size = 1) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos) 
```

<!--2D model-->
```{r}
#| label: triangulate-two-curvy
## Triangulate bin centroids
tr1_object_two_curvy <- tri_bin_centroids(
  df_bin_centroids_two_curvy, x = "c_x", y = "c_y")
tr_from_to_df_two_curvy <- gen_edges(
  tri_object = tr1_object_two_curvy)

## Compute 2D distances
distance_two_curvy <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_two_curvy, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_two_curvy <- find_lg_benchmark(
  distance_edges = distance_two_curvy, 
  distance_col = "distance")

distance_df_small_edges_two_curvy <- distance_two_curvy |>
    filter(distance < benchmark_two_curvy)

# trimesh_removed_two_curvy <- vis_rmlg_mesh(
#   distance_edges = distance_two_curvy, 
#   benchmark_value = benchmark_two_curvy, 
#   tr_coord_df = tr_from_to_df_two_curvy, 
#   distance_col = "distance") +
#   xlim(sc_xlims) + ylim(sc_ylims) +
#   interior_annotation("d", sc_ltr_pos) 

tr_from_to_df_two_curvy <- inner_join(
  tr_from_to_df_two_curvy, distance_df_small_edges_two_curvy,
  by = c("from", "to"))

trimesh_removed_two_curvy <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  coord_equal() +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos)

trimesh_two_curvy_removed1_umap_with_data <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = umap_two_curvy_scaled,
             aes(
               x = UMAP1,
               y = UMAP2
             ),
             color = "#636363",
             alpha = 0.5,
             size = 0.5) +
  #coord_equal() +
  # theme(
  #   aspect.ratio = 1
  #   ) +
  interior_annotation("b", sc_ltr_pos)
```


```{r}
#| label: best-fit-two-curvy-proj-umap1

df_bin_two_curvy_temp <- df_bin_two_curvy

df_bin_two_curvy <- df_bin_two_curvy |>
  select(-hb_id) |>
  mutate(type = "model")

# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy, df_bin_two_curvy)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_true_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "model") |>
  select(-type)


## First projection
projection <- cbind(
  c(-0.08707,-0.06493,0.07017,-0.07349,-0.02955,-0.05286,-0.02765),
  c(0.03398,0.03735,0.09272,0.01871,-0.09224,0.00451,0.08079))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number()) 

## For fitted model
projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_two_curvy |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## For true model

projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% projection_scaled

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")


axes_obj <- gen_axes(  
  proj = projection * 5,
  limits = 0.15,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.01)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_umap_first_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("c", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_umap_all_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = true_model_df_proj, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("d", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

```


To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "S-curve". It is constructed by simulating $n=750$ observations from $\theta \sim U(-3\pi/2, 3\pi/2)$, $X_1 = \sin(\theta)$, $X_2 \sim U(0, 2)$ (adding thickness to the S), $X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)$. The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance. We would consider $T=(X_1, X_2, X_3)$ to be the geometric structure (true model) that we hope to capture.

```{r}
#| label: two-curvy-true-proj
# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

## First projection
projection <- cbind(
    c(-0.08707,-0.06493,0.07017,-0.07349,-0.02955,-0.05286,-0.02765),
    c(0.03398,0.03735,0.09272,0.01871,-0.09224,0.00451,0.08079))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number()) 

projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  wireframe_true_model, 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection * 5,
  limits = 0.15,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.01)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("a", c(0.08, 0.93), cex = 2) +
  theme(
    legend.position = "none"
  )

proj_plot_two_curvy_true <- model_df |>
  ggplot(
    aes(
      x = proj1_from, 
      y = proj2_from)) +
  geom_point(
    color = "#000000",
    alpha = 0.5) +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("a", c(0.08, 0.93), cex = 2) +
  theme(
    legend.position = "none"
  )

## Second projection
projection <- cbind(
    c(-0.02704,0.08281,-0.07671,-0.02996,0.08674,0.06497,-0.02090),
    c(0.00095,0.08031,0.05721,0.10299,0.04816,-0.05837,-0.02226))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number()) 

projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  wireframe_true_model, 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection * 6,
  limits = 0.13,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.0113)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.125, 0.13)) + 
  ylim(c(-0.125, 0.13)) +
  interior_annotation("b", c(0.08, 0.93), cex = 2) +
  theme(
    legend.position = "none"
  )
```

```{r}
#| label: fig-two-curvy-true-proj
#| fig-cap: "Two views of the true model (blue lines) in $2\\text{-}D$ projections from $7\\text{-}D$, for the two C-shaped clusters data (grey points). In one C-shaped cluster, the data is spread throughout the cluster, while in the other C-shaped cluster, the data is concentrated more in one corner of the cluster. The **langevitour** software is used to view the data with a tour, and the full video is available at <>." 
#| fig-pos: H
#| fig-width: 10
#| fig-height: 5

two_curvy_proj_umap_model1 + two_curvy_proj_umap_model2 + 
  plot_layout(ncol = 2)
```

```{r}
#| label: fig-NLDR-two-curvy
#| echo: false
#| fig-cap: "Key steps for constructing the model on the UMAP layout ($k=2$): (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids. The two C-shaped clusters data is shown."
#| fig-width: 12
#| fig-height: 4
#| out-width: 100%
 
nldr_two_curvy + hex_grid_two_curvy + 
  hex_grid_nonempty_two_curvy + 
  trimesh_removed_two_curvy +
  plot_layout(ncol = 4)
```



<!--langevitour with true model for S-curve--> 
<!-- ::: {#fig-two_curvy-true-sc layout-ncol="3" fig-pos="H"} -->

<!-- ![](figures/two_curvy/sc_true_1.png){width="150" fig-align="center"} -->

<!-- ![](figures/two_curvy/sc_true_2.png){width="150" fig-align="center"} -->

<!-- ![](figures/two_curvy/sc_true_3.png){width="150" fig-align="center"} -->

<!-- Three views of the true model (grey points and lines) in $2\text{-}D$ projections from $7\text{-}D$, for the S-curve data (purple points). The data is spread along the S shape, and does not vary much from this curve. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/I5GL23vLiw0>).  -->
<!-- ::: -->

<!-- XXX The points on the "theoretical model" should be equidistant along the S. They are still to bunched in some spots and too far apart in others. -->

### Algorithm to represent the model in \gD{}  

#### Scale the data

Because we are working with distances between points, starting with data having a standard scale, e.g. [0, 1], is recommended. The default should take the aspect ratio produced by the NLDR $(r_1, r_2, ..., r_k)$ into account. When $k=2$, as in hexagon binning, the default range is $[0, y_{i,\text{max}}], i=1,2$, where $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = \frac{r_2}{r_1}$ (@fig-NLDR-two-curvy). If the NLDR aspect ratio is ignored then set $y_ {2,\text{max}} = 1$. <!-- \times \frac{2}{\sqrt{3}}$. (The $\frac{2}{\sqrt{3}}$ accounts for the different height ($a_1$) and width ($a_2$) of a regular hexagon.) The scaling of data should take the size of the hexagons into account, but choice of number of bins should. -->

#### Computing hexagon grid configuration

Although there are several implementations of hexagon binning [@carr1987], and a published paper [@dan2023], surprisingly, none has sufficient detail or components that produce everything needed for this project. So we described the process used here. @fig-hex-param illustrates the notation used. 

The \gD{} hexagon grid is defined by its bin centroids. Each hexagon, $H_h$ ($h = 1, \dots, b$) is uniquely described by centroid, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. The number of bins in each direction is denoted as $(b_1, b_2)$, with  $b = b_1 \times b_2$ being the total number of bins. We expect the user to provide just $b_1$ and we calculate $b_2$ using the NLDR ratio, to compute the grid. 

To ensure that the grid covers the range of data values a buffer parameter ($q$) is set as a proportion of the range. By default,  $q=0.1$. The buffer should be extending a full hexagon width ($a_1$) and height ($a_2$) beyond the data, in all directions. The lower left position where the grid starts is defined as $(s_1, s_2)$, and corresponds to the centroid of the lowest left hexagon, $C_{1}^{(2)} = (c_{11}, c_{12})$. This must be smaller than the minimum data value. Because it is one buffer unit, $q$ below the minimum data values, $s_1 = -q$ and $s_2 = -qr_2$. 

The value for $b_2$ is computed by fixing $b_1$. Considering the upper bound of the first NLDR component, $a_1 > \frac{1+2q}{b_1 -1}$. Similarly, for the second NLDR component, $a_2 > \frac{r_2 + q(1 + r_2)}{(b_2 - 1)}$. Since $a_2 = \frac{\sqrt(3)}{2}a_1$ for regular hexagons, $a_1 > \frac{2[r_2 + q(1 + r_2)]}{\sqrt{3}(b_2 - 1)}$. This is a linear optimization problem. Therefore, the optimal solution must occur on a vertex. Therefore, $b_2 = \Big\lceil1 +\frac{2[r_2 + q(1 + r_2)](b_1 - 1)}{\sqrt{3}(1 + 2q)}\Big\rceil$.

```{r}
#| label: code-illustration
# Code to draw illustration for notation
## hexagon binning to have regular hexagons
hb_obj_notation <- hex_binning(
  data = umap_two_curvy_scaled, 
  bin1 = 7, 
  r2 = 1.6,
  q = 0.1)

a1_temp <- hb_obj_notation$a1
a2_temp <- hb_obj_notation$a2
l_temp <- quad(a=3, b = 2 * a2_temp, c = -(a2_temp^2 + a1_temp^2))

## Data set with all centroids
all_centroids_df_temp <- hb_obj_notation$centroids
hex_grid_temp <- hb_obj_notation$hex_poly

hex_grid_temp45 <- hex_grid_temp |> 
  filter(hex_poly_id == 45)

start_pt <- all_centroids_df_temp |> 
  filter(hexID == 1)
d_rect <- tibble(x1min = 0, 
                 x1max = 1,
                 x2min = 0,
                 x2max = 1.6)

# To move the rectangle to ignore the overlap with the centroids
# rect_adj <- tibble(x1 = 0.03, x2 = 0.03)
rect_adj <- tibble(x1 = -0.03, x2 = 0.03)


a1 <- tibble(x = all_centroids_df_temp$c_x[4],
             xend = all_centroids_df_temp$c_x[5],
             y = all_centroids_df_temp$c_y[21],
             yend = all_centroids_df_temp$c_y[21],
             label = expression(a[1]))
a2 <- tibble(x = all_centroids_df_temp$c_x[25],
             xend = all_centroids_df_temp$c_x[25],
             y = all_centroids_df_temp$c_y[25],
             yend = all_centroids_df_temp$c_y[33],
             label = expression(a[2]))
l <- tibble(x = hex_grid_temp45$x[2],
            xend = hex_grid_temp45$x[3],
            y = hex_grid_temp45$y[2],
            yend = hex_grid_temp45$y[3],
            label = expression(l))

hex_param_vis <- ggplot() + 
    geom_polygon(data = hex_grid_temp, 
                        aes(x = x, 
                            y = y, 
                            group = hex_poly_id),
                 fill = "white", 
                 color = "#bdbdbd") +
    geom_point(data = all_centroids_df_temp, aes(
      x = c_x, 
      y = c_y), 
      color = "#31a354", size = 0.9) +
    geom_point(data = start_pt, aes(x = c_x, 
                                    y = c_y), 
               color = "black") + 
    geom_rect(data=d_rect, 
              aes(xmin = x1min - rect_adj$x1,# - rect_adj$s1, 
                  xmax = x1max - rect_adj$x1,# - rect_adj$s1, 
                  ymin = x2min - rect_adj$x2,# - rect_adj$s2, 
                  ymax = x2max - rect_adj$x2),# - rect_adj$s2), 
              fill = "white", 
              color = "black", 
              alpha = 0, 
              linewidth = 0.7) +
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1max - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2max - rect_adj$x2)) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,0)", 
             hjust=-0.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1max - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,1)", 
             hjust=1.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2max - rect_adj$x2,
                     label = expression(group("(", 
                        list(0, y[2][max]),")")), 
            hjust=-0.1, vjust=1.2) + 
    geom_segment(data=d_rect, aes(
      x = x1min  - rect_adj$x1, # 0 - 0.03, 
      y = -0.31, 
      xend = x1max - rect_adj$x1, #1 - 0.03, 
      yend = -0.31), #-0.35),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
                 color = "black")+
    annotate("text", x=0.5, y=-0.36, 
             label = expression(r[1]), color = "black") +
    geom_segment(data=d_rect, aes(
      x = -0.25, 
      y = x2min - rect_adj$x2, #0 - 0.05, 
      xend = -0.25, 
      yend = x2max - rect_adj$x2), #r2 - 0.05),
      arrow = arrow(length = unit(0.03, "npc"),
                       ends = "both"), 
                 color = "black")+ 
    annotate("text", x=-0.3, y=0.4, 
             label = expression(r[2]), color = "black") +
    geom_segment(data = a1, aes(
      x = x, #-0.1 + 0.2087578, 
      y = y, #-0.15, 
      xend = xend, #-0.1 + 0.2087578*2, 
      yend = yend), #-0.15),
      arrow = arrow(length = unit(0.03, "npc"),
        ends = "both"), 
        color = "black")+ # a1 = 0.2087578
    annotate("text", 
             x=(a1$x+a1$xend)/2, 
             y=a1$y, 
             label = expression(a[1]), 
             color = "black",
             vjust = 1.2) +
    geom_segment(data = a2, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + # a2 = 0.1807896
    annotate("text", x=a2$x, y=(a2$y+a2$yend)/2, 
             label = expression(a[2]), 
             color = "black", hjust=-0.2) +
    annotate("text", x=-0.18, y=-0.24, 
      label = expression(group("(", list(s[1], s[2]), ")")),
      color = "black") +
  geom_segment(data = l, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + 
    annotate("text", x=l$x + 0.03, y=(l$y+l$yend)/2, 
             label = expression(l), 
             color = "black", hjust=-0.2) +
  coord_equal()
```

```{r}
#| label: fig-hex-param
#| fig-cap: "The components of the hexagon grid illustrating notation."
#| out-height: 30%
#| fig-pos: H
 
hex_param_vis
```

<!-- Number of bins is set by fixing $b_1$, which determines the binwidth accounting the offset $q_1$ and breaks on $x_1$, is calculated as  -->

<!-- $$ -->
<!-- a_1 = \frac{r_1 + q_1}{b_1}. -->
<!-- $$ -->

<!-- The computed binwidth then determines the number of vertical bins, $b_2$, and vertical binwidth, which are computed based on $r_2$, the offset $q_2$, and the height/width ratio of a regular hexagon, $\frac{2}{\sqrt{3}}$ as -->

<!-- $$ -->
<!-- a_2 = \frac{r_2 + q_2}{0.75 \times b_2}. -->
<!-- $$ -->

<!-- To define a hexagon grid across the \kD{} space, with hexagons of fixed height ($a_1$), width ($a_2$),  it is necessary to determine how many hexagons should be represented along each axis in the \kD{} space.

When $k=2$, the number of bins along the $x$ and $y$ axes, $b_1$ and $b_2$, is computed by considering the scaling factors ${r_1, r_2}$, along with the offset along the axes ($q_1$, $q_2$), and the height ($a_1$) and width ($a_2$) of a regular hexagon (@fig-NLDR-two-curvy (b)). ($b_1 = \frac{r_1 + q_1}{a_2}$, and $b_2 = \frac{r_2 + q_2}{0.75 \times a_1}$) where $0.75 \times a_1$ accommodate to have hexagons fill the entire 2D space without leaving any gaps between them.)-->

<!-- XXX We don't need parameters for the regular hexagon, these are fixed constants. -->


#### Binning the data

<!-- Points are allocated to the bin they fall into based on the nearest centroid. In situations where a point is equidistant from multiple centroids, tie-breaking rules are applied. If multiple centroids are in the same row, the point is assigned to the leftmost centroid. If multiple centroids are in different rows, the point is assigned to the bottom centroid. -->

<!-- $\{ i \in H_h, h = 1, \dots, b, \text{ and } i = 1, \dots, n\}$ -->

Observations are grouped into bins based on their nearest centroid. This produces a reduction in size of the data from $n$ to $m$, where $m\leq b$ (total number of bins). This can be defined using the function $u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}$, where
$u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2}$, mapping observation $i$ into $H_h = \{i| u(i) = h\}$. 

By default, the bin centroid is used for describing a hexagon (as done in @fig-NLDR-two-curvy (c)), but any measure of center, such as a mean or weighted mean of the points within each hexagon, could be used. The bin centers, and the binned data, are the two important components needed to render the model representation in high dimensions.  

<!-- XXX How are you doing this? Do you check the bounds of the hexagon? Or do you use distance to centroid? -->

<!--
Define a hexagon grid across the \kD{} space, using hexagons with fixed height ($a_2$), width ($a_1$). Each of the \kD{} points will belong to a hexagon bin. That is, for each $y \in \mathbfit{Y}$, we can (uniquely) identify the hexagon that the point belongs to. This identification is done finding the nearest bin centroid for the \kD{} points by considering the \kD{} Euclidean distance.    

When $k=2$, the starting coordinates $(s_1, s_2)$ mark the lower left of the grid. This is the bottom left bin centroid. By starting from there, points are generated to fill the grid accounting $b_1$, $b_2$, $a_1$, and $a_2$. 
-->

<!--We deliberately separate out the creation of the hexagon grid from the mapping of points on the grid.-->

#### Indicating neighborhood

Delaunay triangulation [@lee1980;@alb2024] is used to connect points so that edges indicate neighbouring observations, in both the NLDR layout (@fig-NLDR-two-curvy (d)) and the \pD{} model representation. When the data has been binned the triangulation connectd centroids. The edges preserve the neighborhood information when the model is lifted into \pD{}. 

<!-- When $k = 2$ Delaunay triangulation on $C^{(2)}$ generates the model in \gD{} space, which is a triangular mesh (@fig-NLDR-two-curvy (d)). It generates convex hulls of $C^{(2)}$ such that the circumcircle of every triangle in the triangulation contains no other points from $C^{(2)}$. -->

When shapes are non-linear in the NLDR layout, some edges could be long. It can also happen that distant centroids can be connected, particularly if clustering is present, which can result in long line segments. In order to generate a smooth surface in \gD{}, these long line segments should be removed when tuning the model fit.

<!--need to add what is meant by a long edge-->

### Rendering the model in \pD{}

The last step is to lift the \kD{} model into \pD{} by computing \pD{} vectors that represent bin centroids. We use the \pD{} mean of the points in $H_h$ to map the centroid $C_{h}^{(2)} = (c_{h1}, c_{h2})$ to a point in \pD{}. Let the \pD{} mean be

$$C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.$$
Furthermore, line segments that exist in the \kD{} model generate line segments in \pD{} by connecting the \pD{} means of the corresponding \kD{} bin centroids. If additional long edges need to be removed, compute the edges in \pD{} and pruned any detected long edges to improve the accuracy. Once pruned, re-plot the \gD{} view to ensure it accurately captures the data.

```{r}
#| label: hexbin-regular-two-curvy2
## hexagon binning to have regular hexagons
hb_obj_two_curvy2 <- hex_binning(
  data = umap_two_curvy_scaled, 
  bin1 = 13, 
  r2 = r2)

a1_2 <- calc_bins_y(
  bin1 = 13, 
  r2 = r2
  )$a1

## Data set with all centroids
all_centroids_df2 <- hb_obj_two_curvy2$centroids

## Generate all coordinates of hexagons
hex_grid2 <- hb_obj_two_curvy2$hex_poly

## To obtain the standardise counts within hexbins
counts_df2 <- hb_obj_two_curvy2$std_cts
df_bin_centroids2 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df2, 
  counts_df = counts_df2) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 11")

hex_grid_with_counts_two_curvy2 <- full_join(
  hex_grid2, 
  df_bin_centroids2 |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID")) 

umap_data_two_curvy2_with_hb_id <- hb_obj_two_curvy2$data_hb_id
df_all_two_curvy2 <- dplyr::bind_cols(training_data_two_curvy, umap_data_two_curvy2_with_hb_id)
df_bin_two_curvy2 <- avg_highd_data(data = df_all_two_curvy2, col_start = "x")

### Compute highD distance
dist_vec <- proxy::dist(x = df_bin_two_curvy2[, -1], method = "Euclidean") |> as.vector()

from_vec <- c()
to_vec <- c()
num_obs <- 1:(NROW(df_bin_two_curvy2) - 1)

for (obs in num_obs) {

  from_val <- rep(obs, (NROW(df_bin_two_curvy2) - obs))
  if ((obs + 1) <= NROW(df_bin_two_curvy2)) {
    to_val <- (obs + 1):NROW(df_bin_two_curvy2)
  }
  from_vec <- append(from_vec, from_val)
  to_vec <- append(to_vec, to_val)

}

dist_highd <- tibble::tibble(from = from_vec, to = to_vec, dist_highd = dist_vec)


df_b_two_curvy2 <- df_bin_two_curvy2 |>
  dplyr::filter(hb_id %in% df_bin_centroids2$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_two_curvy2 <- df_b_two_curvy2[match(df_bin_centroids2$hexID, df_b_two_curvy2$hb_id),] |>
  dplyr::select(-hb_id) 

hex_grid_coloured_two_curvy2 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_two_curvy2, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", 
        linewidth=0.2) +
  geom_point(data = umap_two_curvy_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  xlim(c(-0.3, 1.2)) + ylim(c(-0.17, 0.78)) +
  interior_annotation("b", sc_ltr_pos)

## Triangulate bin centroids
tr1_object_two_curvy2 <- tri_bin_centroids(
  df_bin_centroids2, x = "c_x", y = "c_y")
tr_from_to_df_two_curvy2 <- gen_edges(
  tri_object = tr1_object_two_curvy2)

trimesh_two_curvy2 <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy2,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = umap_two_curvy_scaled,
             aes(
               x = UMAP1,
               y = UMAP2
             ),
             color = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  interior_annotation("a1", sc_ltr_pos)

## Compute 2D distances
distance_two_curvy <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_two_curvy2, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To plot the distribution of distances

# Define bin width and starting point
bin_width <- hb_obj_two_curvy2$a1
start_point <- bin_width/2

# Create bins and calculate the mean value for each bin range
bins <- seq(start_point, max(distance_two_curvy$distance) + bin_width, 
            by = bin_width)
bin_labels <- bins[-length(bins)] + bin_width / 2  # mean of each bin range

distance_two_curvy_dist <- distance_two_curvy |> 
  mutate(bin = cut(distance, breaks = bins, include.lowest = TRUE, labels = bin_labels)) 

distance_two_curvy_dist <- left_join(distance_two_curvy_dist, dist_highd, by = c("from", "to"))

distance_points <- ggplot(
  distance_two_curvy_dist, 
  aes(x = distance,
      y = dist_highd)) +
  geom_point(alpha = 0.5) +
  #geom_jitter() +
  # scale_x_continuous(breaks = sort(unique(as.numeric(distance_two_curvy_dist$bin))),
  #                    labels = c(
  #                      expression(a[1]),
  #                      expression(2*a[1]),
  #                      expression(3*a[1]),
  #                      expression(4*a[1]),
  #                      expression(5*a[1]),
  #                      expression(6*a[1]),
  #                      expression(7*a[1]),
  #                      expression(8*a[1])
  #                    )) +
  # geom_vline(xintercept = 2,
  #            linetype="solid",
  #            color = "#bdbdbd",
  #            linewidth=1) +
  # geom_vline(xintercept = 4.5,
  #            linetype="solid",
  #            color = "#bdbdbd",
  #            linewidth=1) +
  ylab(expression(d^{(7)})) +
  xlab(expression(d^{(2)})) +
  #ggtitle("(a)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        panel.grid.major.x = element_blank(),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

distance_two_curvy_dist_summary <- distance_two_curvy_dist |>
  count(bin) |>
  mutate(prop = n/sum(n)) |>
  head(4)

# distance_two_curvy_dist <- distance_two_curvy_dist |>
#   filter(bin != unique(distance_two_curvy_dist$bin)[1]) ## To remove a1 distances, since always small

text_df <- tibble(
  x = c(1.9, 3.4),
  y = c(0.5, 0.5),
  text = c("e1", "d1")
)

distance_hist <- ggplot(
  distance_two_curvy_dist_summary,
  aes(x = as.numeric(bin),
      y = prop)) +
  geom_line() +
  scale_x_continuous(breaks = sort(unique(as.numeric(distance_two_curvy_dist$bin)))[1:4],
                     labels = c(
                       expression(a[1]),
                       expression(2*a[1]),
                       expression(3*a[1]),
                       expression(4*a[1])
                     )) +
  geom_vline(xintercept = 2,
             linetype="solid",
             color = "#bdbdbd",
             linewidth=1) +
  geom_vline(xintercept = 3.5,
             linetype="solid",
             color = "#bdbdbd",
             linewidth=1) +
  geom_text(
    data = text_df,
    aes(
      x = x,
      y = y,
      label = text
    ),
    size = 9
  ) +
  ylab("Proportion of the number of edges") +
  xlab(expression(d^{(2)})) +
  ggtitle("(b)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        panel.grid.major.x = element_blank(),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())


# distance_hist <- ggplot(
#   distance_two_curvy_dist, aes(x = distance)) +
#     geom_histogram(
#       aes(y = after_stat(count) / sum(after_stat(count))), 
#       binwidth = bin_width) +
#   scale_x_continuous(breaks = bin_labels, 
#                      labels = c(
#                        expression(a[1]),
#                        expression(2*a[1]),
#                        expression(3*a[1]),
#                        expression(4*a[1]),
#                        expression(5*a[1]),
#                        expression(6*a[1]),
#                        expression(7*a[1]),
#                        expression(8*a[1])
#                      )) +
#   geom_vline(xintercept = start_point + 2*bin_width,
#              linetype="solid",
#              color = "#bdbdbd",
#              linewidth=1) +
#   ylab("Proportion of the number of edges") +
#   xlab(expression(d^{(2)})) +
#   #ggtitle("(d)") +
#   theme_minimal() +
#   theme(aspect.ratio = 0.75,
#         panel.border = element_rect(fill = 'transparent'),
#         plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
#         panel.grid.major.x = element_blank(),
#         axis.ticks.x = element_line(),
#         axis.ticks.y = element_line())


## To find the benchmark value
benchmark_two_curvy <- find_lg_benchmark(
  distance_edges = distance_two_curvy, 
  distance_col = "distance")

## Benchmark 1

benchmark_two_curvy <- 2 * bin_width

distance_df_small_edges_two_curvy2 <- distance_two_curvy |>
  filter(distance < benchmark_two_curvy) ## 0.231

tr_from_to_df_two_curvy3 <- inner_join(
  tr_from_to_df_two_curvy2, distance_df_small_edges_two_curvy2,
  by = c("from", "to"))

trimesh_two_curvy_removed1 <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy3,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000")+
  geom_point(data = umap_two_curvy_scaled,
             aes(
               x = UMAP1,
               y = UMAP2
             ),
             color = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  interior_annotation("c1", sc_ltr_pos)

trimesh_two_curvy_removed1_with_data <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy3,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = umap_two_curvy_scaled,
             aes(
               x = UMAP1,
               y = UMAP2
             ),
             color = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  #xlim(c(-0.01, 1.1)) + ylim(c(-0.08, 0.57)) +
  interior_annotation("a2", sc_ltr_pos)



## Benchmark 2

benchmark_two_curvy <- 3.5 * bin_width

distance_df_small_edges_two_curvy3 <- distance_two_curvy |>
  filter(distance < benchmark_two_curvy)

tr_from_to_df_two_curvy4 <- inner_join(
  tr_from_to_df_two_curvy2, distance_df_small_edges_two_curvy3,
  by = c("from", "to"))

trimesh_two_curvy_removed2 <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy4,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000")+
  geom_point(data = umap_two_curvy_scaled,
             aes(
               x = UMAP1,
               y = UMAP2
             ),
             color = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  interior_annotation("b1", sc_ltr_pos)

```

```{r}
#| label: best-fit-two-curvy-proj

# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy, df_b_two_curvy2)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_true_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "model") |>
  select(-type)


## First projection
projection <- cbind(
  c(-0.08707,-0.06493,0.07017,-0.07349,-0.02955,-0.05286,-0.02765),
  c(0.03398,0.03735,0.09272,0.01871,-0.09224,0.00451,0.08079))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number()) 

## For fitted model
projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_two_curvy2 |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## Projection with Delauany triangulation

model_df1 <- dplyr::left_join(
  distance_two_curvy |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df1)[3:NCOL(model_df1)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df1 <- dplyr::left_join(model_df1, projected_model_df, by = c("to" = "ID"))
names(model_df1)[(2 + NCOL(projected_model_df)):NCOL(model_df1)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## With benchmark option 2

model_df2 <- dplyr::left_join(
  distance_df_small_edges_two_curvy3 |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df2)[3:NCOL(model_df2)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df2 <- dplyr::left_join(model_df2, projected_model_df, by = c("to" = "ID"))
names(model_df2)[(2 + NCOL(projected_model_df)):NCOL(model_df2)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## For true model

projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% projection_scaled

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")


axes_obj <- gen_axes(  
  proj = projection * 5,
  limits = 0.15,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.01)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_umap_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("c2", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("a3", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )


two_curvy_proj_umap_model_delaunay <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df1, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("a2", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_umap_model_benchmark2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df2, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("b2", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_umap_all_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = true_model_df_proj, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#636363") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("a4", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

```

```{r}
#| label: hexbin-regular-two-curvy3
## hexagon binning to have regular hexagons
hb_obj_two_curvy3 <- hex_binning(
  data = umap_two_curvy_scaled, 
  bin1 = 23, 
  r2 = r2)

a1_3 <- calc_bins_y(
  bin1 = 23, 
  r2 = r2
)$a1

## Data set with all centroids
all_centroids_df3 <- hb_obj_two_curvy3$centroids

## Generate all coordinates of hexagons
hex_grid3 <- hb_obj_two_curvy3$hex_poly

## To obtain the standardise counts within hexbins
counts_df3 <- hb_obj_two_curvy3$std_cts
df_bin_centroids3 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df3, 
  counts_df = counts_df3) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 14")

hex_grid_with_counts_s_curve3 <- full_join(hex_grid3, 
                                          df_bin_centroids3 |> select(hexID, std_counts), 
                                          by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_two_curvy3 <-  ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve3, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", linewidth=0.2) +
  geom_point(data = umap_two_curvy_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "D") +
  xlim(c(-0.3, 1.2)) + ylim(c(-0.17, 0.78)) +
  interior_annotation("c", sc_ltr_pos)
```


```{r}
#| label: model-prediction-umap-quollr

true_pred_df <- predict_emb(
  test_data = true_model_df,
  df_bin_centroids = df_bin_centroids_two_curvy,
  df_bin = df_bin_two_curvy_temp,
  type_NLDR = "UMAP"
)


# Compute radius r
a1 <- calc_bins_y(
  bin1 = 10, 
  r2 = r2, 
  q = 0.1)$a1

r <- a1/2 

# Function to jitter points within a circumcircle of a hexagon
jitter_within_circumcircle <- function(center_x, center_y, radius, num_points) {
  theta <- runif(num_points, 0, 2 * pi)
  r <- radius * sqrt(runif(num_points))
  x <- center_x + r * cos(theta)
  y <- center_y + r * sin(theta)
  jittered_points <- data.frame(UMAP1 = x, UMAP2 = y, ID = points_in_hex$ID)
  return(jittered_points)
}

# Jittered points data frame
jittered_points_df <- data.frame()

# Iterate through each hexagon and jitter points within the circumcircle
for (hex_id in unique(true_pred_df$pred_hb_id)) {
  # hex_points <- hex_grid_nonempty %>% filter(hex_poly_id == hex_id)
  # center_x <- mean(hex_points$x)
  # center_y <- mean(hex_points$y)
  # points_in_hex <- true_pred_df %>% filter(pred_UMAP_1 >= min(hex_points$x) & pred_UMAP_1 <= max(hex_points$x) & pred_UMAP_2 >= min(hex_points$y) & pred_UMAP_2 <= max(hex_points$y))
  
  points_in_hex <- true_pred_df |>
    filter(pred_hb_id == hex_id)
  
  center_x <- points_in_hex |> 
    pull(pred_UMAP_1) |> 
    unique()
  
  center_y <- points_in_hex |> 
    pull(pred_UMAP_2) |> 
    unique()

  if (nrow(points_in_hex) > 0) {
    jittered_points <- jitter_within_circumcircle(center_x, center_y, r, nrow(points_in_hex))
    jittered_points_df <- rbind(jittered_points_df, jittered_points)
  }
}

plot_predict_umap_two_curvy <- ggplot(
  data = true_pred_df, 
  aes(x = pred_UMAP_1,
      y = pred_UMAP_2)) +
  geom_point(alpha = 0.5) +
  interior_annotation("c", c(0.08, 0.93), cex = 2)
```

<!-- XXX You need to have consistent colouring. In langevitour the points corresponding to observations are purple. They should be purple in the \gD{} view too.  -->

<!-- XXX Should there be a second long edge removal? You can compute the edges in \pD{} now, and if there are long edges maybe these should be pruned, and we re-plot the \gD{} view.  -->

<!-- langevitour with pD model for two-curvy clusters -->
```{r}
#| label: best-fit-umap1
#| fig-cap: "Model in \\gD{} ($a_1 = 0.12$), on the layout and two views of the fit in projections from $7\\text{-}D$, for the two C-shaped clusters data ($n =  2000$ and $p = 7$). One view shows the fitted model, and the other shows the fitted and true models alongside the data. The fitted model accurately represents the shape of the clusters but does not capture the edges of the structure. This illustrates a characteristic of UMAP, which compresses the data when transforming the data into \\gD{}. Video of the langevitour animations is available at <>."
#| fig-width: 10
#| fig-height: 8

nldr_two_curvy2 + 
  trimesh_two_curvy_removed1_umap_with_data +
  two_curvy_proj_umap_first_model1 +
  two_curvy_proj_umap_all_model1 +
  plot_layout(ncol = 2, heights = c(1, 2))
```


### Measuring the fit {#sec-summary}
 <!-- Fitted values,  Error calculation-->

The model here is similar to a confirmatory factor analysis model [@brown2015], $\widehat{T}(X_1, X_2, X_3) + \Epsilon$. The difference between the fitted model and observed values would be considered to be residuals, and for this problem are $7\text{-}D$. 

<!--#### Fitted values-->

Observations are associated with their bin center, $C_{h}^{(p)}$, which are also considered to be the *fitted values*. These can also be denoted as $\widehat{X}$. <!--The fitted values of the points in $H_h$ refers to the \pD{} mapping $C_{h}^{(p)}$ of the corresponding \kD{} model point $C_{h}^{(2)}$.-->

<!--#### Error-->

The error is computed by taking the squared \pD{} Euclidean distance, corresponding to computing the mean squared error (MSE) as:

$$\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2$${#eq-equation1} 

where $n$ is the number of observations, $b$ is the number of bins, $n_h$ is the number of observations in $h^{th}$ bin, $p$ is the number of variables, $\mathbfit{x}_{hij}$ is the $j^{th}$ dimensional data of $i^{th}$ observation in $h^{th}$ hexagon.

```{r}
#| label: error-two-curvy

error_two_non_linear_diff_shaped_close_clusters_umap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_umap.rds")

## Find the minimum MSE when have duplicate a1
error_two_non_linear_diff_shaped_close_clusters_umap <- error_two_non_linear_diff_shaped_close_clusters_umap |>
  group_by(a1) |>
  filter(bin1 == min(bin1)) |>
  ungroup() |>
  mutate(prop_dens = 1/(b*side_length^2)) 

base_line_prop_dens <- error_two_non_linear_diff_shaped_close_clusters_umap |>
  filter(a1 == min(a1)) |>
  pull(prop_dens)

error_two_non_linear_diff_shaped_close_clusters_umap <- error_two_non_linear_diff_shaped_close_clusters_umap |>
  mutate(prop_comp = prop_dens/base_line_prop_dens)

mse_two_curvy_b <- ggplot(error_two_non_linear_diff_shaped_close_clusters_umap, 
                     aes(x = a1, 
                         y = MSE)) +
  geom_vline(xintercept = 0.09,
             linetype="solid",
           color = "#d95f02", linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1b9e77", linetype=2,
             linewidth=1) +
  geom_vline(xintercept = 0.13, linetype=2,
             color = "#1f78b4", linewidth=1) +
  geom_line(linewidth = 0.5) + 
  geom_point(size = 1) +
  scale_x_continuous(breaks = sort(unique(round(error_two_non_linear_diff_shaped_close_clusters_umap$a1, 2)))[c(1, 4, 8, 10, 11, 12, 13, 14)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = "MSE") +
  ggtitle("(a)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())


prop_dens_a1 <- ggplot(error_two_non_linear_diff_shaped_close_clusters_umap,
                     aes(x = a1,
                         y = prop_comp)) +
  geom_vline(xintercept = 0.09,
             linetype="solid",
           color = "#d95f02", linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1b9e77", linetype=2,
             linewidth=1) +
  geom_vline(xintercept = 0.13, linetype=2,
             color = "#1f78b4", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_non_linear_diff_shaped_close_clusters_umap$a1, 2)))[c(1, 4, 8, 10, 11, 12, 13, 14)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = paste("Relative proportion density")) +
  ggtitle("(d)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

error_two_non_linear_diff_shaped_close_clusters_umap <- error_two_non_linear_diff_shaped_close_clusters_umap |>
  mutate(prop_bins = b_non_empty/b)

a1_m_two_curvy <- ggplot(error_two_non_linear_diff_shaped_close_clusters_umap,
                     aes(x = a1,
                         y = prop_bins)) +
  geom_vline(xintercept = 0.09,
             linetype="solid",
           color = "#d95f02", linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1b9e77", linetype=2,
             linewidth=1) +
  geom_vline(xintercept = 0.13, linetype=2,
             color = "#1f78b4", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_non_linear_diff_shaped_close_clusters_umap$a1, 2)))[c(1, 4, 8, 10, 11, 12, 13, 14)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = expression(paste("proportion of non-empty bins ", bgroup("(", frac(m, b), ")")))) +
  ggtitle("(b)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

training_data_two_curvy <- training_data_two_curvy |>
  mutate(ID = row_number())

## Compute error
error_df_two_curvy_abs <- augment(
  df_bin_centroids = df_bin_centroids2,
  df_bin = df_bin_two_curvy2,
  training_data = training_data_two_curvy,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "x")

error_df_two_curvy_abs <- error_df_two_curvy_abs |>
  mutate(sqrt_row_wise_abs_error = sqrt(row_wise_abs_error))

error_df_two_curvy_abs <- error_df_two_curvy_abs |>
  bind_cols(umap_two_curvy_scaled |>
              select(-ID))

error_plot_two_curvy <- error_df_two_curvy_abs |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             colour = sqrt_row_wise_abs_error)) +
  geom_point(alpha=0.5) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  theme(
    aspect.ratio = 1
  )

```

```{r}
#| label: fig-p-d-error-in-2d-two-curvy
#| fig-cap: "The $7\\text{-}D$ model error in \\gD{} layout. Color indicates square root of absolute error, dark blue indicating high error and light indicates low error. Most large errors are distributed near the edges of the clusters."
#| out-height: 30%
#| fig-pos: H

error_plot_two_curvy
```

### Prediction into \gD{}

A new benefit of this fitted model is that it allows us to now predict a new observation's value in the NLDR, for any method. The steps are to determine the closest bin centroid in \pD{}, $C^{(p)}_{h}$ and predict it to be the centroid of this bin in \gD{}, $C^{(2)}_{h}$. This can be written as, let $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$, then the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. 

<!--prediction for true-model of two-curvy clust-->
```{r}
#| label: model-prediction-umap-original

predict_umap_two_curvy <- read_rds(file = "data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_predict_true.rds")

predict_two_curvy_obj <- gen_scaled_data(
    data = predict_umap_two_curvy)

predict_umap_two_curvy_scaled <- predict_two_curvy_obj$scaled_nldr

plot_predict_umap_fun_two_curvy <- ggplot(
  data = predict_umap_two_curvy_scaled, 
  aes(x = UMAP1, 
      y = UMAP2)) +
  geom_point(alpha = 0.5) +
  interior_annotation("b", c(0.08, 0.93), cex = 2)
```

```{r}
#| label: model-prediction-points-line

predict_umap_two_curvy_scaled <- predict_umap_two_curvy_scaled |>
  mutate(ID = row_number()) |>
  mutate(type = "predict_from_umap") 

jittered_points_df <- jittered_points_df |>
  mutate(type = "predict_from_model") 

predict_point_df <- bind_rows(
  predict_umap_two_curvy_scaled,
  jittered_points_df
)

plot_predict_umap_connect_two_curvy <- ggplot(
  data = predict_point_df, 
  aes(x = UMAP1, 
      y = UMAP2, 
      group = ID)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.3) +
  interior_annotation("d", c(0.08, 0.93), cex = 2)
```



```{r}
#| echo: false
#| label: fig-predict-two-curvy
#| fig-pos: H
#| fig-width: 10
#| fig-height: 10
#| out-height: 80%
#| fig-cap: "Comparison of prediction generated using the exiting `umap` R package's prediction method and our method: (a) A view of the true model in projection from $7\\text{-}D$, (b) predicted data from the `umap` prediction method, and (c) predicted data from our method."

proj_plot_two_curvy_true + plot_predict_umap_fun_two_curvy +plot_predict_umap_two_curvy +
  plot_layout(ncol = 3)
```


<!--The prediction approach involves finding the nearest \kD{} model point for a new \pD{} point. We define the function $z: \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{m\times p}$, where $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$ maps each \pD{} point to its nearest \pD{} mapping of the model. Therefore, the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$ be the predicted values.-->  



<!-- ::: {#fig-two_curvy-pred-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/two_curvy/sc_true_only.png){width="150" fig-align="center"} -->

<!-- ![](figures/two_curvy/pred_true_view.png){width="150" fig-align="center"} -->


<!-- A view of the true model in projections from $7\text{-}D$, and predictions of the true model in \gD{}, for the S-curve data. The predictions fits the UMAP layout which means that it capture the geometry of S-curve with UMAP. -->
<!-- ::: -->

<!-- XXX Prediction should have a jitter option, to spread the predicted points out fully in the hexagons. -->

### Tuning
<!-- removal of low density bins, removing long edges, choice of bins-->

The model fitting can be adjusted using these parameters: 

- hexagon bin parameters
    - bottom left bin position $(s_1, \ s_2)$, 
    - the total number of bins ($b$), 
- bin density cutoff, to remove low-density hexagons, and 
- edge length maximum, remove long edges from \gD{} representation. 

Default values are provided for each of these, but it is expected that the user will examine the MSE for a range of choices. Choosing these parameters according to MSE can be automated but it is recommended that the user examine the resulting model representation by overlaying it on the data in \pD{}. The next few subsections describe the calculation of default values, and the effect that different choices have on the model fit.

#### Hexagon bin parameters

The values $(s_1, \ s_2)$ define the position of the centroid of the bottom left hexagon. By default, this is at $s_1 = -q, s_2 = -qr_2$, where $q$ is the buffer bound the data. The choice of these values can have some effect on the distribution of bin counts. @fig-bins-two-curvy illustrates this. The distribution of bin counts for $s_1$ varying between $-0.1-0.0$ is shown. Generally, a more uniform distribution among these possibilities would indicate that the bins are reliably capturing the underlying distribution of observations. 

<!--The starting position of the hexagonal grid is important because different starting points result in different distributions of data across bins, even with the same total number of bins. This variation affects the model due to the differing number of non-empty bins. Therefore, it is necessary to evaluate various starting points with different total numbers of bins to determine which configurations are more effective at capturing the structure and fitting the model.--> 

```{r}
#| echo: false
#| label: fig-bins-two-curvy
#| fig-pos: H
#| fig-cap: "Hexbin density plots of UMAP layout of the two C-shaped clusters data, using three different bin inputs: (a) $b = 80 \\text{ } (10, \\text{ }8)$, (b) $b = 130 \\text{ } (13, \\text{ }10)$, and (c) $b = 391 \\text{ } (23, \\text{ }17)$. Color indicates standardized counts, dark indicating high count and light indicates low count. At the smallest bin size, the data structure is discontinuous, suggesting that there are too many bins. Using the MSE of the model fit in $7\\text{-}D$ helps decide on a useful choice of number of bins."
#| fig-width: 9
#| fig-height: 2

# hex_grid_coloured_two_curvy1 + 
#   hex_grid_coloured_two_curvy2 + 
#   hex_grid_coloured_two_curvy3 +
#   plot_layout(guides='collect', ncol = 3,
#               heights = c(2,1)) &
#   theme(legend.position='none', plot.tag = element_text(size = 8))

hex_grid_coloured_two_curvy1 +
  hex_grid_coloured_two_curvy2 +
  hex_grid_coloured_two_curvy3 +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
``` 


The default number of bins $b=b_1\times b_2$ is computed based on the sample size, by setting $b_1=n^{1/3}$, consistent with the Diaconis-Freedman rule [@freedman1981]. The value of $b_2$ is determined analytically by $b_1, q, r_2$. Values of $b_1$ between $2$ and $b_1 = \sqrt{\frac{n}{r_2}}$ are allowed. @fig-param-two-curvy (a) shows the effect of different choices of $b_1$ on the MSE of the fitted model. 

<!-- To determine the effective $b$, candidate values are selected based on the range between the minimum and approximate maximum $b_1$, because $b_2$ is computed from $b_1$. The minimum $b_1$ is set to $2$, while the maximum number is estimated by taking the square root of $\frac{n}{2}$. By evaluating MSE across varying $b$ within this range for different $q$, helps to determine an appropriate values for $b$ and $q$ (@fig-param-two_curvy (a)).--> 

<!--add MSE vs total number of error plot-->

<!--To generate errors for different total number of bins-->

#### Measurement of capturing the data shape in 2-D 

The area of a hexagon is defined as $A = \frac{3\sqrt{3}}{2}l^2$ where $l$ is the side length of the hexagon. If we know $a_1$ and $a_2$, $l$ can be computed (see appendix). The density of a hexagon grid is calculated as $\frac{\sum^{h}_{i=1}n_h}{A}$ and the proportion is $\frac{\sum^{h}_{i=1}n_h}{A \times b}$. The baseline proportion is the proportion density at the smallest possible value of $a_1$. The relative proportion density is the ratio of the observed proportion density to the baseline proportion density.

#### Removal of low density bins

By default, when assessing the choice of $b_1$, the total number of bins is measured by the number of **non-empty** bins. This more accurately reflects the hexagon grid relative the MSE than the full number of bins in the grid. It may also be beneficial to remove low count bins also, in the situation where data is clustered or stringy, where the observed data is sparse. In order to decide if this is necessary, you would examine the distribution of bin counts, or the density which puts the counts on a standard scale. If there is something of a gap at low values, this would suggest a potential value to use as a cutoff. Alternatively, one could choose to remove based on a percentile, the bins with density in the lowest 5% of all bins, for example. @fig-param-two-curvy (c) illustrates the effect on the model representation of removing bins below different percentages. Generally, we would urge caution in removing low count bins. 

<!-- Once setting up the hexagon grid with an appropriate number of bins, some hexagon bins may have few or no data points within them (@fig-bins-two_curvy (b)). To ensure comprehensive coverage of the NLDR data, it is necessary to select hexagon bins with a considerable number of data points. This involves calculating the number of points within each hexagon. Then, the standard count is computed by dividing the number of points within each hexagon by the maximum number of points in the grid. Next, bins with a standard count less than a benchmark value are removed (@fig-param-two_curvy (c)). There is no specific rule for selecting a benchmark value. However, the following steps can help determine a suitable value for removing low-density hexagons:

1. Plot the distribution of the standardized counts (@fig-param-two_curvy (b)).
2. Examine the distribution of counts.
3. Select the first quantile value if the distribution is skewed.
-->



The benchmark value for removing low-density hexagons ranges between $0$ and $1$. When analyzing how these benchmark values influence model performance, it's essential to observe the change in MSE as the benchmark value increases (@fig-param-two-curvy (c)). The MSE shows a gradual decrease as the benchmark value goes from $1$ to $0$. Evaluating this rate of increase is important. If the increment is not considerable, the decision might lean towards retaining low-density hexagons.

<!--add MSE vs density (0 to 1)-->



<!--base line -->

<!-- Furthermore, selecting the benchmark value for removing low-density hexagons is important. Removing unnecessary bins may lead to the formation of long edges and an uneven \gD{} model. Hence, rather than solely relying on the benchmark value to identify hexagons for removal, it's essential to consider the standard number of points in the neighboring hexagons of the identified low-density bins (see @fig-lwd-two_curvy (b)). If neighboring bins also show low counts, only those bins will be removed. The remaining bins are used to construct the \gD{} model.    -->

```{r}
#| label: rm-lwd-bin-error

error_rm_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_rm_lwd_diff_bin.rds") |>
  mutate(bin1  = as.factor(bin1))

mse_two_curvy_lwd <- ggplot(error_rm_two_curvy, 
                     aes(x = benchmark_rm_lwd, 
                         y = MSE,
                         color = bin1)) + 
  geom_point(
    size = 1
    ) +
  geom_line(
    linewidth = 0.5
    ) + 
  # geom_vline(xintercept = benchmark1, linetype="solid", 
  #            color = "#bdbdbd", linewidth=1, alpha = 0.5) +
  # scale_x_continuous("standardized bin count", 
  #        transform = "reverse") + 
  scale_x_continuous(breaks = sort(unique(error_rm_two_curvy$benchmark_rm_lwd))[c(1, 3, 5, 7, 9, 11, 13, 15, 17)]) +
  scale_color_manual(values=c("#1f78b4", "#d95f02", "#1b9e77")) +
  xlab("threshold to remove low-density hexagons") +
  ylab("MSE") +
  ggtitle("(c)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none")
```


```{r}
#| echo: false
#| fig-cap: "Various plots to help assess best hexagon bin parameters and thresholds to remove low-density bins. Both (a) and (c) show MSE, against binwidth ($a_1$) and threshold. A good benchmark value for these parameters is when the MSE drops and then flattens out. Three binwidth choices were made: $0.05$, $0.09$, and $0.13$ to investigate. As the binwidth increases, the proportion of non-empty bins also increases. There are two peaks at binwidths $0.09$ and $0.13$. The relative proportion density decreases and levels off. Binwidths $0.05$ is the favorable choices. Based on the MSE, $0.09$ was chosen as the initial best binwidth for further analysis. There is no need to remove the low-density hexagons because as shown in (b), there is no considerable drop in MSE."
#| label: fig-param-two-curvy
#| out-width: 80%
#| fig-width: 12
#| fig-height: 10
#| fig-pos: H

mse_two_curvy_b + a1_m_two_curvy +
  mse_two_curvy_lwd + prop_dens_a1 +
  plot_layout(ncol=2)
```

#### Removing long edges

Edges define the neighbourhood structure, in order to provide a smooth \gD{} representation of the fitted model. @fig-two-curvy-true-proj shows a wire frame of the true model that was used to generate the two C-shaped clusters example data. The ideal is that the representation of the fitted model, at least for this example where we know the true model, should look similar to this. 

The Delaunay triangulation will ensure that all centroids are connected into a triangular mesh. For some structures, like clustered data, or highly non-linear shapes, breaks in the mesh are meaningful. When separated clusters are present the mesh should be broken across the gaps. For non-linear structures like the C-shaped curvilinear, the mesh should run unbroken along the C, but there should be no edges connecting the top of the C directly to the bottom of the C. For these reasons it is necessary to remove edges from the mesh in some applications.

The decision on edge length removal is made based on the distribution of edge lengths. In particular, a gap between values, where there a concentration of small values and then a few larger values, likely suggests a cutoff for edge removal. Because the triangulation is typically done on the hexagon centroids, there are particular discrete edge lengths, based on bin widths. @fig-rm-lg illustrates edge length distributions. 

There is an additional step that is needed. When the model is lifted into \pD{}, if the fit is good all the edges should be relatively small in this space, too. If this is not the case, then there are several possible actions: (1) re-do the NLDR to get a more representative layout; (2) identify the edge and remove it from the model, in \gD{} and \pD{}; (3) consider different values for the model fit, number of bins, initial bin position or removing low density bins.

```{r}
#| label: fig-rm-lg
#| fig-cap: "Distribution of edge lengths and wireframes in \\gD{} and $7\\text{-}D$ with Delaunay triangulation, and two choices of long edge removal: (b) benchmark = 4.5$a_1$ and (c) benchmark = 2$a_1$, where $a_1$ = 0.09."
#| fig-height: 15
#| fig-width: 15

free(distance_points) /
  wrap_plots(trimesh_two_curvy2, trimesh_two_curvy_removed2, trimesh_two_curvy_removed1, two_curvy_proj_umap_model_delaunay, two_curvy_proj_umap_model_benchmark2, two_curvy_proj_umap_model, ncol = 3) +
  plot_annotation(tag_levels = list(c("(a)", "(b)")))

# free(distance_points) + plot_annotation(tag_levels = 'A') +
#  (trimesh_two_curvy2 + trimesh_two_curvy_removed2 + trimesh_two_curvy_removed1 + two_curvy_proj_umap_model_delaunay + two_curvy_proj_umap_model_benchmark2 + two_curvy_proj_umap_model) +
#   plot_layout(widths = c(0.5, 1, 1, 1)) +
#   plot_annotation(tag_levels = list(c("(a)", "(b)"))) 
```

## Best fit

Deciding on the best fit relies on several elements: 

- the choice of NLDR method, and the parameters used to create it, and
- model fit parameters: bin size, low density bin removal, long edge removal.

Comparing the MSE to obtain the best fit is suitable if one starts from the same NLDR representation. In theory, because the MSE is computed on \pD{} measuring the fit between model and data it might still be useful to compare different NLDR representations. A good NLDR representation should produce a good fit, producing a low MSE if the model fits the data well. However, it technically might be quite variable.

```{r}
#| label: read-two-curvy-clust-nldr
# Read a variety of different NLDR representations of two_non_linear_diff_shaped_close_clusters
# and plot them on same aspect ratio
tsne_two_non_linear_diff_shaped_close_clusters <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_30.rds")

nldr_two_non_linear_diff_shaped_close_clusters1 <- tsne_two_non_linear_diff_shaped_close_clusters |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a", c(0.08, 0.93)) +
  theme(aspect.ratio = 1)

umap_two_non_linear_diff_shaped_close_clusters <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_n-neigbors_15_min-dist_0.1.rds")

nldr_two_non_linear_diff_shaped_close_clusters2 <- umap_two_non_linear_diff_shaped_close_clusters |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#636363') +
  interior_annotation("b", c(0.08, 0.93)) +
  theme(aspect.ratio = 1)

phate_two_non_linear_diff_shaped_close_clusters <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_phate_knn_5.rds")

nldr_two_non_linear_diff_shaped_close_clusters3 <- phate_two_non_linear_diff_shaped_close_clusters |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("c") +
  theme(aspect.ratio = 1)

trimap_two_non_linear_diff_shaped_close_clusters <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_trimap_n-inliers_12_n-outliers_4_n-random_3.rds")

nldr_two_non_linear_diff_shaped_close_clusters4 <- trimap_two_non_linear_diff_shaped_close_clusters |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("d") +
  theme(aspect.ratio = 1)

pacmap_two_non_linear_diff_shaped_close_clusters <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_pacmap_n-neighbors_10_init_random_MN-ratio_0.5_FP-ratio_2.rds")

nldr_two_non_linear_diff_shaped_close_clusters5 <- pacmap_two_non_linear_diff_shaped_close_clusters |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e") +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-two_non_linear_diff_shaped_close_clusters

error_two_non_linear_diff_shaped_close_clusters_umap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_umap.rds")
error_two_non_linear_diff_shaped_close_clusters_tsne <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_tsne.rds")
error_two_non_linear_diff_shaped_close_clusters_phate <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_phate.rds")
error_two_non_linear_diff_shaped_close_clusters_trimap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_trimap.rds")
error_two_non_linear_diff_shaped_close_clusters_pacmap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_pacmap.rds")

error_two_non_linear_diff_shaped_close_clusters <- bind_rows(error_two_non_linear_diff_shaped_close_clusters_umap, 
                         error_two_non_linear_diff_shaped_close_clusters_tsne,
                         error_two_non_linear_diff_shaped_close_clusters_phate,
                         error_two_non_linear_diff_shaped_close_clusters_trimap,
                         error_two_non_linear_diff_shaped_close_clusters_pacmap)

error_two_non_linear_diff_shaped_close_clusters <- error_two_non_linear_diff_shaped_close_clusters |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-two_non_linear_diff_shaped_close_clusters

error_plot_two_non_linear_diff_shaped_close_clusters <- ggplot(error_two_non_linear_diff_shaped_close_clusters, 
                           aes(x = a1, 
                               y = MSE, 
                               colour = method)) + 
  geom_point(size = 0.8) +
  geom_line(linewidth = 0.3) + 
  # geom_vline(xintercept = 15, linetype="solid", 
  #            color = "black", linewidth=0.8, alpha = 0.5) +
  scale_x_continuous(breaks = sort(unique(error_two_non_linear_diff_shaped_close_clusters$a1))[seq(1, length(unique(error_two_non_linear_diff_shaped_close_clusters$a1)), by = 5)]) +
  scale_color_manual(values=c('#e41a1c','#ff7f00','#4daf4a', "#a65628",'#636363')) +
  scale_y_log10() +
  ylab("log(MSE)") +
  xlab(expression(paste("binwidth (", a[1], ")"))) +
  theme_minimal() +
  theme(panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none",
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title.x = element_text(size = 7),
        axis.title.y = element_text(size = 7)) 

```

```{r}
#| fig-cap: "Assessing which of the 5 NLDR layouts on the two C-shaped clusters data is the better representation using MSE for varying binwidth ($a_1$). Colour used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-e). Layout d is universally poor. Layouts a that show two close clusters are universally suboptimal. Layout a is the best choice."
#| label: fig-two_non_linear_diff_shaped_close_clusters-mse
#| fig-pos: H
#| out-height: 60%

free(error_plot_two_non_linear_diff_shaped_close_clusters) + wrap_plots(nldr_two_non_linear_diff_shaped_close_clusters1, nldr_two_non_linear_diff_shaped_close_clusters2, nldr_two_non_linear_diff_shaped_close_clusters3, 
                                    nldr_two_non_linear_diff_shaped_close_clusters4, nldr_two_non_linear_diff_shaped_close_clusters5, ncol = 2)
```

```{r}
#| label: best-fit-tsne-two-curvy

two_curvy_tsne_scaled_obj <- gen_scaled_data(
  data = tsne_two_non_linear_diff_shaped_close_clusters)

tsne_two_curvy_scaled <- two_curvy_tsne_scaled_obj$scaled_nldr |>
  mutate(ID = row_number())
lim1 <- two_curvy_tsne_scaled_obj$lim1
lim2 <- two_curvy_tsne_scaled_obj$lim2
r2_tsne <- diff(lim2)/diff(lim1)

tsne_two_non_linear_diff_shaped_close_clusters_plt <- tsne_two_curvy_scaled |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("b1", sc_ltr_pos) +
  theme(aspect.ratio = 1)

## hexagon binning to have regular hexagons
hb_obj_tsne_two_curvy2 <- hex_binning(
  data = tsne_two_curvy_scaled, 
  bin1 = 8, 
  r2 = r2_tsne)

a1_2_tsne <- calc_bins_y(
  bin1 = 8, 
  r2 = r2_tsne
)$a1

## Data set with all centroids
all_centroids_df4 <- hb_obj_tsne_two_curvy2$centroids

## Generate all coordinates of hexagons
hex_grid4 <- hb_obj_tsne_two_curvy2$hex_poly

## To obtain the standardise counts within hexbins
counts_df4 <- hb_obj_tsne_two_curvy2$std_cts
df_bin_centroids4 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df4, 
  counts_df = counts_df4) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 11")

hex_grid_with_counts_two_curvy4 <- full_join(
  hex_grid4, 
  df_bin_centroids4 |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID")) 

tsne_data_two_curvy2_with_hb_id <- hb_obj_tsne_two_curvy2$data_hb_id
df_all_two_curvy4 <- dplyr::bind_cols(training_data_two_curvy, tsne_data_two_curvy2_with_hb_id)
df_bin_two_curvy4 <- avg_highd_data(data = df_all_two_curvy4, col_start = "x")

df_b_two_curvy4 <- df_bin_two_curvy4 |>
  dplyr::filter(hb_id %in% df_bin_centroids4$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_two_curvy4 <- df_b_two_curvy4[match(df_bin_centroids4$hexID, df_b_two_curvy4$hb_id),] |>
  dplyr::select(-hb_id) 

## Triangulate bin centroids
tr1_object_two_curvy4 <- tri_bin_centroids(
  df_bin_centroids4, x = "c_x", y = "c_y")
tr_from_to_df_two_curvy4 <- gen_edges(
  tri_object = tr1_object_two_curvy4)

## Compute 2D distances
distance_two_curvy4 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_two_curvy4, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_two_curvy4 <- find_lg_benchmark(
  distance_edges = distance_two_curvy4, 
  distance_col = "distance")

benchmark_two_curvy4 <- 0.2

## Benchmark 1

distance_df_small_edges_two_curvy4 <- distance_two_curvy4 |>
  filter(distance < benchmark_two_curvy4) |> ## 0.221
  filter(!(row_number() %in% c(35, 89, 83, 25, 80, 56, 13, 59, 16)))

tr_from_to_df_two_curvy4 <- inner_join(
  tr_from_to_df_two_curvy4, distance_df_small_edges_two_curvy4,
  by = c("from", "to"))



trimesh_two_curvy_removed1_tsne_with_data <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy4,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = tSNE1,
               y = tSNE2
             ),
             color = "#a65628",
             alpha = 0.5,
             size = 0.5) +
  coord_equal() +
  interior_annotation("b2", sc_ltr_pos)

```

```{r}
#| label: best-fit-tsne-two-curvy-proj

# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy, df_b_two_curvy4)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_true_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "model") |>
  select(-type)


## First projection
projection <- cbind(
  c(-0.08707,-0.06493,0.07017,-0.07349,-0.02955,-0.05286,-0.02765),
  c(0.03398,0.03735,0.09272,0.01871,-0.09224,0.00451,0.08079))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number()) 

## For fitted model
projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_two_curvy4 |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## For true model

projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% projection_scaled

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")


axes_obj <- gen_axes(  
  proj = projection * 5,
  limits = 0.15,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.01)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_tsne_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#a65628") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("b3", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_tsne_all_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = true_model_df_proj, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#a65628") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("b4", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )
```

```{r}
#| label: best-fit-pacmap-two-curvy

two_curvy_pacmap_scaled_obj <- gen_scaled_data(
  data = pacmap_two_non_linear_diff_shaped_close_clusters)

pacmap_two_curvy_scaled <- two_curvy_pacmap_scaled_obj$scaled_nldr |>
  mutate(ID = row_number())
lim1 <- two_curvy_pacmap_scaled_obj$lim1
lim2 <- two_curvy_pacmap_scaled_obj$lim2
r2_pacmap <- diff(lim2)/diff(lim1)

pacmap_two_non_linear_diff_shaped_close_clusters_plt <- pacmap_two_curvy_scaled |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("c1", sc_ltr_pos) +
  theme(aspect.ratio = 1)

## hexagon binning to have regular hexagons
hb_obj_pacmap_two_curvy2 <- hex_binning(
  data = pacmap_two_curvy_scaled, 
  bin1 = 8, 
  r2 = r2_pacmap)

a1_2_pacmap <- calc_bins_y(
  bin1 = 8, 
  r2 = r2_pacmap
)$a1

## Data set with all centroids
all_centroids_df4 <- hb_obj_pacmap_two_curvy2$centroids

## Generate all coordinates of hexagons
hex_grid4 <- hb_obj_pacmap_two_curvy2$hex_poly

## To obtain the standardise counts within hexbins
counts_df4 <- hb_obj_pacmap_two_curvy2$std_cts
df_bin_centroids4 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df4, 
  counts_df = counts_df4) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 11")

hex_grid_with_counts_two_curvy4 <- full_join(
  hex_grid4, 
  df_bin_centroids4 |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID")) 

pacmap_data_two_curvy2_with_hb_id <- hb_obj_pacmap_two_curvy2$data_hb_id
df_all_two_curvy4 <- dplyr::bind_cols(training_data_two_curvy, pacmap_data_two_curvy2_with_hb_id)
df_bin_two_curvy4 <- avg_highd_data(data = df_all_two_curvy4, col_start = "x")

df_b_two_curvy4 <- df_bin_two_curvy4 |>
  dplyr::filter(hb_id %in% df_bin_centroids4$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_two_curvy4 <- df_b_two_curvy4[match(df_bin_centroids4$hexID, df_b_two_curvy4$hb_id),] |>
  dplyr::select(-hb_id) 

## Triangulate bin centroids
tr1_object_two_curvy4 <- tri_bin_centroids(
  df_bin_centroids4, x = "c_x", y = "c_y")
tr_from_to_df_two_curvy4 <- gen_edges(
  tri_object = tr1_object_two_curvy4)

## Compute 2D distances
distance_two_curvy4 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_two_curvy4, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_two_curvy4 <- find_lg_benchmark(
  distance_edges = distance_two_curvy4, 
  distance_col = "distance")

benchmark_two_curvy4 <- 0.2

## Benchmark 1

distance_df_small_edges_two_curvy4 <- distance_two_curvy4 |>
  filter(distance < benchmark_two_curvy4) |> ## 0.221
  filter(!(row_number() %in% c(35, 89, 83, 25, 80, 56, 13, 59, 16)))

tr_from_to_df_two_curvy4 <- inner_join(
  tr_from_to_df_two_curvy4, distance_df_small_edges_two_curvy4,
  by = c("from", "to"))

trimesh_two_curvy_removed1_pacmap_with_data <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy4,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = pacmap_two_curvy_scaled,
             aes(
               x = PaCMAP1,
               y = PaCMAP2
             ),
             color = "#e41a1c",
             alpha = 0.5,
             size = 0.5) +
  coord_equal() +
  interior_annotation("c2", sc_ltr_pos)

```

```{r}
#| label: best-fit-pacmap-two-curvy-proj

# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy, df_b_two_curvy4)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_true_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "model") |>
  select(-type)


## First projection
projection <- cbind(
  c(-0.08707,-0.06493,0.07017,-0.07349,-0.02955,-0.05286,-0.02765),
  c(0.03398,0.03735,0.09272,0.01871,-0.09224,0.00451,0.08079))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_two_curvy_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number()) 

## For fitted model
projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_two_curvy4 |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

## For true model

projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% projection_scaled

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")


axes_obj <- gen_axes(  
  proj = projection * 5,
  limits = 0.15,
  axis_pos_x = -0.1,
  axis_pos_y = -0.1,
  axis_labels = names(scaled_two_curvy_data),
  threshold = 0.01)

axes <- axes_obj$axes
circle <- axes_obj$circle

two_curvy_proj_pacmap_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") + #31a354
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#e41a1c") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("c3", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )

two_curvy_proj_pacmap_all_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = true_model_df_proj, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#2b8cbe") +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    size = 0.5,
    alpha = 0.5,
    color = "#e41a1c") +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 4) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.126, 0.125)) + 
  ylim(c(-0.126, 0.125)) +
  interior_annotation("c4", sc_ltr_pos) +
  theme(
    legend.position = "none"
  )
```

```{r}
#| label: best-fit-umap-tsne-pacamp
#| fig-cap: "Best fit for two C-shaped clusters data using UMAP (a1-a4) ($a_1$ = 0.09) and tSNE (b1-b4) ($a_1$ = 0.16) with the same number of non-empty bins ($m$ = 39). Both models show twist in $7\\text{-}D$ (a3, a4, b3, b4). Also, both NLDR methods are unable to accurately capture the width of the curvilinear clusters."
#| fig-width: 15
#| fig-height: 20
#| out-height: 90%

nldr_two_curvy_original2 + tsne_two_non_linear_diff_shaped_close_clusters_plt +
  pacmap_two_non_linear_diff_shaped_close_clusters_plt +
  trimesh_two_curvy_removed1_with_data + trimesh_two_curvy_removed1_tsne_with_data +
trimesh_two_curvy_removed1_pacmap_with_data +
  two_curvy_proj_umap_model2 +
  two_curvy_proj_tsne_model +
  two_curvy_proj_pacmap_model +
  two_curvy_proj_umap_all_model +
  two_curvy_proj_tsne_all_model +
  two_curvy_proj_pacmap_all_model +
  plot_layout(ncol = 3) +
  plot_annotation(tag_levels = list(c("(a)", "(b)", "(c)")))
```

## Linked plots

It’s important to access the \gD{} layout and the generated model overlaid on data in \pD{} together to understand whether it fits the points everywhere, fits better in some places, or simply mismatches the pattern. Interactivity also helps in understanding the quirks that occur with different NLDR techniques (XXXXRefer the video after finalising the two C-shaped clusters data).

## A curious difference between tSNE, UMAP and PaCMAP revealer

<!-- XXX THIS IS NOT INTERESTING IN ITS CURRENT FORM. THIS IS ONLY ANY INTERESTING EXAMPLE WHEN THE FILLED OUT VS FLAT SHAPES ARE DISCUSSED. -->

In this section, the evaluation focuses on how effectively the tSNE, UMAP, and PaCMAP algorithms preserve the clustering structure. The data consists of five $4\text{-}D$ clusters, each with $1000$ points generated from a multivariate normal distribution with a small variance of $0.0025$. The first cluster is centered at $(0, 0, 0, 0)$, the second at $(1, 0, 0, 0)$, the third at $(0, 1, 0, 0)$, the fourth at $(0, 0, 1, 0)$, and the fifth at $(0, 0, 0, 1)$.

<!-- In this section, we will assess how tSNE, UMAP, and PaCMAP algorithms preserve the clustering structure. We use a simulated dataset consisting of five spherical Gaussian clusters where each cluster occupies a different corner of the $4\text{-}D$ space, with each cluster containing an equal number of points ($1000$) and the same within-cluster variation. The data simulated from $\Sigma = \begin{pmatrix} -->
<!-- 0.0025 & 0 & 0 & 0 \\ -->
<!-- 0 & 0.0025 & 0 & 0 \\ -->
<!-- 0 & 0 & 0.0025 & 0 \\ -->
<!-- 0 & 0 & 0 & 0.0025 \\ -->
<!-- \end{pmatrix}$. -->

In the tSNE layout (@fig-five-gau-projs (a1)), the distances between clusters appear smaller, leading to more tightly packed clusters, which is typical of tSNE's focus on preserving local structures. This often results in less emphasis on the global distances between clusters. On the other hand, UMAP (@fig-five-gau-projs (b1)) tends to maintain larger separations between clusters, better reflecting the global structure and preserving the relative distances between clusters more effectively. In contrast, PaCMAP (@fig-five-gau-projs (c1)) aim to balance these aspects by maintaining the distance between clusters and accurately positioning them, effectively capturing both global and local structures.

By visualizing the model of different layouts in $4\text{-}D$ reveal different characteristics of each NLDR method. When examining the model of a specific cluster, which appears more like a filled-out shape in tSNE (@fig-five-gau-projs (a2, a3)), and UMAP (@fig-five-gau-projs (b2, b3)), while the model has a pancake shape in PaCMAP (@fig-five-gau-projs (c2, c3)). The reason to have filled-out shaped clusters in tSNE and UMAP is that the ability of capture the local structure. On the other hand, the reason for the pancake shape in PaCMAP is the fail to capture the local structure.   

<!-- The tSNE, UMAP, and PaCMAP layouts each reveal different characteristics of the clustering structure within the dataset.  -->


<!-- When examining the model of a specific cluster, which appears more like a filled-out pancake in $4\text{-}D$, we observe that tSNE, UMAP, and PaCMAP all effectively capture global structures. However, there is evidence suggesting that PaCMAP, with its default hyper-parameter settings, struggles to preserve the local structure as effectively as tSNE and UMAP. -->

The differences in performance can be attributed to several factors. One key element is the choice of hyper-parameters, particularly the number of neighbors considered during the dimensionality reduction process. This parameter significantly impacts how well local structures are maintained. Additionally, the balance between global and local structures, and issues related to optimization and initialization all contribute to the observed outcomes. 

It's important to note that these layouts are made with default hyper-parameter settings. Adjusting these parameters could lead to different results, potentially improving PaCMAP's ability to capture local structures or altering how tSNE and UMAP handle global and local structures. Thus, while PaCMAP generally balances global and local preservation, careful tuning of its parameters is important for achieving the best results in any given dataset.



<!-- In this section, the effectiveness of the algorithm is described using a simulated dataset. The dataset consists of five spherical Gaussian clusters in $4\text{-}D$, with each cluster containing an equal number of points and the same within-cluster variation. -->

<!-- XXX Add tSNE, UMAP, and PaCMAP NLDR layouts and describe what is happening -->
<!-- XXX Discuss the models of one specific cluster (filled out, pancakes): tSNE, UMAP, PaCMAP are useful to capture global structures, but this is an evidence where PaCMAP fail to capture the local structure with default hyper-parameter settings than tSNE and UMAP -->
<!-- XXX Why? hyper-parameter choice specially number of neighbors, data characteristics (intrinsic dimensionality), imbalance between global and local structures, optimization and initialization issues -->
<!-- XXX Add this is with specific hyper-parameter choices -->

<!-- The \gD{} layouts generated by tSNE, UMAP, and PaCMAP show five well-separated clusters which evident these methods effectively preserve the global structure. In tSNE (@fig-gau-tsne-sc (a)), these clusters appear closely. UMAP arranges all clusters in a parallel manner, with three aligned in one line and the other two in a separate line (@fig-gau-umap-sc (a)). In contrast, PaCMAP shows one central cluster and the remaining four spread out in different directions (@fig-gau-pacmap-sc (a)). -->

<!-- The tSNE and UMAP shows *filled out* clusters which provide evidence that these methods preserve the local structure (@fig-gau-tsne-sc (c) and @fig-gau-umap-sc (c)). On the other hand, PaCMAP shows *flat* shapes clusters in the model and evident that PaCMAP fail to capture the within-cluster variation (@fig-gau-pacmap-sc (c)).  -->

<!--Projections-->
```{r}
#| label: five-gau-proj-tsne-model

training_data_gau <- read_rds("data/five_gau_clusters/data_five_gau.rds")

data_gau <- training_data_gau |> 
  select(-ID) |>
  mutate(type = "data")

tsne_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau_71.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = tsne_data_gau)
tsne_gau_scaled <- gau1_scaled_obj$scaled_nldr

tsne_gau <- tsne_gau_scaled |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.3, color = "#000000") +
  geom_rect(aes(xmin = -0.05, xmax = 0.35, ymin = 0.28, ymax = 0.6),
               fill = "transparent", color = "red", linewidth = 0.8)

## Compute hexbin parameters
num_bins_x_gau1 <- 13
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = tsne_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x",
  q = 0.1
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Compute error

error_df_gau_abs <- augment(
  df_bin_centroids = df_bin_centroids_gau1,
  df_bin = df_bin_gau1,
  training_data = training_data_gau,
  newdata = NULL,
  type_NLDR = "tSNE",
  col_start = "x")

error_df_gau_abs <- error_df_gau_abs |>
  mutate(sqrt_row_wise_abs_error = sqrt(row_wise_abs_error))

error_df_gau_abs <- error_df_gau_abs |>
  bind_cols(tsne_gau_scaled |>
              select(-ID))

error_df_gau_tsne <- error_df_gau_abs |>
  ggplot(aes(x = tSNE1,
             y = tSNE2,
             colour = sqrt_row_wise_abs_error)) +
  geom_point(alpha=0.5) + 
  xlim(c(0.3, 0.66)) + ylim(c(-0.01, 0.35)) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  interior_annotation("a3", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

# tr_from_to_df_gau1 <- tr_from_to_df_gau1 |>
#   filter(row_number() != 76) |>
#   filter(row_number() != 34)

# tr_from_to_df_gau1 <- tr_from_to_df_gau1 |>
#   filter(row_number() != 149)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

benchmark_gau1 <- 0.1

# trimesh_removed_gau1 <- vis_rmlg_mesh(
#   distance_edges = distance_gau1,
#   benchmark_value = benchmark_gau1,
#   tr_coord_df = tr_from_to_df_gau1,
#   distance_col = "distance")

tr_df <- distinct(tibble(
  x = c(tr_from_to_df_gau1[["x_from"]], tr_from_to_df_gau1[["x_to"]]), 
  y = c(tr_from_to_df_gau1[["y_from"]], tr_from_to_df_gau1[["y_to"]])))

distance_df_small_edges_gau1 <- distance_gau1 |>
  filter(distance < benchmark_gau1) |>
  filter(!(row_number() %in% c(33, 40, 48, 138, 143)))

tr_from_to_df_gau1 <- inner_join(
  tr_from_to_df_gau1, distance_df_small_edges_gau1, 
  by = c("from", "to"))

trimesh_removed_gau_tsne <- ggplot() + 
  geom_segment(data = tr_from_to_df_gau1, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  geom_point(
    data = tsne_gau_scaled,
    aes(
      x = tSNE1,
      y = tSNE2
    ),
    alpha = 0.2,
    #size = 0.1,
    color = "#999999"
  ) + 
  geom_rect(
      aes(xmin = 0.3, 
          xmax = 0.65, 
          ymin = -0.05, 
          ymax = 0.35),
          fill = "transparent", 
      color = "red", 
      linewidth = 0.5) +
  interior_annotation("a1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )


## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = tsne_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  q = 0.1)

tsne_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                  tsne_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

# distance_df_small_edges <- distance_df_small_edges |>
#   filter(row_number() != 108)

## First projection
# projection <- cbind(
#     c(0.46477,-0.02024,0.56378,0.39736),
#     c(0.15607,-0.54876,-0.44090,0.41505))
projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558,-0.23854,-0.63659,0.35753))

projection_scaled <- projection * 1

projected <- as.matrix(scaled_gau_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.3,
  axis_pos_x = 0.05,
  axis_pos_y = -0.85,
  axis_labels = names(scaled_gau_data),
  threshold = 0)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_tsne_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2),
    label=rownames(axes), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(0, 0.4)) +
  ylim(c(-0.9, -0.5)) +
  interior_annotation("a2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
# projection <- cbind(
#     c(-0.71895,0.29250,-0.29905,-0.01656),
#     c(-0.16698,-0.62707,-0.23756,0.46328))
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.3,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7,
  axis_labels = names(scaled_gau_data),
  threshold = 0)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_tsne_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2), 
    label=rownames(axes), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.36)) +
  ylim(c(-0.75, -0.36)) +
  interior_annotation("a3", 
                      position = c(0.08, 0.9),
                      cex = 2)

# five_gau_proj_tsne_model <- gen_proj_langevitour(
#   points_df = df_exe,
#   projection = projection,
#   edge_df = distance_df_small_edges |> select(-distance)
# ) +
#   xlim(c(-0.18, 0.1)) +
#   ylim(c(-0.78, -0.55))
```

```{r}
#| label: five-gau-proj-umap-model

umap_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = umap_data_gau)
umap_gau_scaled <- gau1_scaled_obj$scaled_nldr

umap_gau <- umap_gau_scaled |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.3, color = "#000000")

## Compute hexbin parameters
num_bins_x_gau1 <- 45
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = umap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Compute error

error_df_gau_abs <- augment(
  df_bin_centroids = df_bin_centroids_gau1,
  df_bin = df_bin_gau1,
  training_data = training_data_gau,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "x")

error_df_gau_abs <- error_df_gau_abs |>
  mutate(sqrt_row_wise_abs_error = sqrt(row_wise_abs_error))

error_df_gau_abs <- error_df_gau_abs |>
  bind_cols(umap_gau_scaled |>
              select(-ID))

error_df_gau_umap <- error_df_gau_abs |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             colour = sqrt_row_wise_abs_error)) +
  geom_point(alpha=0.5) + 
  xlim(c(0.47, 0.5475)) + ylim(c(0.0125, 0.09)) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  interior_annotation("b3", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

tr_df <- distinct(tibble(
  x = c(tr_from_to_df_gau1[["x_from"]], tr_from_to_df_gau1[["x_to"]]), 
  y = c(tr_from_to_df_gau1[["y_from"]], tr_from_to_df_gau1[["y_to"]])))

distance_df_small_edges_gau1 <- distance_gau1 |>
  filter(distance < benchmark_gau1) |>
  mutate(ID = row_number()) |>
  filter(!(ID %in% c(112)))

tr_from_to_df_gau1 <- inner_join(
  tr_from_to_df_gau1, distance_df_small_edges_gau1, 
  by = c("from", "to"))

trimesh_removed_gau_umap <- ggplot() + 
  geom_segment(data = tr_from_to_df_gau1, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  geom_point(
    data = umap_gau_scaled,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    alpha = 0.05,
    #size = 0.1,
    color = "#999999"
  ) + 
  geom_rect(
    aes(xmin = 0.45, 
        xmax = 0.57, 
        ymin = 0, 
        ymax = 0.1),
    fill = "transparent", 
    color = "red", 
    linewidth = 0.5)  +
  interior_annotation("b1", 
                      position = c(0.06, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = umap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1)

umap_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                umap_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## First projection
# projection <- cbind(
#   c(0.02743,0.21427,0.62893,-0.49816),
#   c(-0.70709,0.42156,-0.02593,0.10965))
# projection <- cbind(
#   c(-0.00215,-0.68905,-0.04778,-0.54223),
#   c(0.42558,-0.23854,-0.63659,0.35753))

projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558, -0.23854, -0.63659, 0.35753))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.28,
  axis_pos_x = -0.58,
  axis_pos_y = -0.35,
  axis_labels = names(scaled_gau_data),
  threshold = 0)
  
axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.5,
    alpha = 0.3,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.64, -0.29)) +
  ylim(c(-0.4, -0.05)) +
  interior_annotation("b2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.28,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7,
  axis_labels = names(scaled_gau_data),
  threshold = 0)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.4)) +
  ylim(c(-0.75, -0.4)) +
  interior_annotation("b3", 
                      position = c(0.08, 0.9),
                      cex = 2)

# five_gau_proj_umap_model <- gen_proj_langevitour(
#   points_df = df_exe,
#   projection = projection,
#   edge_df = distance_df_small_edges |> select(-distance)
# ) +
#   xlim(c(-0.18, 0.1)) +
#   ylim(c(-0.78, -0.55))
```

```{r}
#| label: five-gau-proj-pacmap-model

pacmap_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = pacmap_data_gau)
pacmap_gau_scaled <- gau1_scaled_obj$scaled_nldr |>
  select(PaCMAP1, PaCMAP2, ID)

pacmap_gau <- pacmap_gau_scaled |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2)) +
  geom_point(alpha=0.3, color = "#000000")

## Compute hexbin parameters
num_bins_x_gau1 <- 21
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = pacmap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x",
  q = 0.1
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Compute error

error_df_gau_abs <- augment(
  df_bin_centroids = df_bin_centroids_gau1,
  df_bin = df_bin_gau1,
  training_data = training_data_gau,
  newdata = NULL,
  type_NLDR = "PaCMAP",
  col_start = "x")

error_df_gau_abs <- error_df_gau_abs |>
  mutate(sqrt_row_wise_abs_error = sqrt(row_wise_abs_error))

error_df_gau_abs <- error_df_gau_abs |>
  bind_cols(pacmap_gau_scaled |>
              select(-ID))

error_df_gau_pacmap <- error_df_gau_abs |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2,
             colour = sqrt_row_wise_abs_error)) +
  geom_point(alpha=0.6,
             size = 1.5) + 
  xlim(c(-0.02, 0.18)) + ylim(c(0.12, 0.32)) +
  scale_colour_continuous_sequential(palette = "YlOrRd", breaks = 20) +
  interior_annotation("c3", 
                      position = c(0.08, 0.9),
                      cex = 2)


## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

# trimesh_removed_gau1 <- vis_rmlg_mesh(
#   distance_edges = distance_gau1,
#   benchmark_value = benchmark_gau1,
#   tr_coord_df = tr_from_to_df_gau1,
#   distance_col = "distance")

tr_df <- distinct(tibble(
  x = c(tr_from_to_df_gau1[["x_from"]], tr_from_to_df_gau1[["x_to"]]), 
  y = c(tr_from_to_df_gau1[["y_from"]], tr_from_to_df_gau1[["y_to"]])))

distance_df_small_edges_gau1 <- distance_gau1 |>
  filter(distance < benchmark_gau1) |>
  mutate(ID = row_number()) |>
  filter(!(ID %in% c(97)))

tr_from_to_df_gau1 <- inner_join(
  tr_from_to_df_gau1, distance_df_small_edges_gau1, 
  by = c("from", "to"))

trimesh_removed_gau_pacmap <- ggplot() + 
  geom_segment(data = tr_from_to_df_gau1, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  geom_point(
    data = pacmap_gau_scaled,
    aes(
      x = PaCMAP1,
      y = PaCMAP2
    ),
    alpha = 0.3,
    #size = 0.1,
    color = "#999999"
  ) +
  geom_rect(
    aes(xmin = -0.04, 
        xmax = 0.22, 
        ymin = 0.08, 
        ymax = 0.34),
    fill = "transparent", 
    color = "red", 
    linewidth = 0.5) +
  interior_annotation("c1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = pacmap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  q = 0.1)

pacmap_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                pacmap_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

## First projection
projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558,-0.23854,-0.63659,0.35753))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.25,
  axis_pos_x = -0.41,
  axis_pos_y = 0.23,
  axis_labels = names(scaled_gau_data),
  threshold = 0)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_pacmap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.5,
    alpha = 0.3,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.46, -0.12)) +
  ylim(c(0.18, 0.52)) +
  interior_annotation("c2", 
                      position = c(0.08, 0.9),
                      cex = 2)

### Second projection
# projection <- cbind(
#     c(-0.60995,0.33361,0.32920,-0.31518),
#     c(0.19525,-0.30554,0.74417,0.07600))
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 0.3,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7,
  axis_labels = names(scaled_gau_data),
  threshold = 0)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_pacmap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    linewidth = 1) +
  geom_point(
    #size = 0.8,
    alpha = 0.2,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.35)) +
  ylim(c(-0.75, -0.35))+
  interior_annotation("c3", 
                      position = c(0.08, 0.9),
                      cex = 2)


```

```{r}
#| label: fig-five-gau-projs
#| fig-height: 15
#| fig-width: 15
#| fig-pos: H
#| fig-cap: "Model in \\gD{}, on the layouts a1, b1, and c1, and two views of the fit in projections of each layout from $4\\text{-}D$, for the five Gaussian cluster data. All \\gD{} models show five well-separated clusters with different gaps. Layout a1 shows small separations, while layout b2 shows large gaps. On the other hand, layout b3 shows average gaps with exact positions as in $4\\text{-}D$. Assessing the different models fit for different layouts for a specific cluster (highlighted in red) shows filled-out shape in a1, a2, and b1, b2. On the other hand, c1 and c2 shows pancake shape.  Videos of the langevitour animations are available at <https://youtu.be/5Y1hE4i7N2k>, <https://youtu.be/5Y1hE4i7N2k>, and <https://youtu.be/5Y1hE4i7N2k> respectively."

trimesh_removed_gau_tsne + trimesh_removed_gau_umap + trimesh_removed_gau_pacmap +
five_gau_proj_tsne_model1 + five_gau_proj_umap_model1 + five_gau_proj_pacmap_model1 + error_df_gau_tsne + error_df_gau_umap + error_df_gau_pacmap +
  plot_layout(guides = "collect", nrow = 3) &
  theme(legend.position='none')
```

<!-- ::: {#fig-gau-tsne-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/tsne_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_tsne.png) -->

<!-- ![](figures/five_gau_clusters/sc_tsne_3.png) -->

<!-- The tSNE layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and tries to *filled out* the clusters. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/RASEE7N5MbM>).  -->
<!-- ::: -->

<!-- ::: {#fig-gau-umap-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/umap_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_umap.png) -->

<!-- ![](figures/five_gau_clusters/sc_umap_3.png) -->

<!-- The UMAP layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and tries to *filled out* the clusters, but not as much as tSNE. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/iG4bCPkJilw>).  -->
<!-- ::: -->

<!-- ::: {#fig-gau-pacmap-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/pacmap_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_pacmap.png) -->

<!-- ![](figures/five_gau_clusters/sc_pacmap_2.png) -->

<!-- The PaCMAP layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and shows *flat* shaped clusters. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/z07cKXi8EJQ>).  -->
<!-- ::: -->

## Applications {#sec-applications}

### Single-cell gene expression
<!--
- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use our method to get a better representation
-->

In the field of single-cell studies, a common analytical task involves clustering to identify groups of cells with similar expression profiles. NLDR methods are commonly used to display clusters, and help to verify the results. For example, @chen2023 illustrates the use of UMAP to identify clusters in Human Peripheral Blood Mononuclear Cells (PBMC3k). There are $2622$ single cells. First 9 principal components are used to generate the UMAP. @fig-NLDR-variety (a) is the reproduction of the published plot. The objective is to assess the published layout, and if it does not accurately represent the three clusters with small separations of the PBMC3k dataset (@fig-model-pbmc-author-proj (a2)), then select a reasonable \gD{} layout. 

<!-- UMAP layout with author's suggested parameter choice-->
```{r}
#| label: published-pbmc
## Import data
training_data_pbmc <- read_rds("data/pbmc3k/pbmc_pca_50.rds")
names(training_data_pbmc) <- paste0("pc", 1:50)

training_data_pbmc <- training_data_pbmc[, 1:9] |>
  mutate(ID = 1:NROW(training_data_pbmc))

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc)
umap_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

# umap_pbmc <- umap_pbmc_scaled |>
#   ggplot(aes(x = UMAP1,
#              y = UMAP2)) +
#   geom_point(alpha=0.3, color = "#000000") 
```

The @fig-NLDR-variety (a) shows three well-separated clusters with big separations. However, as shown in @fig-model-pbmc-author-proj (a2), there is no big separation between three clusters in $9\text{-}D$. Therefore, the suggested UMAP representation (@fig-NLDR-variety (a)) does not accurately represent the structure of PBMC3k dataset.

As a result, it is necessary to find an appropriate layout for the dataset. MSE for different binwidths ($a_1$) using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with various (hyper-)parameter settings were computed (@fig-pbmc-mse). Layouts c, d, and e, which show small separations between clusters, are universally optimal. However, layout d performs well with smaller binwidths and poorly with larger binwidths. On the other hand, layout e performs well with larger binwidths. Layout c was selected for further analysis due to its better (hyper-)parameter selection for the same method of the published plot.

The visualization of the selected layout in the $9\text{-}D$ shows edges between the clusters (@fig-mnist-tri-proj (b2)). This supports the presence of small separations between clusters. Additionally, the data points are not uniformly distributed across the clusters, resulting in dense areas (@fig-mnist-tri-proj (b2)). Furthermore, the clusters shows non-linear shapes (@fig-mnist-tri-proj (b3)).

<!-- As shown in @fig-NLDR-variety, layout c, d, and e that show small separations between clusters are universally optimal. But layout d with little separations perform well at tiny binwidth and poorly as binwidth increases. Also, layout e performs well at large binwidths. The layout c is chosen as the best, since it used the UMAP with a different (hyper-) parameter settings. -->

<!-- The visualization of the model generated for the selected layout in $9\text{-}D$ shows edges between the clusters (@fig-mnist-tri-proj (b2)). This provides evidence for the small separations between clusters. Furthermore, the data does not uniformally distributed along the clusters and therefore shows densed points (@fig-mnist-tri-proj (b2). Also, the clusters are non-linear shaped (@fig-mnist-tri-proj (b3).   -->

<!-- To determine whether the UMAP representation with the specific (hyper-)parameter choice suggested by @chen2023 preserves the data structure present in $9\text{-}D$ (@fig-model-pbmc-author-proj (b)), we visualize the model constructed with UMAP overlaid on the $9\text{-}D$ data.  -->

<!-- But, when visualizing the *model-in-the-data-space* some unobserved structures can be seen. Some clusters have non-linear continuity patterns and high-density patches (@fig-model-pbmc-author-proj). -->

<!-- As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)). This demonstrates that tSNE accurately captures the data structure of the PBMC3k dataset, which UMAP did not achieve. -->


<!-- In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $b_1$ using UMAP with different (hyper-)parameter settings and tSNE with default (hyper-)parameter setting (@fig-pbmc-mse) were calculated. After analyzing the results, it was found that tSNE with default (hyper-)parameter setting (perplexity: $30$) achieved the lowest error. Therefore, tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. -->


<!--Fit the best model for author suggestion and compute error-->
```{r}
#| label: hexbin-pbmc
## Compute hexbin parameters
num_bins_x_pbmc <- 26
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  q = 0.1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "pc"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

tr_df <- distinct(tibble(
  x = c(tr_from_to_df_pbmc[["x_from"]], tr_from_to_df_pbmc[["x_to"]]), 
  y = c(tr_from_to_df_pbmc[["y_from"]], tr_from_to_df_pbmc[["y_to"]])))

distance_df_small_edges_pbmc <- distance_pbmc |>
  filter(distance < benchmark_pbmc)

tr_from_to_df_pbmc <- inner_join(
  tr_from_to_df_pbmc, distance_df_small_edges_pbmc, 
  by = c("from", "to"))

# trimesh_removed_pbmc <- vis_rmlg_mesh(
#   distance_edges = distance_pbmc,
#   benchmark_value = benchmark_pbmc,
#   tr_coord_df = tr_from_to_df_pbmc,
#   distance_col = "distance") +
#   geom_point(
#     data = umap_pbmc_scaled,
#     aes(
#       x = UMAP1,
#       y = UMAP2
#     ),
#     alpha = 0.3,
#     size = 0.1,
#     color = "#000000"
#   ) 

trimesh_removed_pbmc <- ggplot() + 
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  geom_point(
    data = umap_pbmc_scaled,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    alpha = 0.2,
    color = '#a65628'
  )  +
  interior_annotation("a1", cex = 2) +
  theme(
    aspect.ratio = 1
  )

# ## Compute error
# error_df <- augment(
#   df_bin_centroids = df_bin_centroids_pbmc,
#   df_bin = df_bin_pbmc,
#   training_data = training_data_pbmc,
#   newdata = NULL,
#   type_NLDR = "UMAP",
#   col_start = "PC_")

# ## Categorize error
# 
# error_df <- error_df |>
#   mutate(type = case_when(
#     row_wise_abs_error <= 5 ~ "error 0-5",
#     row_wise_abs_error <= 10 ~ "error 5-10",
#     row_wise_abs_error <= 15 ~ "error 10-15",
#     row_wise_abs_error <= 20 ~ "error 15-20",
#     .default = "error greter than 20"
#   )) |>
#   mutate(type = factor(type, levels = c(
#     "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error greter than 20")))
# 
# ## To join embedding
# error_df <- error_df |>
#   bind_cols(umap_pbmc_scaled |>
#               select(-ID))
# 
# error_plot_pbmc <- error_df |>
#   ggplot(aes(x = UMAP1,
#              y = UMAP2,
#              color = type,
#              group = ID)) +
#   geom_point(alpha=0.5,
#              size = 0.1) +
#   interior_annotation("b", sc_ltr_pos)

```

```{r}
#| label: pbmc-umap-model-proj

data_pbmc <- training_data_pbmc |> 
  select(-ID) |>
  mutate(type = "data")

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hb_id %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)

# Combine with the true model for visualization
df <- dplyr::bind_rows(scaled_pbmc_data_model |> mutate(type = "model"),
                       scaled_pbmc_data |> mutate(type = "data"))

## First projection
projection <- cbind(
    c(-0.4545,-0.5533,-0.1235,0.3984,-0.2065,-0.3586,-0.0088,0.3572,-0.1368),
    c(0.4576,-0.3157,0.4708,0.1410,-0.4479,-0.1685,0.3668,-0.2692,0.1332))

projection_scaled <- projection * 1.23

# projection_filtered <- projection
# projection_filtered[apply(projection_filtered, 1, function(row) all(row < 0)), ] <- 0.75

#rows_to_keep <- c(1, 2, 4, 5, 6)

#projection_filtered <- projection[rows_to_keep, ]

projected <- as.matrix(scaled_pbmc_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  proj = projection,
  limits = 1,
  axis_pos_x = -0.48,
  axis_pos_y = -0.48,
  axis_labels = names(scaled_pbmc_data),
  threshold = 0.082)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = '#a65628') +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.65, 0.65)) +
  ylim(c(-0.65, 0.65)) +
  interior_annotation("a2", cex = 2)

## Second projection
projection <- cbind(
    c(0.0734,0.6138,-0.4671,0.2589,0.1189,0.0778,-0.0819,0.0308,-0.5561),
    c(0.6625,0.0034,0.2412,0.3360,0.2463,0.1070,-0.2903,-0.4705,0.1293))

projection_scaled <- projection * 1.25

projected <- as.matrix(scaled_pbmc_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  proj = projection,
  limits = 1.35,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75,
  axis_labels = names(scaled_pbmc_data),
  threshold = 0.12)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1,
      y = proj2)) +
  geom_segment(
    data = model_df,
    aes(
      x = proj1_from,
      y = proj2_from,
      xend = proj1_to,
      yend = proj2_to),
    color = "#000000") +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = '#a65628') +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2),
    label=rownames(axes),
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle,
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 0.75)) +
  ylim(c(-1, 0.75)) +
  interior_annotation("a3", cex = 2)

# pbmc_proj_umap_model2 <- ggplot() +
#   geom_segment(
#     data = model_df, 
#     aes(
#       x = proj1_from, 
#       y = proj2_from, 
#       xend = proj1_to, 
#       yend = proj2_to), 
#     color = "#000000") +
#   geom_segment(
#     data=axes, 
#     aes(x=x1, y=y1, xend=x2, yend=y2), 
#     colour="grey70") +
#   geom_text(
#     data=axes,
#     aes(x=x2, y=y2), 
#     label=rownames(axes),
#     colour="grey50",
#     size = 3) +
#   geom_path(
#     data=circle, 
#     aes(x=c1, y=c2), colour="grey70") +
#   coord_fixed() +
#   xlim(c(-0.65, 0.65)) +
#   ylim(c(-0.65, 0.65)) +
#   interior_annotation("a3", cex = 2)

```

<!-- ::: {#fig-pbmc1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/umap_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_1.png) -->

<!-- ![](figures/pbmc3k/sc_2.png) -->

<!-- ![](figures/pbmc3k/sc_3.png) -->

<!-- Model in \gD{}, on the UMAP layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.041)$, $b = 870 \  (30, \ 29)$, $m = 135$, benchmark value to remove large edges is $0.099$) (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/VqqWuE0Jj6A>). -->
<!-- ::: -->
<!--compute absolute error for different parameter choices-->

```{r}
#| label: combine-data-pbmc

error_pbmc_umap <- read_rds("data/pbmc3k/error_pbmc_umap_30_min_dist_0.3.rds")
error_pbmc_umap2 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.01.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_15_min_dist_0.99.rds")
error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_12_min_dist_0.99.rds")
error_pbmc_tsne <- read_rds("data/pbmc3k/error_pbmc_tsne_5.rds")
error_pbmc_tsne2 <- read_rds("data/pbmc3k/error_pbmc_tsne_30.rds")
error_pbmc_phate <- read_rds("data/pbmc3k/error_pbmc_phate_5.rds")
error_pbmc_trimap <- read_rds("data/pbmc3k/error_pbmc_trimap_12_4_3.rds")
error_pbmc_pacmap <- read_rds("data/pbmc3k/error_pbmc_pacmap_30_random_0.9_5.rds")

error_pbmc <- bind_rows(error_pbmc_umap, 
                        error_pbmc_umap2,
                        error_pbmc_umap3,
                        error_pbmc_tsne,
                        error_pbmc_tsne2,
                        error_pbmc_phate,
                        error_pbmc_trimap,
                        error_pbmc_pacmap)

error_pbmc <- error_pbmc |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-pbmc
# read the png file from device 

#pbmc_layouts <- readPNG("figures/pbmc3k/pbmc_nldr_layouts.png", native = TRUE) 

error_plot_pbmc <- ggplot(error_pbmc, 
                          aes(x = a1, 
                              y = MSE, 
                              colour = method)) + 
  geom_point(size = 0.8) +
  geom_line(linewidth = 0.3) + 
  # geom_vline(xintercept = 15, linetype="solid", 
  #            color = "black", linewidth=0.8, alpha = 0.5) +
  scale_x_continuous(breaks = sort(unique(error_pbmc$a1))[c(1, 5, 9, 13, 17, 21, 26)]) +
  scale_color_manual(values=c('#e41a1c','#377eb8','#4daf4a','#ff7f00',
                              '#984ea3','#999999','#a65628','#f781bf')) +
  scale_y_log10() +
  ylab("log(MSE)") +
  xlab(expression(paste("binwidth (", a[1], ")"))) +
  theme_minimal() +
  theme(panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none",
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title.x = element_text(size = 7),
        axis.title.y = element_text(size = 7)) 

```

```{r}
#| fig-cap: "Assessing which of the 8 NLDR layouts on the PBMC3k data  (shown in @fig-NLDR-variety) is the better representation using MSE for varying binwidth ($a_1$). Colour  used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-h). Layout f is universally poor. Layouts a, c, g, h that show large separations between clusters are universally suboptimal. Layout d with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. The choice of best is between layouts b and e, that have small separations between oddly shaped clusters. Layout e is the best choice."
#| label: fig-pbmc-mse
#| fig-pos: H
#| out-height: 100%

free(error_plot_pbmc) + wrap_plots(nldr1c, nldr2c, nldr3c, nldr4c,
           nldr5c, nldr6c, nldr7c, nldr8c, ncol = 2)
```

<!-- The MSE for different binwidths ($a_1$) using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with different (hyper-)parameter settings (@fig-pbmc-mse) were calculated.  -->

<!-- It is found that tSNE (@fig-mnist-mse (a)) provide the most reasonable representation for the digit 1 dataset, showing universally best. However, \gD{} representation shows a big non-linear cluster and a small cluster with a small gap (@fig-tsne-best).  -->

<!-- In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $a_1$ using UMAP, tSNE, PHATE, PaCMAP, and TriMAP with different (hyper-)parameter settings (@fig-pbmc-mse) were calculated. tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)). -->
<!-- The visualization of the *model-in-the-data-space* provides evidence for this (@fig-pbmc2-sc). Therefore, tSNE with perplexity $30$ accurately captures the data structure of the PBMC3k dataset, which is better than UMAP. -->

<!--best choice-->

```{r}
#| label: umap-pbmc-read-best
umap_pbmc2 <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc2)
umap_pbmc_scaled_best <- pbmc_scaled_obj$scaled_nldr

# tsne_pbmc <- tsne_pbmc_scaled |>
#   ggplot(aes(x = tSNE1,
#              y = tSNE2)) +
#   geom_point(alpha=0.3, color = "#000000")
```

<!-- We then fit the model for tSNE, and visualize the resultant model in the $9\text{-}D$ data space. The model shows a quirk, as shown in @fig-pbmc2-sc. All three clusters are connected by an edge except the small and large clusters. Because the clusters are so close in \gD{}, they attempt to maintain the structure in \pD{} as well. This is evident that tSNE with perplexity $30$ provides a reasonable representation of PBMC3k data. -->

<!--Fit the best model and compute error-->
```{r}
#| label: num-bins-pbmc
## Compute hexbin parameters
num_bins_x_pbmc <- 13
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled_best,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  q = 0.1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "pc"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

# tr_from_to_df_pbmc <- tr_from_to_df_pbmc |>
#   filter(!(row_number() %in% c(65, 112, 130)))

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

#benchmark_pbmc <- 0.2

## Remove an edge that exist as long edge in p-D
# tr_from_to_df_pbmc |> mutate(ID = row_number()) |> filter(from == 37) |> filter(to == 43) 

tr_df <- distinct(tibble::tibble(
  x = c(tr_from_to_df_pbmc[["x_from"]], tr_from_to_df_pbmc[["x_to"]]),
  y = c(tr_from_to_df_pbmc[["y_from"]], tr_from_to_df_pbmc[["y_to"]])))

distance_df_small_edges_pbmc <- distance_pbmc |>
  filter(distance < benchmark_pbmc) |>
  filter(!(row_number() %in% c(6, 12, 14, 16, 17, 19, 23, 29, 30, 143, 144, 146, 147, 157)))

tr_from_to_df_pbmc <- inner_join(
  tr_from_to_df_pbmc, distance_df_small_edges_pbmc,
  by = c("from", "to"))

# tr_from_to_df_pbmc <- tr_from_to_df_pbmc |>
#   filter(!(row_number() %in% c(155)))

trimesh_removed_pbmc_best <- ggplot() + 
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  geom_point(
    data = umap_pbmc_scaled_best,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    alpha = 0.3,
    color = "#999999"
  )  +
  interior_annotation("b1", cex = 2) +
  theme(
    aspect.ratio = 1
  )

# trimesh_removed_pbmc <- vis_rmlg_mesh(
#   distance_edges = distance_pbmc,
#   benchmark_value = benchmark_pbmc,
#   tr_coord_df = tr_from_to_df_pbmc,
#   distance_col = "distance") +
#   geom_point(
#     data = tsne_pbmc_scaled,
#     aes(
#       x = tSNE1,
#       y = tSNE2
#     ),
#     alpha = 0.2,
#     color = "#000000"
#   ) 
#   #xlim(sc_xlims) + ylim(sc_ylims) +
#   #interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "pc")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    row_wise_abs_error <= 25 ~ "error 20-25",
    .default = "error greter than 25"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error 20-25", 
    "error greter than 25")))

## To join embedding
error_df <- error_df |>
  bind_cols(umap_pbmc_scaled_best |>
              select(-ID))

error_plot_pbmc_best <- error_df |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

```{r}
#| label: pbmc-umap-model-proj-best

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hb_id %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)

# Combine with the true model for visualization
df <- dplyr::bind_rows(scaled_pbmc_data_model |> mutate(type = "model"),
                       scaled_pbmc_data |> mutate(type = "data"))

## Removed long edge in high-d
# distance_df_small_edges_pbmc <- distance_df_small_edges_pbmc |>
#   filter(!(row_number() %in% c(155)))

## First projection
projection <- cbind(
    c(-0.4545,-0.5533,-0.1235,0.3984,-0.2065,-0.3586,-0.0088,0.3572,-0.1368),
    c(0.4576,-0.3157,0.4708,0.1410,-0.4479,-0.1685,0.3668,-0.2692,0.1332))

projection_scaled <- projection * 1.23

projected <- as.matrix(scaled_pbmc_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  proj = projection,
  limits = 1,
  axis_pos_x = -0.48,
  axis_pos_y = -0.48,
  axis_labels = names(scaled_pbmc_data),
  threshold = 0.082)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model1_best <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = "#999999") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.65, 0.65)) +
  ylim(c(-0.65, 0.65)) +
  interior_annotation("b2", cex = 2)

## Second projection
projection <- cbind(
    c(0.0734,0.6138,-0.4671,0.2589,0.1189,0.0778,-0.0819,0.0308,-0.5561),
    c(0.6625,0.0034,0.2412,0.3360,0.2463,0.1070,-0.2903,-0.4705,0.1293))

projection_scaled <- projection * 1.25

projected <- as.matrix(scaled_pbmc_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  proj = projection,
  limits = 1.35,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75,
  axis_labels = names(scaled_pbmc_data),
  threshold = 0.12)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model2_best <- projected_df |>
  ggplot(
    aes(
      x = proj1,
      y = proj2)) +
  geom_segment(
    data = model_df,
    aes(
      x = proj1_from,
      y = proj2_from,
      xend = proj1_to,
      yend = proj2_to),
    color = "#000000") +
  geom_point(
    #size = 0.5,
    alpha = 0.2,
    color = '#999999') +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2),
    label=rownames(axes),
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle,
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 0.75)) +
  ylim(c(-1, 0.75)) +
  interior_annotation("b3", cex = 2)


```

<!-- <!--bin1 = 15, bin2 = 20, b = 300, non_empty = 136--> 
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| fig-cap: "(a) Model generated in $2\\text{-}D$ with tSNE, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows three well-separated distant clusters. The $p\\text{-}D$ model errors are distributed along clusters, but most low $p\\text{-}D$ model errors present in the large cluster." -->
<!-- #| label: fig-model-pbmc -->
<!-- #| fig-pos: H -->
<!-- #| out-height: 30% -->

<!-- trimesh_removed_pbmc + error_plot_pbmc +  -->
<!--   plot_layout(guides='collect', ncol=2) & -->
<!--   theme(legend.position='bottom') -->
<!-- ``` -->

```{r}
#| echo: false
#| fig-cap: "Model in \\gD{}, on the layout a and b, and two views of the fit in projections from $9\\text{-}D$, for the PBMC3k data ($n =  2622$ and $p = 9$). Layout a shows three-well separated clusters with big separation, while layout b shows close clusters. In $9\\text{-}D$, the data shows three clusters with two of them being close and one separated from the others. This is why b1, b2, and b3 shows a connected edge between the close clusters, as proven by @fig-pbmc-mse as a reasonable layout. Viewing the fitted model in $9\\text{-}D$, helps to see some unobserved patterns of the data: (i) dense points, and (ii) non-linear clusters. Videos of the langevitour animations are available at <https://youtu.be/0cKX_HG_n0k> and <https://youtu.be/KhJvsRtaX04> respectively."
#| label: fig-model-pbmc-author-proj
#| fig-pos: H
#| fig-width: 10
#| fig-height: 15
#| out-height: 80%

free(trimesh_removed_pbmc) + free(trimesh_removed_pbmc_best) +
  pbmc_proj_umap_model1 + pbmc_proj_umap_model1_best +
  pbmc_proj_umap_model2 + pbmc_proj_umap_model2_best +
  plot_layout(nrow=3) &
  theme(legend.position='none')
```

<!-- ::: {#fig-pbmc2-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/tsne_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_4.png) -->

<!-- ![](figures/pbmc3k/sc_5.png) -->

<!-- ![](figures/pbmc3k/sc_6.png) -->

<!-- Model in \gD{}, on the tSNE layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.058)$, $b = 300 \  (15, \ 20)$, $m = 136$, benchmark value to remove large edges is $0.133$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/5Y1hE4i7N2k>). -->
<!-- ::: -->

#### scDEED

```{r}
result_umap <- read_rds(here::here('data/pbmc3k/pbmc_scdeed_umap_results.rds'))

result_umap |>
  kableExtra::kable(format = "latex",
                    booktabs = TRUE,
                    longtable = TRUE,
                    label = "scdeedumap") |>
  kableExtra::kable_styling(latex_options = "scale_down")
```

### Hand-written digits
<!--
- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 
-->

<!--add different NLDR layouts-->

The digit 1 of the MNIST dataset consists of $7877$ grayscale images of handwritten digits [@lecun2010]. Before further analysis, PCA was used to preprocess the data, where the first $10$ principal components, explaining 83% of the total variation, were selected. The objective is to select a reasonable \gD{} layout, representing the non-linear structure of the digit 1 dataset in $10\text{-}D$ (@fig-mnist-tri-proj (a)). 


<!-- @yingfan2021 used this dataset to evaluate how PaCMAP preserves local structures in \gD{}. Because of the large number of images, the focus was specifically on the handwritten digit 1, which exhibits a non-linear structure in \gD{} (@fig-pacmap-author). There are $7877$ images of the digit 1 in the dataset. Additionally, PCA was applied as a preprocessing step and the first $10$ principal components were selected. The objective is to assess whether PaCMAP effectively preserves the non-linear structure in \gD{}.  -->

```{r}
#| label: read-mnist-nldr
# Read a variety of different NLDR representations of mnist
# and plot them on same aspect ratio
tsne_mnist <- read_rds("data/mnist/mnist_tsne30.rds")

nldr_mnist1 <- tsne_mnist |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("a", c(0.08, 0.93)) +
  theme(aspect.ratio = 1)

umap_mnist <- read_rds("data/mnist/mnist_umap.rds")

nldr_mnist2 <- umap_mnist |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("b") +
  theme(aspect.ratio = 1)

phate_mnist <- read_rds("data/mnist/mnist_phate.rds")

nldr_mnist3 <- phate_mnist |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("c", position = c(0.94, 0.92)) +
  theme(aspect.ratio = 1)

trimap_mnist <- read_rds("data/mnist/mnist_trimap.rds")

nldr_mnist4 <- trimap_mnist |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("d", position = c(0.86, 0.92)) +
  theme(aspect.ratio = 1)

pacmap_mnist <- read_rds("data/mnist/mnist_pacmap.rds")

nldr_mnist5 <- pacmap_mnist |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e") +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-mnist

error_mnist_umap <- read_rds("data/mnist/error_mnist_umap.rds")
error_mnist_tsne <- read_rds("data/mnist/error_mnist_tsne.rds")
error_mnist_phate <- read_rds("data/mnist/error_mnist_phate.rds")
error_mnist_trimap <- read_rds("data/mnist/error_mnist_trimap.rds")
error_mnist_pacmap <- read_rds("data/mnist/error_mnist_pacmap.rds")

error_mnist <- bind_rows(error_mnist_umap, 
                        error_mnist_tsne,
                        error_mnist_phate,
                        error_mnist_trimap,
                        error_mnist_pacmap)

error_mnist <- error_mnist |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-mnist

error_plot_mnist <- ggplot(error_mnist, 
                          aes(x = a1, 
                              y = MSE, 
                              colour = method)) + 
  geom_point(size = 0.8) +
  geom_line(linewidth = 0.3) + 
  # geom_vline(xintercept = 15, linetype="solid", 
  #            color = "black", linewidth=0.8, alpha = 0.5) +
  scale_x_continuous(breaks = sort(unique(error_mnist$a1))[append(seq(1, length(unique(error_mnist$a1)), by = 5), 24)]) +
  scale_color_manual(values=c('#e41a1c','#377eb8',"#ff7f00", '#999999','#a65628')) +
  scale_y_log10() +
  ylab("log(MSE)") +
  xlab(expression(paste("binwidth (", a[1], ")"))) +
  theme_minimal() +
  theme(panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none",
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title.x = element_text(size = 7),
        axis.title.y = element_text(size = 7)) 

```


```{r}
#| fig-cap: "Assessing which of the 5 NLDR layouts on the MNIST digit 1 data is the better representation using MSE for varying binwidth ($a_1$). Colour used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-e). All the layouts appear to be very similar. Layout c is universally poor. Layouts a that show two close clusters are universally suboptimal. Layout a is the best choice."
#| label: fig-mnist-mse
#| fig-pos: H
#| out-height: 100%

free(error_plot_mnist) + wrap_plots(nldr_mnist1, nldr_mnist2, nldr_mnist3, 
                                    nldr_mnist4, nldr_mnist5, ncol = 2)
```

<!-- PaCMAP preserves non-linear structure in $784\text{-}D$. To evaluate whether PaCMAP provides a reasonable representation of the data, the \gD{} embedding of the handwritten digit 1 was selected. As shown in @fig-pacmap-author, the angle of the digit 1 images varies along the \gD{} structure. -->

The MSE for different binwidths ($a_1$) using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with default (hyper-)parameter setting (@fig-mnist-mse) were calculated. It is found that tSNE (@fig-mnist-mse (a)) provide the most reasonable representation for the digit 1 dataset, showing universally best. However, \gD{} representation shows a big non-linear cluster and a small cluster with a small gap (@fig-tsne-best). 

In the case of digit 1 data, there should not be any clusters unless anomalies exist, indicating different digit 1 patterns. The angle of the digit 1 images varies along this non-linear clustering structure (@fig-tsne-best), while the small cluster contains the digit 1 images with different patterns of the digit 1, unlike the usual (@fig-model-error-mnist (c)). This provides the evidence for two close clusters. 

By visualizing the model generated for tSNE in $10\text{-}D$ helps to assess the \gD{} layout. As shown in @fig-mnist-tri-proj (a), the model provides the evidence for the non-linear structure of the digit 1 data in $10\text{-}D$. The model shows some quirks. The model's twisted pattern provides evidence for the $10\text{-}D$ data structure, which is not observed by \gD{} layout (@fig-mnist-tri-proj (b)). Furthermore, the presence of long edges indicates the existence of the small cluster located closely together (@fig-mnist-tri-proj (c)).


<!-- PaCMAP layout with author's suggested parameter choice-->
```{r}
#| label: read-mnist
## Import data
training_data_mnist <- read_rds("data/mnist/mnist_10_pcs_of_digit_1.rds")
training_data_mnist <- training_data_mnist |>
  mutate(ID = 1:NROW(training_data_mnist))

data_mnist <- training_data_mnist |>
  select(-ID) |>
  mutate(type = "data")

mnist_scaled_obj <- gen_scaled_data(
  data = tsne_mnist)
tsne_minst_scaled <- mnist_scaled_obj$scaled_nldr 

tsne_plot_mnist <- tsne_minst_scaled |> 
  ggplot(aes(x = tSNE1, 
             y = tSNE2)) + 
  geom_point(alpha=0.1, color = "#000000") 

# read the png file from device
left_top_img <- readPNG("figures/mnist/mnist_right_top_img.png", native = TRUE)
center_img <- readPNG("figures/mnist/mnist_middle_img.png", native = TRUE)
right_bottom_img <- readPNG("figures/mnist/mnist_right_bottom_img.png", native = TRUE)

tsne_plot_mnist <- tsne_plot_mnist +
    inset_element(p = left_top_img,
                  left = 1,
                  bottom = 0.85,
                  right = 0,
                  top = 0.95) +
    inset_element(p = center_img,
                  left = 0.8,
                  bottom = 0.5,
                  right = 0,
                  top = 0.6) +
    inset_element(p = right_bottom_img,
                  left = 1,
                  bottom = 0.2,
                  right = 0,
                  top = 0.3)
```

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->
```{r}
#| fig-cap: "The $\\gD{}$ layout from tSNE applied for the MNIST digit 1 dataset with defualt (hyper-)parameters. We use our model in the $10\\text{-}D$ space to assess whether this is an accurate representation of data structure present in $10\\text{-}D$, or if it is misleading. The angle of the digit 1 images varies along this structure. Images at the top-right of the $\\gD{}$ layout show the digit 1 angled more to the right, while those at the bottom-right show the digit 1 angled more to the left."
#| label: fig-tsne-best
#| fig-pos: H
#| out-height: 22%
#| eval: false

tsne_plot_mnist
```

<!-- Fit the model and compute error-->
```{r}
#| label: hexbin-mnist
## Compute hexbin parameters
num_bins_x_mnist <- 19
lim1 <- mnist_scaled_obj$lim1
lim2 <- mnist_scaled_obj$lim2
r2_mnist <- diff(lim2)/diff(lim1) 

mnist_model <- fit_highd_model(
  training_data = training_data_mnist,
  emb_df = tsne_minst_scaled,
  bin1 = num_bins_x_mnist,
  r2 = r2_mnist,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC"
)

df_bin_centroids_mnist <- mnist_model$df_bin_centroids
df_bin_mnist <- mnist_model$df_bin

## Triangulate bin centroids
tr1_object_mnist <- tri_bin_centroids(
  df_bin_centroids_mnist, x = "c_x", y = "c_y")
tr_from_to_df_mnist <- gen_edges(
  tri_object = tr1_object_mnist)

## Compute 2D distances
distance_mnist <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_mnist, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_mnist <- find_lg_benchmark(
  distance_edges = distance_mnist, 
  distance_col = "distance")

benchmark_mnist <- 0.1

sc_ltr_pos_mnist <- c(0.96, 0.96)

tr_df <- distinct(tibble::tibble(
  x = c(tr_from_to_df_mnist[["x_from"]], tr_from_to_df_mnist[["x_to"]]), 
  y = c(tr_from_to_df_mnist[["y_from"]], tr_from_to_df_mnist[["y_to"]])))

distance_df_small_edges_mnist <- distance_mnist |>
  filter(distance < benchmark_mnist) |>
  filter(!(row_number() %in% c(155, 554, 140)))

tr_from_to_df_mnist <- inner_join(
  tr_from_to_df_mnist, distance_df_small_edges_mnist, 
  by = c("from", "to")) |>
  mutate(ID = row_number())

tsne_minst_scaled_temp <- tsne_minst_scaled |>
  mutate(cluster = if_else((tSNE1 <= 0.92) & (tSNE1 >= 0.76) & (tSNE2 <= 0.62) & (tSNE2 >= 0.44), "small_clust", "big_clust"))

trimesh_removed_mnist <- ggplot() + 
  geom_segment(data = tr_from_to_df_mnist, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_minst_scaled_temp,
             aes(
               x = tSNE1,
               y = tSNE2,
               colour = cluster
             ),
             alpha=0.2)+
  scale_color_manual(values=c("#999999",'#ff7f00')) +
  interior_annotation("a", sc_ltr_pos_mnist) +
  theme(aspect.ratio = 1,
        legend.position = "none") 

# trimesh_removed_mnist <- vis_rmlg_mesh(
#   distance_edges = distance_mnist, 
#   benchmark_value = benchmark_mnist, 
#   tr_coord_df = tr_from_to_df_mnist, 
#   distance_col = "distance") +
#   geom_point(data = pacmap_minst_scaled,
#              aes(
#                x = PaCMAP1,
#                y = PaCMAP2
#              ),
#              alpha=0.5, 
#              size = 0.1) +
#   interior_annotation("a", sc_ltr_pos_mnist)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_mnist, 
  df_bin = df_bin_mnist, 
  training_data = training_data_mnist, 
  newdata = NULL, 
  type_NLDR = "tSNE", 
  col_start = "PC") 

## Categorize error

# error_df <- error_df |>
#   mutate(type = case_when(
#     row_wise_abs_error <= 2 ~ "error 0-2",
#     row_wise_abs_error <= 4 ~ "error 2-4",
#     row_wise_abs_error <= 6 ~ "error 4-6",
#     row_wise_abs_error <= 8 ~ "error 6-8",
#     row_wise_abs_error <= 10 ~ "error 8-10",
#     .default = "error greter than 10"
#   )) |>
#   mutate(type = factor(type, levels = c(
#     "error 0-2", "error 2-4", "error 4-6", "error 6-8", "error 8-10", 
#     "error greter than 10")))

## To join embedding
error_df <- error_df |>
  bind_cols(tsne_minst_scaled |> 
              select(-ID))

## To transform the data
## ggplot(data = error_df, aes(x=row_wise_abs_error)) + geom_density() ## right skewed

error_df <- error_df |>
  mutate(sqrt_row_wise_abs_error = sqrt(row_wise_abs_error))
  
highd_error_2d_mnist <- error_df |>
  ggplot(aes(x = tSNE1,
             y = tSNE2,
             colour = sqrt_row_wise_abs_error)) +
  geom_point(alpha=0.2) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  geom_rect(aes(xmin = 0.8, xmax = 0.9, ymin = 0.45, ymax = 0.6),
               fill = "transparent", color = "#000000", linewidth = 0.7) +
  geom_rect(aes(xmin = 0.75, xmax = 0.85, ymin = 0.8, ymax = 0.95),
               fill = "transparent", color = "#e41a1c", linewidth = 0.7)  +
  geom_rect(aes(xmin = 0.45, xmax = 0.55, ymin = 0.45, ymax = 0.6),
               fill = "transparent", color = "#4daf4a", linewidth = 0.7)  +
  geom_rect(aes(xmin = 0.7, xmax = 0.8, ymin = 0.25, ymax = 0.4),
               fill = "transparent", color = "#984ea3", linewidth = 0.7) 
  # scale_colour_brewer(
  #   type = "seq",
  #   palette = 1
  # ) #+ 
  # theme(
  #     legend.position = "bottom",
  #     legend.text = element_text(size = 10),          # Increase legend text size
  #     legend.key.size = unit(1, "lines")              # Increase size of legend keys
  # ) +
  # guides(
  #     color = guide_legend(
  #         override.aes = list(size = 2.5, alpha = 1)      # Increase point size and opacity in legend
  #     )
  # ) 
```

<!-- As a result of visualizing the *model-in-the-data-space*, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data (@fig-mnist-tri-proj). It also shows some quirks like twisted patterns and long edges (@fig-mnist-tri-proj). Furthermore, the results help to identify anomalies (with large model errors) within and the outside the non-linear structure, displaying different patterns of the digit 1 (@fig-model-error-mnist).  -->

<!-- According to @fig-mnist-tri-proj (b), the non-linear structure observed in the \gD{} layout of PaCMAP (@fig-pacmap-author) is also visible when visualizing the model overlaid on the data space. This indicates that PaCMAP accurately captures the non-linear structure of the $10\text{-}D$ data. Additionally, the model shows a twisted pattern within the non-linear structure in $10\text{-}D$ space (@fig-mnist-tri-proj (c)), which is an additional pattern not visible in the \gD{} representation (@fig-pacmap-author). Furthermore, as shown in @fig-mnist-tri-proj (d), some long edges exist in the $10\text{-}D$ space that are not recognized as long edges in the \gD{} representation. However, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data.  -->

<!--bin1 = 22, bin2 = 17, b = 374, non_empty = 140-->
```{r}
#| label: mnist-model-proj

df_b_mnist <- df_bin_mnist |>
  dplyr::filter(hb_id %in% df_bin_centroids_mnist$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_mnist <- df_b_mnist[match(df_bin_centroids_mnist$hexID, df_b_mnist$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_mnist <- bind_rows(data_mnist, df_b_mnist)
scaled_mnist <- scale_data_manual(df_model_data_mnist, "type") |>
  as_tibble()

scaled_mnist_data <- scaled_mnist |>
  filter(type == "data") |>
  # bind_cols(tsne_minst_scaled_temp) |>
  # select(-type, -tSNE1, -tSNE2, -ID) |>
  select(-type)

# scaled_mnist_data <- bind_cols(scaled_mnist_data,
#                                tsne_minst_scaled |> select(-ID))

scaled_mnist_data_model <- scaled_mnist |>
  filter(type == "model") |>
  select(-type)

# model_2d <- df_bin_centroids_mnist |>
#   select(c_x, c_y) |>
#   rename(c("tSNE1" = "c_x",
#            "tSNE2" = "c_y"))
# 
# scaled_mnist_data_model <- bind_cols(scaled_mnist_data_model, model_2d)


# Combine with the true model for visualization
# df <- dplyr::bind_rows(scaled_mnist_data_model |> mutate(type = "model"),
#                        scaled_mnist_data)


# scaled_mnist_data <- scaled_mnist_data |>
#   select(-type)

## First projection
# projection <- cbind(
#     c(0.71503,0.00502,-0.04370,-0.01811,-0.01103,0.09877,0.06199,0.05396,0.07771,-0.01713),
#     c(0.00587,0.71879,0.03485,0.08284,-0.07534,-0.05420,0.04315,-0.00547,-0.02995,0.01810)) 

projection <- cbind(
    c(-0.09936,0.36550,0.07658,0.29975,0.09085,-0.14009,-0.13417,0.48357,-0.00699,-0.13587),
    c(0.23061,0.19332,-0.11264,0.14797,0.30351,0.14980,0.02785,-0.30209,-0.13540,-0.43284))

projection_scaled <- projection * 2.5

projected <- as.matrix(scaled_mnist_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number()) |>
  mutate(cluster = tsne_minst_scaled_temp$cluster)

projected_model <- as.matrix(scaled_mnist_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 1,
  axis_pos_x = -0.83,
  axis_pos_y = -0.83,
  axis_labels = names(scaled_mnist_data),
  threshold = 0.06)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_tsne_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2,
      colour = cluster)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    #size = 0.2,
    alpha = 0.3) +
  scale_color_manual(values = c("#999999",'#ff7f00')) +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  #coord_fixed() +
  xlim(c(-1, 0.6)) +
  ylim(c(-1, 0.6)) +
  interior_annotation("b") +
  theme(
    legend.position = "none"
  )


## Second projection
projection <- cbind(
    c(-0.33305,0.20223,0.06303,0.11124,0.13299,-0.30093,0.34082,0.15581,-0.08864,0.33573),
    c(0.53615,0.14969,-0.09393,-0.20053,0.07992,-0.34791,0.13057,-0.06814,-0.16737,0.03716)) 

projection_scaled <- projection * 2

projected <- as.matrix(scaled_mnist_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number()) |>
  mutate(cluster = tsne_minst_scaled_temp$cluster)


projected_model <- as.matrix(scaled_mnist_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 1,
  axis_pos_x = -0.83,
  axis_pos_y = -0.83,
  axis_labels = names(scaled_mnist_data),
  threshold = 0.06)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_tsne_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2,
      colour = cluster)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    #size = 0.2,
    alpha = 0.3) +
  scale_color_manual(values = c("#999999",'#ff7f00')) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("c") +
  theme(
    legend.position = "none"
  )


## Third projection
projection <- cbind(
    c(-0.12783,-0.50063,-0.02631,-0.00484,0.17733,0.37157,0.11310,0.09424,0.19844,0.19792),
    c(-0.42542,0.29570,0.31106,-0.30307,0.00614,0.16714,0.21779,-0.00025,0.07020,-0.00689))

projection_scaled <- projection * 1.5

projected <- as.matrix(scaled_mnist_data) %*% projection_scaled

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number()) |>
  mutate(cluster = tsne_minst_scaled_temp$cluster)


projected_model <- as.matrix(scaled_mnist_data_model) %*% projection_scaled

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(  
  proj = projection,
  limits = 1,
  axis_pos_x = -0.83,
  axis_pos_y = -0.83,
  axis_labels = names(scaled_mnist_data),
  threshold = 0.06)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_tsne_model3 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2,
      colour = cluster)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000") +
  geom_point(
    #size = 0.2,
    alpha = 0.3) +
  scale_color_manual(values = c("#999999",'#ff7f00')) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2), 
    label=rownames(axes),
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 0.6)) +
  ylim(c(-1, 0.6)) +
  interior_annotation("d") +
  theme(
    legend.position = "none"
  )

```

<!--add langevitour screenshots and youtube animation link-->
```{r}
#| label: fig-mnist-tri-proj
#| fig-pos: H
#| fig-height: 10
#| fig-width: 10
#| fig-cap: "Model in \\gD{}, on the layout a and three views of the fit in projections from $10\\text{-}D$, for the MNIST digit 1 data ($n =  7877$ and $p = 10$). There is a big non-linear cluster and a small clusterlocated very close to one corner of the big cluster. There is a twisted pattern that is hardly visible in the static plot. Video of the langevitour animation are available at <need to add the link>."

free(trimesh_removed_mnist) + mnist_proj_tsne_model1 +
  mnist_proj_tsne_model2 + mnist_proj_tsne_model3 +
  plot_layout(ncol = 2)

```

<!-- ::: {#fig-mnist1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/mnist/mnist_model_2d.png){#fig-mnist1-model} -->

<!-- ![](figures/mnist/sc_1.png){#fig-mnist1-sc1} -->

<!-- ![](figures/mnist/sc_2.png){#fig-mnist1-sc2} -->

<!-- ![](figures/mnist/sc_3.png){#fig-mnist1-sc3} -->

<!-- Model in \gD{}, on the PaCMAP layout, and three views of the fit in projections from $10\text{-}D$, for the digit 1 of MNIST data ($(s_1, \ s_2) = (-0.100, \ -0.059)$, $b = 374 \  (22, \ 17)$, $m = 140$, benchmark value to remove large edges is $0.094$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/zcg_GXBmqjA>). -->


<!-- ::: -->
<!-- need to update the youtube recording-->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

```{r}
#| label: img-diff-pos

## Data with pixel values
mnist_data <- read_rds("data/mnist/mnist_digit_1.rds")

img_right_top <- c(4795, 259, 6564)

pixels_gathered_within <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_right_top)

right_top_img <- pixels_gathered_within |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Reds 2") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")

img_middle <- c(1069, 4604, 3579)

pixels_gathered_within <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_middle)

middle_img <- pixels_gathered_within |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Greens 3") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")

#img_right_bottom <- c(1695, 5338, 2805)
img_right_bottom <- c(2035, 7710, 2517)

pixels_gathered_within <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_right_bottom)

right_bottom_img <- pixels_gathered_within |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Purples 2") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")

#img_error_outside <- c(3554, 501, 4422, 6860, 3869, 1529, 4916, 6144)
img_error_outside <- c(501, 1529, 6144)

pixels_gathered_outside <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_error_outside)

imge_error_sample_outside <- pixels_gathered_outside |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none") 

```

```{r}
#| label: fig-model-error-mnist
#| fig-cap: "The $10\\text{-}D$ model error in layout a of the MNIST digit 1 dataset show a pattern. Most low model errors are distributed along the big non-linear clutser, while most large model errors are distributed along the small cluster. Along the non-linear cluster, the angle of digit 1 changes. Some images have large errors due to their deviation from the non-linear cluster, which makes them anomalies. The images associated with large model errors shows different patterns of digit 1."
#| fig-pos: H
#| fig-width: 10
#| fig-height: 5
#| out-height: 80%

highd_error_2d_mnist + (imge_error_sample_outside / right_top_img / middle_img / right_bottom_img)
```

<!-- These anomalies can be classified into two types: those that are anomalies within the non-linear cluster and those that lie in the small cluster. The images associated with large model error points within the non-linear cluster display different patterns of the digit 1.  -->

<!-- There are certain data points that exhibit high error rates due to their deviation from the usual $10\text{-}D$ data structure, which makes them anomalies (@fig-model-mnist). These anomalies can be classified into two types: those that are anomalies within the non-linear structure and those that lie outside of it. The images associated with high model error points within the non-linear structure display different patterns of the digit 1, as shown in @fig-mnist1-within. However, when comparing these images to the ones found outside of the non-linear structure, it becomes evident that the latter display different patterns of the digit 1 (@fig-mnist1-out).  -->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

<!-- ::: {#fig-mnist-anomalies layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/mnist/img_error_sample_within.png){#fig-mnist1-within} -->

<!-- ![](figures/mnist/img_error_sample_out.png){#fig-mnist1-out} -->

<!-- Some images of handwritten digit 1 which occur high model error (a) within the non-linear structure, and (b) outside the non-linear structure. The images shows different patterns of digit 1. -->
<!-- ::: -->

## Discussion {#sec-discussion}

<!-- - Summarise contributions -->
<!-- - Explain where it is expected or not expected to work, eg higher dimensional relationships -->
<!-- - Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views (need to add) -->
<!-- - Predicting new observations in $k$-D -->
<!-- - Extending layouts beyond $k$-D, when 2D is clearly inadequate. -->
<!-- - Diagnostic app to explore differences in distances (need to add) -->
<!-- - What might be useful enhancements -->


This study makes several important contributions to the field of NLDR. We have developed an algorithm to evaluate the most useful NLDR method and (hyper-)parameter choices for creating an accurate \gD{} layout of high-dimensional data. Our objective is to fit a model for the \gD{} layout that preserves the relationships between neighboring points and turns it into a high-dimensional wireframe, which can be overlaid on the data and visualized using a tour. This approach is defined as *model-in-data-space*. Viewing a model in the data space is an ideal way to examine the fit.

The effectiveness of this approach is illustrated through various examples. For instance, the two-curvy clusters example demonstrates how the model accurately fits the points, capturing both local and global structures in high-dimensional space. Our simulation case study further, five Gaussian cluster example shows that while all observed NLDR methods preserve the global structure, only tSNE effectively maintains the local structure, highlighting the specific strengths and quirks of different methods.

Human behavior often shows a desire for more certainty and a tendency to prefer well-separated views. This emphasizes the importance of clear and distinct clusters. For example, in the UMAP layout of the **PBMC3k** dataset suggested by @chen2023, three distant, well-separated clusters are shown. However, our model reveals that these clusters are actually close to each other in \pD{}. Additionally, the model discovers non-uniform data distribution and non-linear structures within the clusters that are not visible in the UMAP layout, demonstrating the ability of our model in uncovering hidden data characteristics.

Evaluating the error or unexplained variance is important for assessing how well the model fits the data. By examining the error for different numbers of bins, we found that tSNE with a perplexity of $30$ provides a reasonable representation for the **pbmc** dataset. Connecting the closest clusters with line segments in the fitted model further supports the preservation of neighborhood relationships.

The **digit: 1** example further illustrates the model's ability to accurately capture non-linear structures and provide additional information. Key findings include a twisted pattern that compresses the structure in some projections and long line segments that detect anomalies.

Predicting new observations in \kD{} is particularly valuable due to the limitations of some NLDR methods, like tSNE, which don't provide a straightforward method for prediction. As a result, our approach offers a solution that capable of generating predicted \kD{} embedding regardless of the NLDR method employed, effectively addressing this functional gap.

In conclusion, while our method effectively captures and represents high-dimensional data structures, further enhancements could involve introducing approaches to bind the data, indicate line segments beyond \gD{}, and diagnose the fitted model. These improvements would help in creating a more accurate representation of the data when \gD{} layout is inadequate.

## Supplementary Materials

Appendix: The appendix includes more details about the hexagonal binning algorithm (appendix.pdf, Portable Document Format file).

R package `quollr`: The R package `quollr` containing codes to fit, and visualize the model. (need to add quollr .zip file, GNU zipped tar file)

## Acknowledgments

These `R` packages were used for the work: `tidyverse` (@hadley2019), `png` (@simon2022), `Rtsne` (@jesse2015), `umap` (@tomasz2023), `ggplot2` (@hadley2016), `patchwork` (@thomas2024), `colorspace` (@achim2020), `langevitour` (@harisson2024), `conflicted` (@hadley2023), `reticulate` (@kevin2024), `kableExtra` (@hao2024). These `python` packages were used for the work: `trimap` (@amid2022) and `pacmap` (@yingfan2021). The article was created with `R` packages `rticles` (@jjallaire2024), `knitr` (@yihui2014), and `rmarkdown` (@yihui2020). The project's GitHub repository ([https://github.com/JayaniLakshika/paper-nldr-vis-algorithm](https://github.com/JayaniLakshika/paper-nldr-vis-algorithm)) contains all materials required to reproduce this article.


## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
