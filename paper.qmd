---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional (high-D) data using non-linear transformation. The methods and parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of high-D distributions. To help evaluate the NLDR we have developed an algorithm to show the 2D NLDR model in the high-D space, viewed with a tour. One can see if the model fits everywhere or better in some subspaces, or completely mismatches the data. It is used to evaluate which 2D layout is the best representation of the high-D distribution and see how different methods may have similar summaries or quirks.
  
keywords: [high-dimensional data, dimension reduction, hexagonal binning, low-dimensional manifold, tour, data vizualization, model in the data space]
keywords-formatted: [high-dimensional data, dimension reduction, hexagonal binning, low-dimensional manifold, tour, data vizualization, model in the data space]

bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$\ }
  \newcommand\kD{$k\text{-}D$\ }
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(quollr)
library(dplyr)
# remotes::install_github("jlmelville/snedata")
library(snedata)
library(ggflowchart)
library(purrr) ## map function
library(gridExtra) ## for grid.arrange
library(rsample)
library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)
library(tidyr)

library(Rtsne)
library(uwot)
library(phateR)
library(patchwork)
library(langevitour)
library(colorspace)
library(kableExtra)
library(grid)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "none", 
     legend.title = element_text(size=5), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label) {
  annotation_custom(grid::textGrob(label = label,
      x = unit(0.92, "npc"), y = unit(0.92, "npc"),
      gp = grid::gpar(cex = 1, col="grey70")))
}
```

```{r}
#| label: code-setup
set.seed(20240110)
source("nldr_code.R", local = TRUE)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD) representation of high-dimensional (\pD) data. Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2022], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
# label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr2 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_15_min_dist_0.99.rds")
nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_51.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e")

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f")

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g")

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h")
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Six different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 51), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The paper is organised as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD representations of \pD data have been computed using multidimensional scaling (MDS) [@borg2005], which includes principal components analysis (PCA) [@jolliffe2011] as a special case.  The \kD representation can be considered to be a layout of points in \kD produced by an embedding procedure that maps the data from \pD. In MDS, the \kD layout is constructed by minimizing a stress function that differences distances between points in \pD with potential distances between points in \kD. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the $k-D$ minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: 

- highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6, 
- **stringy branches** (f), and 
- **barely separated clusters** (c, d) which apparently **contradicts** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is 2D, it is also possible in \pD, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of of NLDR as a model fitted to the data might seem strange. It is a model in the sense that it is a "a simplified representation or abstraction of a system, process, or phenomenon in the real world". The \pD observations are the realization of the phenomenon, and the \kD NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD layout to be variance that the model explains, and the (relative) difference with their distances in \pD is the error, or unexplained variance. Abstractly, we can also imagine that the positioning of points in 2D represents fitted values, that will have some prescribed position in \pD that can be compared with the observed value. 

### Notation

```{r}
# Notations used in the paper

notation_df <- read_csv("data/paper_notations.csv")

# Create the table
kable(notation_df, format = "latex", booktabs = TRUE, escape = FALSE, 
      caption = "Notations used in this paper.", label = "tbl-notations") |>
  kable_styling(latex_options = c("HOLD_position"), position = "center", full_width = FALSE) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("5cm", "12cm"))
```

<< XXX Jayani, please insert a table of notation >>

Once we have notation, we need to have some math that gives precision to the paragraph above. (XXX Michael??)

### Constructing the 2D model

### Displaying the model in \pD

### Measuring the fit

### What can be learned

- Overview: Generate a form that maps the model, that is the interpoint distances. What is the model?
- Notation
- Create a representation of the model 
    - using hex-binning in 2D, 
    - parameters, 
    - tuning, 
    - pre-processing
- How does this map to the representation in high-d
    - Centroids, 
    - Edges
- Measuring fit
    - Fitted values
    - Error calculation
- What is learned about simulated examples
    - Interesting organisation of points in UMAP
    - 

## Applications {#sec-applications}

### pbmc

- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use out method to get a better representation
     
### digits: 1

- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 
     
## Discussion {#sec-discussion}

- Summarise contributions
- Explain where it is expected or not expected to work, eg higher dimensional relationships
- Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views
- Diagnostic app to explore differences in distances
- What might be useful enhancements

  
## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
