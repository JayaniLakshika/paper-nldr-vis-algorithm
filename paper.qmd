---
title: "Visualising How Non-linear Dimension Reduction Warps Your Data"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-Linear Dimension Reduction (NLDR) techniques have emerged as powerful tools to visualize high-dimensional data. However, their complexity and parameter choices may lead to distrustful or misleading results. To address this challenge, we propose a novel approach that combines the tour technique with a low-dimensional manifold generated using NLDR techniques, hexagonal binning, and triangulation. This integration enables a clear examination of the low-dimensional representation in the original high-dimensional space. Our approach not only preserves the advantages of both tours and NLDR but also offers a more intuitive perception of complex data structures and facilitates accurate data transformation assessments. The method and example data sets are available in the **quollr** R package.
  
keywords: [high-dimensional data, dimension reduction, triangulation, hexagonal binning, low-dimensional manifold, manifold learning, tour, data vizualization]
keywords-formatted: [high-dimensional data, dimension reduction, triangulation, hexagonal binning, low-dimensional manifold, manifold learning, tour, data vizualization]

bibliography: bibliography.bib  
---

```{r}
#| warning: false
#| echo: false
library(dplyr)
# remotes::install_github("jlmelville/snedata")
library(snedata)
library(ggflowchart)
library(purrr) ## map function
library(gridExtra) ## for grid.arrange
library(rsample)
library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)

library(Rtsne)
library(umap)
library(phateR)
library(patchwork)
library(langevitour)
library(colorspace)

library(grid)


# install.packages("Seurat")
# remotes::install_github("satijalab/seurat-data")
#library(SeuratData) ## For the application

# use_python("~/miniforge3/envs/pcamp_env/bin/python")
# use_condaenv("pcamp_env")
# 
# reticulate::source_python(paste0(here::here(), "/examples_for_the_paper/function_scripts/Fit_PacMAP_code.py"))
# reticulate::source_python(paste0(here::here(), "/examples_for_the_paper/function_scripts/Fit_TriMAP_code.py"))

set.seed(20240110)

source("quollr_code.R", local = TRUE)
source("nldr_code.R", local = TRUE)
```

```{=html}
<!-- 
Notes
* Use American spelling
-->
```

## Introduction {#sec-intro}

High-dimensional (high-D) data is widespread in many fields including ecology and bioinformatics [@Guo2023], in part because of new data collection technologies [@Johnstone2009; @ayesha2020overview]. Working with high-dimensional data poses considerable challenges due to the difficulty in visualizing beyond two dimensions [@Jia2022]. High-dimensional data also presents difficulties for model fitting [@Johnstone2009], both computationally and interpretation, each of which benefits from visualization.

To create visual representations of high-dimensional data, it is common to apply dimension reduction techniques. Linear methods such as principal component analysis (PCA) [@Karl1901] have been used for many years, and remain popular. Non-linear methods such as multi-dimensional scaling (MDS) [@Torgerson1967] have also been routinely used. In the past decade, there has merged many new techniques non-linear dimension reduction (NLDR), such as t-distributed stochastic neighbor embedding (tSNE) [@Laurens2008], uniform manifold approximation and projection (UMAP) [@Leland2018], designed to capture the complex and non-linear relationships present within high-dimensional data [@Johnstone2009].

However, projecting high-dimensional data has limitations, such as information loss and potential distortion of essential structures and patterns [@Jia2022, @article53]. The choice of technique and parameters further impacts the accuracy of the visualization, necessitating careful consideration for meaningful interpretation (see @fig-nldervis).

Interactive and dynamic graphics systems have also been developed over the years to enable visualizing high dimensions. One method, called a tour [@Asimov1985], shows a sequence of linear projections is shown as a movie, allowing exploration without warping the space [@lee2021review]. Interactive tools like **XGobi** and **GGobi** have been successful in incorporating tours for exploring high-dimensional data [@article60]. The R package **tourr** [@article61] further enhances tour visualization within R, although it may face limitations in frame rate and interactive features compared to **GGobi**.

To overcome these limitations, the R package **detourr** [@article22] has been developed, leveraging a Javascript widget via htmlwidgets [@Ramnath2023] to achieve higher frame rates and enhanced interactivity. Additionally, the R package **langevitour** [@article09] utilizes Langevin Dynamics to generate a continuous path of projections, eliminating the need for interpolation between projections for animation. The tour technique has proven valuable in exploring statistical model fits [@article58] and factorial experimental designs [@article59]. Augmenting the results of non-linear dimensional reduction methods with the tour, as demonstrated in the **liminal** R package [@article21], further enhances data exploration.

While tours [@Asimov1985] preserve space without warping [@lee2021review], they require integrating multiple low-dimensional views mentally to perceive high-dimensional structures. To address this challenge, we propose a novel approach by combining the tour technique with a low-dimensional manifold. This manifold is created through the synergistic use of Non-Linear Dimension Reduction (NLDR) techniques, hexagonal binning, and triangulation. By merging these techniques, our approach offers a comprehensive and efficient means to visualize and explore high-dimensional data while retaining the advantages of both tours and NLDR. This integration facilitates a more intuitive perception of complex data structures and empowers analysts with a robust tool for assessing the accuracy of data transformations. The implementation of our approach is available as an R package called **quollr**.

The outline of this paper is as follows. The @sec-background provides an detailed overview of dimension reduction methods, triangulation, and tours. Building upon this foundation, the @sec-methods delves into the proposed algorithm, **quollr**, and its implementation details. In @sec-prediction, discusses the effectiveness of the learned low-dimensional manifold in accurately representing the complex high-dimensional data. Following that, @sec-simpleex presents simple examples from simulations to illustrate the functionality of the algorithm. Subsequently, @sec-applications showcases real-world applications of **quollr** on different data sets, particularly in single-cell RNA-seq data. These applications reveal insights into the performance and trustworthiness of NLDR algorithms. We analyze the results to identify situations where NLDR techniques may lead to misleading interpretations. Finally, @sec-conclusions concludes by summarizing the findings and emphasizing the significance of the proposed approach in tackling the challenges of high-dimensional data visualization.

## Background {#sec-background}

### Dimension Reduction

Consider the high-dimensional data a rectangular matrix $X$, where $X = \begin{bmatrix} \textbf{x}_{1} & \textbf{x}_{2} & \cdots & \textbf{x}_{n}\\ \end{bmatrix}$, with $n$ observations in $p$ dimensions. The objective is to discover a low-dimensional projection $Y = \begin{bmatrix} \textbf{y}_{1} & \textbf{y}_{2} & \cdots & \textbf{y}_{n}\\ \end{bmatrix}$, represented as an $n$ × $d$ matrix, where $d \ll p$. The reduction process seeks to remove noise from the original data set while retaining essential information.

There are two main categories of dimension reduction techniques: linear and non-linear methods. Linear techniques involve a linear transformation of the data, with one popular example being PCA. PCA performs an eigen-decomposition of the sample covariance matrix to obtain orthogonal principal components that capture the variance of the data [@Karl1901]. However, linear methods may not fully capture complex non-linear relationships present in the data.

In contrast, NLDR techniques generate the low-dimensional representation $Y$ from the high-dimensional data $X$, often using pre-processing techniques like $k$-nearest neighbors graph or kernel transformations. Multidimensional Scaling (MDS) is a class of NLDR methods that aims to construct an embedding $Y$ in a low-dimensional space, approximating the pair-wise distances in $X$ [@Torgerson1967]. Variants of MDS include non-metric scaling [@article62] and Isomap, which estimate geodesic distances to create the low-dimensional representation [@article63]. Other approaches based on diffusion processes, like diffusion maps [@article64] and the PHATE algorithm [@article03], also fall under NLDR methods.

A challenge with NLDR methods is selecting and tuning appropriate parameters. One specific technique we focus on is Pairwise Controlled Manifold Approximation (PaCMAP). Similar considerations apply to related methods like tSNE [@Laurens2008], UMAP [@Leland2018], and TrMAP [@article02].

It is important to note that methods like PCA and auto-encoders [@article65] provide a reverse mapping from the low-dimensional space back to the high-dimensional space, enabling data reconstruction. However, many non-linear methods, including tSNE, prioritize visualization and exploration over reconstruction. Their focus is on capturing complex structures that may not be easily represented in the original space, making a straightforward reverse mapping challenging.

#### Non-linear dimension reduction techniques

Non-linear dimension reduction techniques play a crucial role in the analysis and visualization of high-dimensional data, where the complexities of relationships among variables may not be adequately captured by linear methods. Among these techniques, tSNE stands out for its emphasis on preserving pairwise distances. By minimizing the divergence between probability distributions in both the high and low-dimensional spaces, t-SNE effectively reveals intricate structures and patterns within the data. Its application is widespread in tasks requiring the visualization of clusters and local relationships, though it does require careful consideration of the perplexity parameter for optimal results.

UMAP is another powerful non-linear technique that strikes a balance between preserving local and global structures. Constructing a fuzzy topological representation using a weighted k-nearest neighbors graph, UMAP optimizes the low-dimensional embedding to resemble this representation. Known for its efficiency and scalability, UMAP is versatile across various scales of relationships in the data, although parameter sensitivity, particularly concerning the choice of neighbors, must be taken into account.

For trajectory data, PHATE provides specialized capabilities. It models the affinity between data points, simulating a heat diffusion process to capture developmental processes, particularly in single-cell genomics. While PHATE excels in revealing trajectory structures and offering insights into cellular development, it necessitates careful parameter tuning due to its specialized nature.

TriMAP adopts a unique approach by approximating the data manifold through the construction of a triangulated graph representation. This technique efficiently captures both global and local structures by representing the data as a network of triangles. TriMAP's strength lies in its ability to efficiently capture complex structures, albeit with sensitivity to parameter choices, including the number of neighbors.

In contrast, PaCMAP introduces supervised learning to create a low-dimensional representation while preserving pair-wise relationships. Constructing a graph based on pair-wise distances, PaCMAP optimizes an embedding using a customizable loss function. Particularly notable is PaCMAP's flexibility in incorporating class labels or additional information to guide the embedding process, offering users a means to customize its behavior and performance.


```{r}
#| warning: false
#| echo: false

data <- read_csv(paste0(here::here(), "/data/s_curve.csv"))
```

```{r}
#| warning: false
#| echo: false

# data <- data |>
#   mutate(ID = row_number())
# 
# data_split <- initial_split(data)
# training_data <- training(data_split) |>
#   arrange(ID)
# test_data <- testing(data_split) |>
#   arrange(ID)
training_data <- read_rds("data/s_curve/s_curve_training.rds")
test_data <- read_rds("data/s_curve/s_curve_test.rds")
```

```{r}
#| warning: false
#| echo: false

#plot_list <- list()

tSNE_data <- read_rds("data/s_curve/s_curve_tsne_27.rds")
#(perplexity: ", calculate_effective_perplexity(data), ")
plot_list1 <- plot_tSNE_2D(tSNE_data) + #ggtitle("(a)") + 
  theme_linedraw() +
    theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

UMAP_data <- read_rds(file = "data/s_curve/s_curve_umap.rds")

#(n-neighbors: 50)
plot_list2 <- plot_UMAP_2D(UMAP_data) + #ggtitle("(b)") + 
  theme_linedraw() +
    theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```


```{r}
#| warning: false
#| echo: false

#(knn: 5)
# PHATE_data <- Fit_PHATE(training_data, knn = 5, with_seed = 20240110)
# write_csv(PHATE_data, paste0(here::here(), "/data/phate_data_s_curve.csv"))

PHATE_data <- read_rds(file = "data/s_curve/s_curve_phate.rds")

plot_list3 <- plot_PHATE_2D(PHATE_data) + #ggtitle("(c)") + 
  theme_linedraw() +
    theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

# tem_dir <- tempdir()
# 
# Fit_TriMAP_data(training_data, tem_dir)
# 
# path <- file.path(tem_dir, "df_2_without_class.csv")
# path2 <- file.path(tem_dir, "dataset_3_TriMAP_values.csv")
# 
# Fit_TriMAP(as.integer(2), as.integer(5), as.integer(4), as.integer(3), path, path2)
# 
# TriMAP_data <- read_csv(path2)
# write_csv(TriMAP_data, paste0(here::here(), "/data/trimap_data_s_curve.csv"))

TriMAP_data <- read_rds(file = "data/s_curve/s_curve_trimap.rds")

#(n-inliers: 5, \n n-outliers: 4, n-random: 3)
plot_list4 <- plot_TriMAP_2D(TriMAP_data) + #ggtitle("(d)") + 
  theme_linedraw() +
    theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

```


```{r}
#| warning: false
#| echo: false

# tem_dir <- tempdir()
# 
# Fit_PacMAP_data(training_data, tem_dir)
# 
# path <- file.path(tem_dir, "df_2_without_class.csv")
# path2 <- file.path(tem_dir, "dataset_3_PaCMAP_values.csv")
# 
# Fit_PaCMAP(as.integer(2), as.integer(10), "random", 0.9, as.integer(2), path, path2)
# 
# PacMAP_data <- read_csv(path2)
# write_csv(PacMAP_data, paste0(here::here(), "/data/pacmap_data_s_curve.csv"))

PaCMAP_data <- read_rds(file = "data/s_curve/s_curve_pacmap.rds")

#(knn: 10, init: random, \n MN-ratio: 0.9, FP-ratio: 2)
plot_list5 <- plot_PaCMAP_2D(PaCMAP_data) + #ggtitle("(e)") + 
  theme_linedraw() +
    theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied the same data: (a) tSNE (perplexity = 27), (b) UMAP (n_neighbors = 50), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis
#| out-width: 100%

plot_list1 + plot_list2 + plot_list3 + plot_list4 + plot_list5 +
  plot_layout(ncol=5, widths=c(0.35, 0.15, 0.30, 0.12, 0.11))
```


### Linear overviews using tours

A tour is a powerful visualization technique used to explore high-dimensional data by generating a sequence of projections, typically into two dimensions. There are two main types of tours: the Grand Tour and the Guided Tour. The Grand Tour explores the data's shape and global structure by using random projections [@Asimov1985]. In contrast, the Guided Tour focuses on specific patterns by moving towards interesting projections defined by a predefined index function [@article29].

The process begins with a real data matrix $X$ containing $n$ observations in $p$ dimensions. It generates a sequence of $p$ × $d$ orthonormal projection matrices (bases), usually 1 or 2 dimensions. For each pair of orthonormal bases $A_t$ and $A_{t+1}$, a geodesic path is interpolated to create smooth animation between projections.

In the Grand Tour, new orthonormal bases are randomly chosen to explore the $d$-dimensional subspace. The data is often sphered via principal components to reduce dimensionality. The Guided Tour uses a predefined index function to generate a sequence of 'interesting' projections. The resulting tour continuously visualizes the projected data $Y_t$ = $XA_t$ as it interpolates between successive bases.

While both tours can be used to visualize data, examples often focus on using the Grand Tour to observe global structures. However, software like **langevitour** can visualize both types of tours, providing flexibility for exploring high-dimensional data with various objectives.

## Methodology {#sec-methods}

<!--In this paper, we introduce a novel approach, which is designed to determine which method, and which parameter choice provide the most useful representation of high dimensional data. Our approach involves dividing the data set into two subsets: a training set, which serves to construct the low-dimensional manifold, and a test set, utilized to generate predictive values and residuals.

For the implementation of our approach, a 2D embedding data set serves as the initiation point for the subsequent analysis and visualization (see @fig-nldervis (b)).-->

<!--### Overview {#sec-algorithm}-->

Our algorithm comprises two main phases: (1) generate the model in the 2D space, and (2) generate the model in the high-D space. These two phases are described in details in this section.

![A flow diagram detailing the steps taken to create the low-dimensional manifold in the high dimensional space. There are two basic phases, one to generate the model in the 2D space, and other to generate the model in the high-D space.](figures/workflow.png){#fig-meth fig-align="center" width="100%" height="100%"}

### Preprocessing steps

To tackle the complexities and noise in high-dimensional data, we apply PCA as a pre-processing step [@article67, @article68, @article69]. This step helps in noise reduction by identifying principal components that represent directions of maximum variance, capturing essential patterns in the data.

### Constructing the 2D model

**Step 1: Computing the hexagonal grid configuration**

Hexagonal binning is a data visualization technique that aggregates high-dimensional data into a two-dimensional representation using hexagonal regions called bins [@article66]. The hexagonal shape is preferred due to its circular-like appearance, providing smoother transitions between neighboring bins and efficient data aggregation. This technique is especially useful for handling large data sets as it reduces visual clutter while capturing the underlying data distribution effectively. By considering the data distribution, important features are captured, and gaps between bins are minimized, leading to a more accurate representation of the data. In our algorithm, hexagonal binning is utilized to create a low-dimensional manifold.

**(a) Determine the number of bins along the x-axis** ($b_1$)

The first step involves to determine the optimal number of bins ($b$) for creating regular hexagons in the hexagonal grid. To achieve this, we rely on the relationship between the diameter ($h$) and the area ($A$) of regular hexagons, as given by @eq-equation3.

$$
 \text{A} = \frac{\sqrt{3}}{2}h^2
$$ {#eq-equation3}

To capture the data's structure effectively, we consider the optimal $A = 1$ (see @fig-binsize). Using @eq-equation4, we then calculate $h$ of a regular hexagon.

$$
 \text{h} = \sqrt{\frac{2}{\sqrt{3}}A}
$$ {#eq-equation4}

Next, we refer the `hexbin` function in R, which provides a relationship between the diameter of the hexagon ($h$) and the number of bins along the x-axis ($b_1$) (@eq-equation6).

$$
 h = \frac{r_1}{b_1}
$$ {#eq-equation6}

By utilizing the calculated $h$ and the range of the non-linear projection component 1 ($r_1$), we determine the $b_1$ (@eq-equation5). The result is rounded up to the nearest whole number to ensure an appropriate number of bins that capture the variability of the data along the x-axis.

$$
 b_1 = \frac{r_1}{h}
$$ {#eq-equation5}

**(b) Determine the effective shape parameter** ($s$)

In this step, we determine the optimal shape parameter ($s$) for the hexagonal bins, which significantly influences their shape and arrangement within the grid. The shape parameter ($s$) in the hexagonal binning algorithm is defined as the ratio of the height ($y$) to the width ($x$) of the plotting region, referring to the `hexbin` function in R (see @eq-equation1). It determines the shape of the plotting regions and plays a vital role in generating an appropriate hexagonal grid for the data visualization.

$$
  s = \frac{y}{x}
$$ {#eq-equation1}

To calculate the effective shape parameter ($s$) for our algorithm, we consider the ranges of the non-linear projection components. Denoting the range of the non-linear projection component 1 as $r_1$ and the range of the non-linear projection component 2 as $r_2$, we find the shape parameter using @eq-equation2.

$$
s = \frac{r_2}{r_1}
$$ {#eq-equation2}


<!--first value-->

```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop1 <- read_rds("data/s_curve/s_curve_hex_3.rds") 

p1 <-  ggplot(data = hex_full_count_df_loop1, aes(x = x, y = y)) +
      geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
            geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
      scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  xlim(-5, 7) +
  ylim(-10, 10) +
        theme_void() +
        theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
              axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
              legend.title = element_text(size=8), #change legend title font size
              legend.text = element_text(size=6)) +
        guides(fill = guide_colourbar(title = "Standardized count")) +
        annotate(geom = 'text', label = "a", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```


```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop2 <- read_rds("data/s_curve/s_curve_hex_11.rds") 

p2 <-  ggplot(data = hex_full_count_df_loop2, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
    geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
    xlim(-5, 8) +
  ylim(-10, 10) +
        theme_void() +
    theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
          legend.title = element_text(size=8), #change legend title font size
          legend.text = element_text(size=6)) +
    guides(fill = guide_colourbar(title = "Standardized count")) +
    annotate(geom = 'text', label = "b", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```

```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop3 <- read_rds("data/s_curve/s_curve_hex_20.rds") 

p3 <-  ggplot(data = hex_full_count_df_loop3, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
        geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
        theme_void() +
    theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
          legend.title = element_text(size=8), #change legend title font size
          legend.text = element_text(size=6)) +
    guides(fill = guide_colourbar(title = "Standardized count")) +
    annotate(geom = 'text', label = "c", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```

```{r}
#| echo: false
#| label: fig-binsize
#| fig-pos: H
#| fig-cap: "Hexbin plots from different number of bins for the same **s_curve_noise_umap** data: (a) b = (4, 8), s = 1.643542, (b) b = (12, 22), s = 1.643542, and (c) b = (21, 40), s = 1.643542. The hexbins are colored based on the density of points, with yellow indicating higher point density and darker colors representing lower point density within each bin. Does  a value of number of bins exist to effectively represent the low-dimensional data?"

p1 + p2 + p3
```

**Step 2: Obtain bin centroids**

After computing hexagonal grid configurations, the 2D emdeddings are binned into hexagon cells. As a result of hexagonal binning , the bin centroids ($C_k^{(2)} \equiv (C_{kx}, C_{ky}$)) (see @fig-meth (Step 2)) are obtained.

**Step 3: Triangulate bin centroids**

In this step, the algorithm proceeds to triangulate the centroids (see @fig-meth (Step 3)) of the hex bins. Triangulation is a fundamental process in computational geometry and computer graphics that involves dividing a set of points in a given space into interconnected triangles [@article30]. One common algorithm used for triangulation is Delaunay triangulation [@article26], where points are connected in a way that maximizes the minimum angles of the resulting triangles, leading to a more regular and well-conditioned triangulation. In our algorithm, triangulation helps us identify geometric relationships and patterns in the data.

Since we are working with the centroids of regular hexagonal bins, the resulting mesh will predominantly comprise equal-sized regular triangles. However, the triangulation also helps span any gaps that may exist between clusters of points, allowing for a more complete and interconnected representation of the data.

### Lifting the model into high dimensions

Creating the model in the high-D space involves a two-step process. Initially, we identify observations within each hexagonal bin. After this, we calculate the average of the high-dimensional data within each hex bin. The resulting values represent the averaged high-dimensional data points ($C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p}$)). Essentially, these averaged high-dimensional data points serve as the representation of the hex bin centroids in the high-D space. 

```{r}
#| echo: false

# UMAP_fit <- umap(training_data |> dplyr::select(-ID), n_neighbors = 50, n_components =  2)
# 
# UMAP_data <- UMAP_fit$layout |>
#   as.data.frame()
# names(UMAP_data)[1:(ncol(UMAP_data))] <- paste0(rep("UMAP",(ncol(UMAP_data))), 1:(ncol(UMAP_data)))
# 
# UMAP_data <- UMAP_data |>
#   mutate(ID = training_data$ID)

num_bins_x <- calculate_effective_x_bins(.data = UMAP_data, x = UMAP1,
                           cell_area = 1)

shape_value <- calculate_effective_shape_value(.data = UMAP_data, 
                                             x = UMAP1, y = UMAP2)

## To extract bin centroids
hexbin_data_object <-extract_hexbin_centroids(nldr_df = UMAP_data, num_bins = num_bins_x, shape_val = shape_value)
    
df_bin_centroids <- hexbin_data_object$hexdf_data

# ##########
# 
# ## Data set with all possible centroids in the hexagonal grid
# 
# full_centroid_df <- generate_full_grid_centroids(df_bin_centroids)
# 
# ## To map hexID to hexbin centroids in the full grid
# 
# vec1 <- stats::setNames(rep("", 2), c("x", "y"))  ## Define column names
# 
# full_grid_with_hexbin_id <- dplyr::bind_rows(vec1)[0, ]
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::mutate_if(is.character, as.numeric)
# 
# for(i in 1:length(sort(unique(full_centroid_df$y)))){
#   
#   ## Filter the data set with specific y value
#   specific_y_val_df <- full_centroid_df |>
#     dplyr::filter(y == sort(unique(full_centroid_df$y))[i])
#   
#   ordered_x_df <- specific_y_val_df |>
#     dplyr::arrange(x) 
#   
#   full_grid_with_hexbin_id <- dplyr::bind_rows(full_grid_with_hexbin_id, ordered_x_df)
#   
# }
# 
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::mutate(hexID = row_number())
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::rename("c_x" = "x",
#          "c_y" = "y") 
# 
# full_grid_with_hexbin_id <- dplyr::full_join(full_grid_with_hexbin_id, df_bin_centroids, by = c("hexID" = "hexID")) |>
#   dplyr::select(-c(x, y)) #|> 
#   #dplyr::mutate(counts = tidyr::replace_na(counts, 0))
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#     dplyr::mutate(std_counts = counts/max(counts, na.rm = TRUE))
# 
# ## Generate all coordinates of hexagons
# hex_grid <- full_hex_grid(full_centroid_df)
# 
# full_grid_with_polygon_id_df <- map_polygon_id(full_grid_with_hexbin_id, hex_grid)
# 
# full_grid_with_hexbin_id_rep <- full_grid_with_polygon_id_df |>
#   dplyr::slice(rep(1:n(), each = 6)) |>
#   dplyr::arrange(polygon_id)

hex_full_count_df <- read_rds("data/s_curve/s_curve_hex_11.rds") 

##########

min_std_cell_threshold <- 0.05

df_bin_centroids_all <- hexbin_data_object$hexdf_data ## All the centroids without removing low-density hexagons


df_bin_centroids <- df_bin_centroids |>
  dplyr::mutate(stand_cell_count = counts/max(counts)) |>
  dplyr::filter(stand_cell_count > min_std_cell_threshold)
        
UMAP_data_with_hb_id <- UMAP_data |> 
  dplyr::mutate(hb_id = hexbin_data_object$hb_data@cID)
        
## To generate a data set with high-D and 2D training data
df_all <- dplyr::bind_cols(training_data |> dplyr::select(-ID), UMAP_data_with_hb_id)
        
## Averaged on high-D
df_bin <- avg_highD_data(.data = df_all)

## Triangulate bin centroids
tr1_object <- triangulate_bin_centroids(df_bin_centroids, x, y)
tr_from_to_df <- generate_edge_info(triangular_object = tr1_object)

## Compute 2D distances
distance <- cal_2D_dist(.data = tr_from_to_df)

## To find the benchmark value
benchmark <- find_benchmark_value(.data = distance, distance_col = distance)


trimesh <- ggplot(df_bin_centroids, aes(x = x, y = y)) + 
    geom_segment(data = tr_from_to_df, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
    geom_point(size = 2, colour = "#33a02c") +
    coord_equal()
  
  # ggplot(df_bin_centroids, aes(x = x, y = y)) + 
  # geom_point(size = 1, colour = "#33a02c") + 
  # geom_trimesh() + 
  # coord_equal() 

trimesh <- trimesh +
  #ggtitle("(a)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
    theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
    ) 
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7))

trimesh_gr <- colour_long_edges(.data = distance, benchmark_value = benchmark, 
                                triangular_object = tr1_object, distance_col = distance) 

trimesh_gr <- trimesh_gr + 
    geom_point(size = 2, colour = "#33a02c") + 
  #ggtitle("(b)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
    #coord_equal() +
    theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
    ) 

trimesh_removed <- remove_long_edges(.data = distance, benchmark_value = benchmark, 
                                triangular_object = tr1_object, distance_col = distance)

trimesh_removed <- trimesh_removed + 
       geom_point(size = 2, colour = "#33a02c") + 
  #ggtitle("(b)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
    #coord_equal() +
    theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
    ) 



tour1 <- show_langevitour(df_all, df_bin, df_bin_centroids, benchmark_value = benchmark, distance = distance, distance_col = distance)
```

```{r}
#| echo: false


## To plot the distribution of distance
plot_dist <- function(distance_df){
  distance_df$group <- "1"
  dist_plot <- ggplot(distance_df, aes(x = group, y = distance)) +
    geom_quasirandom()+
    ylim(0, max(unlist(distance_df$distance))+ 0.5) + coord_flip()
  return(dist_plot)
}
```

```{r}
#| echo: false

distance_plot <- plot_dist(distance) +
  #ggtitle("(b)" ) + 
  ylab(expression(d^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 12))
```



```{r}
#| echo: false
# distance |> dplyr::filter(distance == max(distance))
# df_bin_centroids |> dplyr::filter(row_number() %in% c(10, 65))

a1 <- colour_long_edges(.data = distance, benchmark_value = 15, 
                  triangular_object = tr1_object, distance_col = distance) + 
    geom_point(size = 2, colour = "#33a02c") + 
    #ggtitle("(b)") + 
    xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
    theme_light() +
    #coord_equal() +
    theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
    ) 

UMAP_data_with_hb_id_selected <- UMAP_data_with_hb_id |> dplyr::filter(hb_id %in% c(31, 239)) |>
  dplyr::mutate(select_point = if_else(hb_id == 31, "data1", "data2"))

a2 <- ggplot(data = hex_full_count_df, aes(x = x, y = y)) + 
  geom_polygon(color = "black", aes(group = polygon_id), fill = "#ffffff") +
  geom_point(data = UMAP_data_with_hb_id_selected, aes(x = UMAP1, y = UMAP2, colour = select_point), alpha = 0.5) +
  scale_color_manual(values=c("#e6550d", "#000000")) +
  geom_point(data = df_bin_centroids_all |> dplyr::filter(hexID %in% c(31, 239)), aes(x = x, y = y), colour = "#33a02c") +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  theme_light() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
          axis.title.x = element_blank(), axis.title.y = element_blank(),
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
    ) 


## Script to make coloring data points with a line

df1 <- df_all |> 
  dplyr::filter(hb_id == "31") |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data1") ## original dataset

df2 <- df_all |> 
  dplyr::filter(hb_id == "239") |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data2") ## original dataset

df3 <- df_all |> 
  dplyr::filter(!(hb_id %in% c("31", "239"))) |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin |> 
  dplyr::filter(hb_id %in% df_bin_centroids$hexID) |>
  dplyr::select(-hb_id) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

df_exe <- dplyr::bind_rows(df_b, df1, df2, df3)

distance_df_small_edges <- distance |> 
  dplyr::filter(from == 10) |>
  dplyr::filter(to == 65) |>
  head(1)

lg_high <- langevitour::langevitour(df_exe[1:(length(df_exe)-1)], group = df_exe$type, pointSize = 3, levelColors = c("#6a3d9a", "#e6550d", "#000000", "#33a02c"), lineFrom = distance_df_small_edges$from, lineTo = distance_df_small_edges$to, lineColors = "#e31a1c")

```

```{r}
#| warning: false
#| echo: false
#| label: fig-wkhighD
#| fig-cap: How the 2D model lift into high dimensions? (a) visualize the points and the hexagonal bin centroids related 31th and 239th hexagons, (b) visualization of the edge connected the 31th and 239th hexagons (colored in red) in the triangular mesh. A video of tour animation is available at (need to add the link of lg_high).

a2 + a1 +
  plot_annotation(tag_levels = 'a') +
  plot_layout(ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
```

### Tunning the model

In our model tuning process, we strategically adjust three key parameters to optimize the performance and accuracy of our approach. They are (i) the total number of bins ($b$), (ii) a benchmark value to remove low-density hexagons, and (iii) a benchmark value to remove long edges.

### Total number of bins

The total number of bins is a crucial parameter that defines the granularity of our hexagonal grid. This parameter particularly influential in capturing the intricacies of the data structure. It is determined by multiplying the number of bins along the x-axis (\(b_1\)) with the number of bins along the y-axis (\(b_2\)), as per the formula:

$$
b = b_1 \times b_2
$${#eq-equationp1}

Here, \(b\) represents the total number of bins. Adjusting the parameter \(b_1\) provides control over the total bin count (\(b\)), allowing us to fine-tune the grid's configuration along the x-axis based on specific requirements.

### Benchmark value to remove low-density hexagons

Addressing low-density hexagons is a systematic process to handle sparsely represented data in certain regions. For each hex bin, we identify the six nearest hex bins using an equal 2D distance metric. Then, we calculate the mean density, as outlined in the equations:

$$
\text{standard count} = \frac{\text{count}}{\text{max count}} 
$${#eq-equationp2}

$$
\text{mean density} = \frac{\text{standard count}}{6} 
$${#eq-equationp3}

The standard count is derived from the number of observations in the hex bins. By examining the distribution of mean densities and designating the first quartile as the benchmark value, hex bins with mean densities below this benchmark are removed. This process ensures the elimination of regions with insufficient data density, focusing on areas with more significant data representation and preserving the overall structure in the low-dimensional space.

### Benchmark value to remove long edges

The removal of long edges is a critical step to create a smoother representation  by iteratively eliminating hexagons with excessive distances between centroids. This process eliminates outliers and noise while preserving essential local relationships within the data. To achieve this, distances between vertices are sorted, and unique distance values are extracted. By setting a threshold based on the largest difference between consecutive distance values, long edges are identified and removed. This refinement step contributes to enhancing the quality of the triangular mesh, ensuring a more accurate representation of the data structure.

### Summaries of the model {#sec-summary}

#### Predicted values and residuals

In this context, the term "prediction values" refers to the 2D coordinates predicted for the NLDR technique. The approach involves employing the K-nearest neighbors (KNN) algorithm to identify the nearest hexagonal bin centroid in the 2D space. Subsequently, the coordinates of this centroid are assigned as the low-dimensional predicted values for the test data in 2D space. It is noteworthy that traditional NLDR methods, such as t-SNE, often lack a direct predict function, making our approach valuable for generating predicted values in the absence of such functionalities.

The concept of "residuals" is pivotal in evaluating the accuracy of representing bin centroids in high dimensions. To quantify this accuracy, we introduce an error metric, which measures the sum of squared differences between the high-dimensional data (\(x_{ij}\)) and the predicted bin centroid data in high-dimensional space (\(C_{x_ij}\)) across all bins and dimensions. Mathematically, this error is expressed as:

$$
\text{Error} = \sum_{j = 1}^{n}\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2
$$ {#eq-equation11}


Here, \(n\) represents the number of bins, \(p\) represents the dimensions, \(x_{ij}\) is the actual high-dimensional data, and \(C_{x_ij}\) is the predicted bin centroid data in high dimensions.

The error metric outlined above provides valuable insights into the overall accuracy of our predictive model. By quantifying the squared deviations between the actual and predicted values across all bins and dimensions, we gain a comprehensive understanding of how well our method captures and represents the underlying structure of the data in the reduced 2D space. This assessment is crucial for evaluating the efficacy of our NLDR technique in preserving the essential information present in the original high-dimensional data.


#### Goodness of fit statistics

Moving on to the assessment of prediction accuracy, we calculate the Mean Squared Error (MSE). The MSE helps measure the average squared differences between the actual high-dimensional data ($x_{ij}$) and the predicted bin centroid data in high-D ($C_{x_ij}$) values across all bins. Mathematically, this is expressed as:

$$
\text{MSE} = \sum_{j = 1}^{n} \frac{\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2}{\text{total number of bins}}
$$ {#eq-equation9}

Here, $b$ signifies the total number of bins, $p$ denotes the number of dimensions in the high-dimensional data, and $n$ represents the number of observations.

Additionally, we gauge the model's performance using the Akaike Information Criterion (AIC), calculated by the formula:

$$
\text{AIC} = 2bp + np * log(\text{MSE})
$$ {#eq-equation10}

These metrics, MSE and AIC, collectively offer valuable insights into the model's predictive performance, considering both accuracy and complexity in the predictions.


```{r}
#| warning: false
#| echo: false

data <- read_csv("data/s_curve.csv")
data <- data |>
  mutate(ID = row_number())

## tSNE

shape_value_curve <- calculate_effective_shape_value(.data = tSNE_data,
                                                   x = tSNE1, y = tSNE2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = tSNE_data, nldr_df_test = tSNE_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "tSNE1", y = "tSNE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_1 <- eval_data_training |>
  dplyr::mutate(method = "tSNE")
```


```{r}
#| warning: false
#| echo: false

## UMAP
## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                   x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_2 <- eval_data_training |>
  dplyr::mutate(method = "UMAP")
```

```{r}
#| warning: false
#| echo: false
## PAHTE
## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = PHATE_data,
                                                   x = PHATE1, y = PHATE2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = PHATE_data, nldr_df_test = PHATE_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "PHATE1", y = "PHATE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_3 <- eval_data_training |>
  dplyr::mutate(method = "PHATE")

```

```{r}
#| warning: false
#| echo: false

## TriMAP

## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = TriMAP_data,
                                                   x = TriMAP1, y = TriMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = TriMAP_data, nldr_df_test = TriMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "TriMAP1", y = "TriMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_4 <- eval_data_training |>
  dplyr::mutate(method = "TriMAP")

```

```{r}
#| warning: false
#| echo: false

## PaCMAP

## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = PaCMAP_data,
                                                   x = PaCMAP1, y = PaCMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = PaCMAP_data, nldr_df_test = PaCMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "PaCMAP1", y = "PaCMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_5 <- eval_data_training |>
  dplyr::mutate(method = "PaCMAP")

```

```{r}
#| warning: false
#| echo: false

MSE_df <- dplyr::bind_rows(MSE_df_1, MSE_df_2, MSE_df_3, MSE_df_4, MSE_df_5)

## To draw with AIC
aic_plot <- ggplot(MSE_df |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_discrete_qualitative() +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot <- ggplot(MSE_df, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training S-curve dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltScurve
#| out-width: 100%

aic_plot + mse_plot +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 7
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_7.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_7_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_1 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 7")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 15
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_15.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_15_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_2 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 15")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 32
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_32.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_32_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_3 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 32")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 50
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_4 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 50")
```


```{r}
#| warning: false
#| echo: false

MSE_df_umap <- dplyr::bind_rows(MSE_df_umap_1, MSE_df_umap_2, MSE_df_umap_3, MSE_df_umap_4)

## To draw with AIC
aic_plot_param <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                         y = total_error,
                                                                         color = param
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_discrete_qualitative() +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_param_training <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                               y = total_mse,
                               color = param
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

mse_plot_param_test <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "test"), aes(x = number_of_bins,
                                                                                             y = total_mse,
                                                                                             color = param
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different n_neighbors parameter of UMAP applied to training S-curve dataset. What is the best parameter choice in UMAP to represent the original data in 2D?
#| label: fig-diagnosticpltDiffParam
#| out-width: 100%

aic_plot_param + mse_plot_param_training + mse_plot_param_test  +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```


### Simulated data example {#sec-simpleex}

In this section, we showcase the effectiveness of our methodology using simulated data. The dataset comprises five spherical Gaussian clusters in 4-$d$, with each cluster containing an equal number of points and consistent within variation.  

We *strongly* recommend viewing the linked videos for each study while reading. Links to the videos are available in the figures for each example. The videos show the visual appearance of the **langevitour** interface with low-dimensional view and how we can interact with the tour via the controls.


```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/five_gau_clusters/data_five_gau.rds")
training_data_1 <- read_rds("data/five_gau_clusters/data_five_gau_training.rds")
test_1 <- read_rds("data/five_gau_clusters/data_five_gau_test.rds")

tSNE_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau.rds")
UMAP_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")
PHATE_data_gau <- read_rds("data/five_gau_clusters/phate_data_five_gau.rds")
TriMAP_data_gau <- read_rds("data/five_gau_clusters/trimap_data_five_gau.rds")
PaCMAP_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")

## Visualise embeddings

plot_list1_gau <- plot_tSNE_2D(tSNE_data_gau) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_gau <- plot_UMAP_2D(UMAP_data_gau) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_gau <- plot_PHATE_2D(PHATE_data_gau) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_gau <- plot_TriMAP_2D(TriMAP_data_gau) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_gau <- plot_PaCMAP_2D(PaCMAP_data_gau) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied the same data: (a) tSNE (perplexity = 61), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5Gau
#| out-width: 100%

plot_list1_gau + plot_list2_gau + plot_list3_gau + plot_list4_gau + plot_list5_gau +
  plot_layout(ncol=5)
```

```{r}
#| warning: false
#| echo: false

## tSNE

shape_value_gau <- calculate_effective_shape_value(.data = tSNE_data_gau,
                                                   x = tSNE1, y = tSNE2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = tSNE_data_gau, nldr_df_test = tSNE_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "tSNE1", y = "tSNE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_1_gau <- eval_data_training |>
  dplyr::mutate(method = "tSNE")
```


```{r}
#| warning: false
#| echo: false

## UMAP
## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = UMAP_data_gau,
                                                   x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = UMAP_data_gau, nldr_df_test = UMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_2_gau <- eval_data_training |>
  dplyr::mutate(method = "UMAP")
```

```{r}
#| warning: false
#| echo: false
## PAHTE
## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = PHATE_data_gau,
                                                   x = PHATE1, y = PHATE2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = PHATE_data_gau, nldr_df_test = PHATE_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "PHATE1", y = "PHATE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_3_gau <- eval_data_training |>
  dplyr::mutate(method = "PHATE")

```

```{r}
#| warning: false
#| echo: false

## TriMAP

## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = TriMAP_data_gau,
                                                   x = TriMAP1, y = TriMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = TriMAP_data_gau, nldr_df_test = TriMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "TriMAP1", y = "TriMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_4_gau <- eval_data_training |>
  dplyr::mutate(method = "TriMAP")

```

```{r}
#| warning: false
#| echo: false

## PaCMAP

## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = PaCMAP_data_gau,
                                                   x = PaCMAP1, y = PaCMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 6), c("number_of_bins", "number_of_observations", "total_error", "totol_error_method_2", "totol_error_method_3", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = PaCMAP_data_gau, nldr_df_test = PaCMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "PaCMAP1", y = "PaCMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_5_gau <- eval_data_training |>
  dplyr::mutate(method = "PaCMAP")

```

```{r}
#| warning: false
#| echo: false

MSE_df_gau <- dplyr::bind_rows(MSE_df_1_gau, MSE_df_2_gau, MSE_df_3_gau, MSE_df_4_gau, MSE_df_5_gau)

## To draw with AIC
aic_gau_plot <- ggplot(MSE_df_gau |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                             y = total_error,
                                                             color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_discrete_qualitative() +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_gau_plot <- ggplot(MSE_df_gau, aes(x = number_of_bins,
                   y = total_mse,
                   color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training five spherical Gaussian cluster dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltGau
#| out-width: 100%

aic_gau_plot + mse_gau_plot +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```



<!--six different clusters-->

<!--Gaussian globe or sphere-->

<!--In this section, we demonstrate the effectiveness of our approach through the use of simulation data. These simulations are designed to have known cluster structures and underlying data geometries. We begin with a straightforward example where we generate spherical clusters with a doublet structure. This allows us to assess how well our algorithm can identify and represent these known structures in the low-dimensional space.

We then proceed to a more complex example where the data exhibits a more intricate geometry. This simulation provides a challenging test for our approach to capture and preserve the underlying data structure accurately.

We *strongly* recommend viewing the linked videos for each study while reading. Links to the videos are available in the figures for each example. The videos show the visual appearance of the **langevitour** interface with low-dimensional view and how we can interact with the tour via the controls.

**Example 1: Exploring spherical Gaussian clusters with doublet structure**

The next data set consists of two distinct types of clusters embedded in a 10-$d$ space. The first set of clusters includes three Gaussian clusters with equal variance. Each of these Gaussian clusters is equidistant from the others, forming a well-separated and symmetrical arrangement. The second set of clusters consists of three clusters with a doublet structure. These clusters are also embedded in the 10-$d$ space, but each is positioned in the middle of its parent Gaussian clusters. Moreover, each doublet cluster contains an average number of points from its corresponding parent Gaussian clusters.

For analysis, we run tSNE on the training data with a perplexity setting 18. @fig-example3 (a) displays the tSNE results, indicating that the sub-clusters have been correctly identified. However, the relative locations of certain clusters to each other appear distorted in the low-dimensional view, such as the red and pink clusters being far apart.

Nonetheless, we find no apparent impact on the perception of the data structure when we examine the tour view and the model (see @fig-example3) through the linked video (see @fig-exp1_sc). The model demonstrates accurate identification of clusters and captures the internal variations within each cluster, reflecting the inherent variance present in the data. The configurations used for the analysis are detailed in @tbl-table02. Additionally, the representation of the parent clusters in the low-dimensional space appears stretched, aligning with the variance of these clusters. On the other hand, the doublet clusters are squeezed, reflecting their specific characteristics and relationship to their parent Gaussian clusters.

```{r}
#| warning: false
#| echo: false
sample_size <- 450
cluster_size <- sample_size/4.5

df1 <- tibble::tibble(x1=rnorm(cluster_size, mean = 3, sd = 0.05), x2 = rnorm(cluster_size, mean = 1, sd = 0.05), x3=rnorm(cluster_size, mean = 1, sd = 0.05), x4=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x5=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x6=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x7=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x8=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x9=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x10=rnorm(cluster_size, mean = 1, sd = 0.05))

df1 <- df1 %>%
  mutate(cluster = 1)

df2 <- tibble::tibble(x1=rnorm(cluster_size, mean = 1, sd = 0.05), x2=rnorm(cluster_size, mean = 1, sd = 0.05), x3=rnorm(cluster_size, mean = 1, sd = 0.05), x4=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x5=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x6=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x7=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x8=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x9=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x10=rnorm(cluster_size, mean = 1, sd = 0.05))

df2 <- df2 %>%
  mutate(cluster = 2)

df3_new <- (df1 + df2) / 2

samp <- sample(nrow(df3_new), cluster_size * 0.50) ## 50% from the original dataset

# data in the sample
df3 <- df3_new[samp,]

df3 <- df3 %>%
  mutate(cluster = 3)

df4 <- tibble::tibble(x1=rnorm(cluster_size, mean = 1, sd = 0.05), x2=rnorm(cluster_size, mean = 1, sd = 0.05), x3=rnorm(cluster_size, mean = 1, sd = 0.05), x4=rnorm(cluster_size, mean = 3, sd = 0.05),
                      x5=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x6=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x7=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x8=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x9=rnorm(cluster_size, mean = 1, sd = 0.05),
                      x10=rnorm(cluster_size, mean = 1, sd = 0.05))

df4 <- df4 %>%
  mutate(cluster = 4)

df5_new <- (df2 + df4) / 2

samp1 <- sample(nrow(df5_new), cluster_size * 0.50) ## 50% from the original dataset

df5 <- df5_new[samp1,]

df5 <- df5 %>%
  mutate(cluster = 5)

df6_new <- (df1 + df4) / 2

samp2 <- sample(nrow(df6_new), cluster_size * 0.50) ## 50% from the original dataset

df6 <- df6_new[samp2,]

df6 <- df6 %>%
  mutate(cluster = 6)

df_3 <- bind_rows(df1, df2, df3, df4, df5, df6)
```

```{r}
#| warning: false
#| echo: false

# langevitour::langevitour(df_3[, 1:10], group = df_3$cluster, levelColors = c("#b15928", "#1f78b4", "#bc80bd", "#33a02c", "#fb9a99", "#e31a1c"))
```

```{r}
#| warning: false
#| echo: false

df_3 <- df_3 |>
  mutate(ID = row_number())

data_split_sp <- initial_split(df_3)
training_data_5 <- training(data_split_sp) |>
  arrange(ID)
test_data_5 <- testing(data_split_sp) |>
  arrange(ID)

clusters <- training_data_5$cluster
```

```{r}
#| warning: false
#| echo: false 
tSNE_data_gau <- Fit_tSNE(training_data_5 |> dplyr::select(-ID, -cluster), with_seed = 20240110)
```

```{r}
#| warning: false
#| echo: false 


tSNE_cluster <- tSNE_data_gau %>%
  ggplot(aes(x = tSNE1,
             y = tSNE2, color = as.character(clusters))) +
  geom_point(alpha=0.5) +
  coord_equal() +
  theme_linedraw() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text = element_text(size = 5),
          axis.title = element_text(size = 7), #change legend key width
        legend.title = element_text(size=7), #change legend title font size
        legend.text = element_text(size=7)) +
  scale_color_manual(values=c("#b15928", "#1f78b4", "#bc80bd", "#33a02c", "#fb9a99", "#e31a1c", "#6a3d9a", "#ff7f00", "#cab2d6", "#fdbf6f", "#ffff99", "#a6cee3", "#8dd3c7", "#ffffb3", "#bebada", "#fb8072", "#80b1d3", "#fdb462", "#b3de69", "#fccde5", "#d9d9d9", "#b2df8a", "#ccebc5", "#ffed6f", "#000000", "#bdbdbd")) +
  guides(color = guide_legend(title = "Cluster"))

```

```{r}
#| warning: false
#| echo: false 

cell_area <- 1

num_bins_tsne_gau <- calculate_effective_x_bins(.data = tSNE_data_gau, x = tSNE1,
                                                cell_area = 1)
shape_val_tsne_gau <- calculate_effective_shape_value(.data = tSNE_data_gau,
                                                      x = tSNE1, y = tSNE2)

## To extract bin centroids
hexbin_data_object_tsne_gau <- extract_hexbin_centroids(nldr_df = tSNE_data_gau, num_bins = num_bins_tsne_gau, shape_val = shape_val_tsne_gau, x = tSNE1, y = tSNE2)

df_bin_centroids_tsne_gau <- hexbin_data_object_tsne_gau$hexdf_data

# min_std_cell_threshold3 <- 0.01
# 
# df_bin_centroids_pbmc <- df_bin_centroids_pbmc |>
#   dplyr::mutate(stand_cell_count = counts/max(counts)) |>
#   dplyr::filter(stand_cell_count > min_std_cell_threshold3) 

tSNE_data_with_hb_id_gau <- tSNE_data_gau |> 
  dplyr::mutate(hb_id = hexbin_data_object_tsne_gau$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_tsne_gau <- dplyr::bind_cols(training_data_5 |> dplyr::select(-ID), tSNE_data_with_hb_id_gau)

## Averaged on high-D
df_bin_tsne_gau <- avg_highD_data(.data = df_all_tsne_gau)

## Triangulate bin centroids
tr1_object_tsne_gau <- triangulate_bin_centroids(df_bin_centroids_tsne_gau, x, y)
tr_from_to_df_tsne_gau <- generate_edge_info(triangular_object = tr1_object_tsne_gau)

## Compute 2D distances
distance_tsne_gau <- cal_2D_dist(.data = tr_from_to_df_tsne_gau)

## To find the benchmark value
benchmark_tsne_gau <- find_benchmark_value(.data = distance_tsne_gau, distance_col = distance)
#benchmark_tsne <- 5

tour_tsne_gau <- show_langevitour(df_all_tsne_gau, df_bin_tsne_gau, df_bin_centroids_tsne_gau, benchmark_value = benchmark_tsne_gau, distance = distance_tsne_gau, distance_col = distance)

trimesh_cluster <- ggplot(df_bin_centroids_tsne_gau, aes(x = x, y = y)) +
  geom_trimesh() +
  coord_equal() 

trimesh_cluster <- trimesh_cluster +
  #ggtitle("(b)") +
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 7))

trimesh_gr_cluster <- colour_long_edges(.data = distance_tsne_gau, benchmark_value = benchmark_tsne_gau,
                                     triangular_object = tr1_object_tsne_gau, distance_col = distance)

trimesh_gr_cluster <- trimesh_gr_cluster +
  #ggtitle("(c)") +
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 7), #change legend key width
        legend.title = element_text(size=7), #change legend title font size
        legend.text = element_text(size=5))

tour_cluster <- show_langevitour(df_all_tsne_gau, df_bin_tsne_gau, df_bin_centroids_tsne_gau, benchmark_value = benchmark_tsne_gau, distance = distance_tsne_gau, distance_col = distance)
```


```{r}
#| warning: false
#| echo: false
#| out-width: 100%
#| label: fig-example3
#| fig-cap: Visualization and refinement of low-dimensional representation for sub-clustered data$\colon$ (a) 2D layout of tSNE (perplexity = 18), (b) triangular mesh, and (c) triangular mesh colored by edge type. Edges with lengths less than the benchmark value of $2$ are classifiedas small, while edges with lengths greater than or equal to this value are labeled as long. The combination of tSNE, hexagonal binning, and triangulation provides the effectiveness of our approach in visualizing and refining the low-dimensional representation of the sub-clustered data.

tSNE_cluster + trimesh_cluster + trimesh_gr_cluster +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect') &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

::: {#fig-exp1_sc layout-ncol="3"}
![](figures/example_1a.png){#fig-exp1_sc1}

![](figures/example_1b.png){#fig-exp1_sc2}

![](figures/example_1c.png){#fig-exp1_sc3}

Screen shots of the **langevitour** with the low-dimensional view applied to sub-clustered data set, a video of the tour animation is available at (\url{https://drive.google.com/file/d/15nAbmc4UoVlUyFcGtnlnmyxrJEbK39nx/view}). Combination of the **langevitour** and low-dimensional representation of the sub-clustered data provides the effectiveness of our approach.
:::

**Example 2: Exploring data with piecewise linear structure**

In our next exploration, we delve into simulated noisy tree-structured data designed to mimic branching trajectories of cell differentiation. The simulation is crafted to resemble the hierarchical clustering pattern observed in cell differentiation trajectories, where mature cells are akin to the tips of branches.

To begin our analysis, we apply principal components to the data and focus on the first ten principal components, collectively accounting for approximately 68% of the variance explained in the data set. Then, we run TriMAP with the default parameters on the training data (n_inliers: 5, n_outliers: 4, and n_random: 3).  Through this process, we create a low-dimensional manifold using the configurations detailed in @tbl-table02. Finally, we present the model along with the data using the tour view (Figure @fig-exp2_sc), allowing us to visualize and explore the intricate relationships and structures present in the data's low-dimensional representation.

```{r}
#| warning: false
#| echo: false 

data_phate <- read_csv(paste0(here::here(), "/data/data_phate.csv"))

data_phate <- data_phate |>
  dplyr::mutate(ID = row_number())
```

```{r}
#| warning: false
#| echo: false 
data_split <- initial_split(data_phate)
training_data <- training(data_split) |>
  arrange(ID)
test_data <- testing(data_split) |>
  arrange(ID)
```

```{r}
#| warning: false
#| echo: false 
## num_pcs: selected number of PCs to get results (deafukt is all) 

calculate_pca <- function(data, num_pcs = NCOL(data), center = TRUE, scale = FALSE){
  pcaY_cal <- prcomp(data, center = TRUE, scale = FALSE)
  
  PCAresults <- tibble::as_tibble(pcaY_cal$x[, 1:num_pcs])
  names(PCAresults) <- paste0(rep("PC", num_pcs), 1:num_pcs)
  
  summary_pca <- summary(pcaY_cal)
  
  var_explained_df <- tibble::tibble(PC = paste0("PC",1:NCOL(data)),
                               var_explained = (pcaY_cal$sdev[1:NCOL(data)])^2/sum((pcaY_cal$sdev[1:NCOL(data)])^2))
  
  ## Scree plot
  plot <- var_explained_df %>%
    ggplot(aes(x = PC,y = var_explained, group = 1))+
    geom_point(size=1)+
    geom_line() +
    theme_minimal() +
    scale_x_discrete(limits = paste0(rep("PC", 20), 1:20)) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
          axis.text = element_text(size = 5),
          axis.title = element_text(size = 7)) +
    xlab("Principal Components (PC)") +
    ylab("Variance explained")
    
  return(list(prcomp_out = pcaY_cal, pca_components = PCAresults, summary = summary_pca, var_explained_pca  = var_explained_df, plot = plot))
}
```

```{r}
#| warning: false
#| echo: false 
pca_out <- calculate_pca(training_data, 10)
data_pca <- pca_out$pca_components
data_pca <- data_pca %>% 
  distinct()

var_explain_example2 <- pca_out$plot
```


```{r}
#| warning: false
#| echo: false 
TriMAP_data_tree <- read_csv(paste0(here::here(), "/data/trimap_data_linear_structure_pca.csv")) %>%
    mutate(ID = row_number())

```

```{r}
#| warning: false
#| echo: false 
TriMAP_tree <- plot_TriMAP_2D(TriMAP_data_tree) +
  theme_linedraw() +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(),
              axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank(),
              panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


```{r}
#| warning: false
#| echo: false

cell_area <- 1

num_bins_tree <- calculate_effective_x_bins(.data = TriMAP_data_tree, x = TriMAP1,
                                            cell_area = 1)
shape_val_tree <- calculate_effective_shape_value(.data = TriMAP_data_tree,
                                                  x = TriMAP1, y = TriMAP2)

## To extract bin centroids
hexbin_data_object_tree <- extract_hexbin_centroids(nldr_df = TriMAP_data_tree, num_bins = num_bins_tree, shape_val = shape_val_tree, x = TriMAP1, y = TriMAP2)

df_bin_centroids_tree <- hexbin_data_object_tree$hexdf_data

# min_std_cell_threshold3 <- 0.01
# 
# df_bin_centroids_tree <- df_bin_centroids_tree |>
#   dplyr::mutate(stand_cell_count = counts/max(counts)) |>
#   dplyr::filter(stand_cell_count > min_std_cell_threshold3) 

TriMAP_data_with_hb_id_tree <- TriMAP_data_tree |> 
  dplyr::mutate(hb_id = hexbin_data_object_tree$hb_data@cID)

## To generate a data set with high-D and 2D training data
data_pca_n <- data_pca
names(data_pca_n) <- paste0(rep("x", 7), 1:7)
df_all_tree <- dplyr::bind_cols(data_pca_n, TriMAP_data_with_hb_id_tree)

## Averaged on high-D
df_bin_tree <- avg_highD_data(.data = df_all_tree)

## Triangulate bin centroids
tr1_object_tree <- triangulate_bin_centroids(df_bin_centroids_tree, x, y)
tr_from_to_df_tree <- generate_edge_info(triangular_object = tr1_object_tree)

## Compute 2D distances
distance_tree <- cal_2D_dist(.data = tr_from_to_df_tree)

## To find the benchmark value
benchmark_tree <- find_benchmark_value(.data = distance_tree, distance_col = distance)

benchmark_tree <- 2

names(df_all_tree)[1:7] <- paste0(rep("PC_", 7), 1:7)
names(df_bin_tree) <- c("hb_id", paste0(rep("PC_", 7), 1:7))

# tour_tree <- show_langevitour(df_all_tree, df_bin_tree, df_bin_centroids_tree, benchmark_value = benchmark_tree, distance = distance_tree, distance_col = distance, col_start = "PC")


trimesh_tree <- ggplot(df_bin_centroids_tree, aes(x = x, y = y)) +
  geom_trimesh() +
  coord_equal() 

trimesh_tree <- trimesh_tree +
  #ggtitle("(b)") +
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 7))

trimesh_gr_tree <- colour_long_edges(.data = distance_tree, benchmark_value = benchmark_tree,
                                     triangular_object = tr1_object_tree, distance_col = distance)

trimesh_gr_tree <- trimesh_gr_tree +
  #ggtitle("(c)") +
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 7), #change legend key width
        legend.title = element_text(size=7), #change legend title font size
        legend.text = element_text(size=5))

tour_tree <- show_langevitour(df_all_tree, df_bin_tree, df_bin_centroids_tree, benchmark_value = benchmark_tree, distance = distance_tree, distance_col = distance, col_start = "PC")

```


```{r}
#| warning: false
#| echo: false
#| out-width: 100%
#| label: fig-example2
#| fig-pos: H
#| fig-cap: Visualization and refinement of low-dimensional representation for tree structured data$\colon$ (a) 2D layout of TriMAP (n-inliers = 5, n-outliers = 4, n-random = 3), (b) triangular mesh, and (c) triangular mesh colored by edge type. Edges with lengths less than the benchmark value of $2$ are classified as small, while edges with lengths greater than or equal to this value are labeled as long. The combination of TriMAP, hexagonal binning, and triangulation provides the effectiveness of our approach in visualizing and refining the low-dimensional representation of the tree structured data.

TriMAP_tree + trimesh_tree + trimesh_gr_tree +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect') &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

::: {#fig-exp2_sc layout-ncol="3"}
![](figures/example_2a.png)

![](figures/example_2b.png)

![](figures/example_2c.png)

Screen shots of the **langevitour** with the low-dimensional view applied to tree structured data set, a video of the tour animation is available at (\url{https://drive.google.com/file/d/1YQx4UGlzDify1mGqeDtlqTHncgqidDhq/view}). Combination of the **langevitour** and low-dimensional representation of the tree structured data provides the effectiveness of our approach.
:::
-->

## Applications {#sec-applications}

### Single-cell RNA-seq data of human

In the field of single-cell studies, a common analysis task involves clustering to identify groups of cells with similar expression profiles. Analysts often turn to NLDR techniques to verify and identify these clusters and explore developmental trajectories (e.g., example 1). In clustering workflows, the main objective is to verify the existence of clusters and subsequently identify them as specific cell types by examining the expression of "known" marker genes. In this context, a "faithful" embedding should ideally preserve the topology of the data, ensuring that cells corresponding to the same cell type are situated close to the high-dimensional space.

To begin our analysis, we installed the Peripheral Blood Mononuclear Cells (pbmc) data set obtained from 10x Genomics using the `SeuratData` R package [@Rahul2019], which facilitates the distribution of data sets in the form of Seurat objects [@Yuhan2021]. This data set contains 13,714 features across 2,700 samples within a single assay. The active assay is RNA, with 13,714 features representing different gene expressions. After loading the data set, we obtained the principal components (PCs) and assessed the variance explained by each PC. Based on this evaluation, we selected seven PCs, representing approximately 50% of the variance in gene expression, for further analysis.

Next, we employed the UMAP technique with default parameter settings. As illustrated in @fig-pbmc, the cell types B and Platelet are well-separated in the UMAP layout. Moreover, CD14+ Mono, FCGR3A+ Mono, and DC form a distinct cluster, while Naive CD4 T, NK, Memory CD4 T, and CD8 T are grouped together in another cluster. The values utilized to construct the smooth low-dimensional manifold are presented in @tbl-table02. The linked video, demonstrating the tour with the model, showcases the generation of a smooth surface for this application, enabling a comprehensive exploration of the data's structure and relationships (see @fig-pbmc_sc).


```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/pbmc/pbmc.rds")
training_data_1 <- read_rds("data/pbmc/pbmc_training.rds")
test_1 <- read_rds("data/pbmc/pbmc_test.rds")

tSNE_pbmc <- read_rds("data/pbmc/pbmc_tsne_30.rds")
UMAP_pbmc <- read_rds("data/pbmc/pbmc_umap.rds")
PHATE_pbmc <- read_rds("data/pbmc/pbmc_phate.rds")
TriMAP_pbmc <- read_rds("data/pbmc/pbmc_trimap.rds")
PaCMAP_pbmc <- read_rds("data/pbmc/pbmc_pacmap.rds")

## Visualise embeddings

plot_list1_pbmc <- plot_tSNE_2D(tSNE_pbmc) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_pbmc <- plot_UMAP_2D(UMAP_pbmc) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_pbmc <- plot_PHATE_2D(PHATE_pbmc) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_pbmc <- plot_TriMAP_2D(TriMAP_pbmc) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_pbmc <- plot_PaCMAP_2D(PaCMAP_pbmc) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied for the training PBMC dataset: (a) tSNE (perplexity = 30), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5PBMC
#| out-width: 100%

plot_list1_pbmc + plot_list2_pbmc + plot_list3_pbmc + plot_list4_pbmc + plot_list5_pbmc +
  plot_layout(ncol=5)
```



```{r}
#| warning: false
#| echo: false

MSE_df_pbmc <- read_rds("data/pbmc/summary_pbmc.rds")

## To draw with AIC
aic_plot_pbmc <- ggplot(MSE_df_pbmc |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_discrete_qualitative() +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_pbmc <- ggplot(MSE_df_pbmc, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training PBMC dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltPBMC
#| out-width: 100%

aic_plot_pbmc + mse_plot_pbmc +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```


### Single-Cell Tagged Reverse Transcription sequencing data of mouse

The Zeisel mouse brain dataset, obtained through Spatial Transcriptomics (STRT-Seq). Within this dataset, information is collected from a substantial 2,816 individual mouse brain cells. Each of these cells acts as a molecular snapshot, capturing the distinctive genetic activity within various cell types. This diversity spans neurons, glial cells, and other essential components of the brain, offering a comprehensive view of the cellular tapestry.

What makes this dataset particularly valuable is its ability to shed light on the spatial distribution of cells. Researchers can explore how gene expression patterns vary across different regions of the mouse brain, unlocking insights into the functional specialization of these regions and the intricate networks that underpin neural processes.

```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/zeisel/zeisel.rds")
training_data_1 <- read_rds("data/zeisel/zeisel_training.rds")
test_1 <- read_rds("data/zeisel/zeisel_test.rds")

tSNE_zeisel <- read_rds("data/zeisel/zeisel_tsne_30.rds")
UMAP_zeisel <- read_rds("data/zeisel/zeisel_umap.rds")
PHATE_zeisel <- read_rds("data/zeisel/zeisel_phate.rds")
TriMAP_zeisel <- read_rds("data/zeisel/zeisel_trimap.rds")
PaCMAP_zeisel <- read_rds("data/zeisel/zeisel_pacmap.rds")

## Visualise embeddings

plot_list1_zeisel <- plot_tSNE_2D(tSNE_zeisel) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_zeisel <- plot_UMAP_2D(UMAP_zeisel) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_zeisel <- plot_PHATE_2D(PHATE_zeisel) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_zeisel <- plot_TriMAP_2D(TriMAP_zeisel) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_zeisel <- plot_PaCMAP_2D(PaCMAP_zeisel) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied for the training Zeisel mouse brain dataset: (a) tSNE (perplexity = 30), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5Mouse
#| out-width: 100%

plot_list1_zeisel + plot_list2_zeisel + plot_list3_zeisel + plot_list4_zeisel + plot_list5_zeisel +
  plot_layout(ncol=5)
```


```{r}
#| warning: false
#| echo: false

MSE_df_zei <- read_rds("data/zeisel/summary_zei.rds")

## To draw with AIC
aic_plot_zei <- ggplot(MSE_df_zei |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_discrete_qualitative() +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_zei <- ggplot(MSE_df_zei, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_discrete_qualitative() +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training Zeisel mouse brain dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltZEI
#| out-width: 100%

aic_plot_zei + mse_plot_zei +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

## Discussion {#sec-discussion}

Our research introduces a comprehensive framework that leverages tours for interactive exploration of high-dimensional data coupled with a low-dimensional manifold, facilitated by the `quollr` R package. Regardless of the Non-Linear Dimension Reduction (NLDR) technique in use, our approach demonstrates effectiveness through simulation examples, particularly in the iterative removal of long edges for a smoother representation and capturing cluster variance.

In the example with doublets, our method successfully captures the tweak within each cluster, indicating the variance present within them. However, the model may not appear smooth in high-dimensional space due to considerable noise when the data has a piecewise linear geometry, such as the tree simulation.

The practical application of our framework, as showcased with the UMAP view, enables visual inspection of well-separated clusters. Furthermore, the combined tour and model provide a robust assessment of whether UMAP preserves the data structure and accurately transforms the data.

The advantages of our approach include its versatility across various NLDR techniques and the ability to generate interactive visualizations for detailed exploration. The tour provides an intuitive way to navigate and comprehend high-dimensional data while assessing the accuracy of dimensionality reduction.

However, one limitation is that the approach may be less effective in cases with significant noise, as seen in the tree simulation example. Additionally, while our method aids in visual verification, quantifying the accuracy of embeddings might require further evaluation metrics.

In conclusion, our framework presents a powerful tool for researchers and analysts in single-cell studies to assess their embeddings by visually inspecting them alongside the original data. By leveraging the advantages of tours and low-dimensional manifolds, our approach offers valuable insights into the data transformation process, empowering users to make informed decisions in analyzing high-dimensional data. Future work could enhance the method's robustness in the presence of noise and explore additional evaluation metrics for quantifying embedding accuracy.


## References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}
