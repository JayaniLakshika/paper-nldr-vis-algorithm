---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P. Gamage
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: https://www.michaellydeamore.com/
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-linear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data (\pD{}) by applying a non-linear transformation. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of \pD{} distributions. The NLDR methods and (hyper)parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the \pD{} data, we have developed an algorithm to show the \gD{} NLDR model in the \pD{} space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks. 
  
keywords: [high-dimensional data vizualization, non-linear dimension reduction, tour]
keywords-formatted: [high-dimensional data vizualization, non-linear dimension reduction, tour]

bibliography: paper.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r include=FALSE}
#| label: install-libraries
#| warning: false
#| echo: false

options(repos = c(CRAN = "https://cran.rstudio.com")) # Setup mirror

packages_to_check <- c("remotes", "tidyverse", "patchwork", "colorspace", "kableExtra", "conflicted")

for (pkg in packages_to_check) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Installing package:", pkg))
    install.packages(pkg)
  } else {
    installed_version <- packageVersion(pkg)
    available_version <- tryCatch({
      utils::packageDescription(pkg)$Version
    }, error = function(e) NA) # Handle cases where package info isn't readily available

    if (!is.na(available_version) && installed_version < package_version(available_version)) {
      message(paste("A newer version of package", pkg, "is available. Updating..."))
      install.packages(pkg)
    } else {
      message(paste("Package", pkg, "is up-to-date (version", installed_version, ")."))
    }
  }
}

if (!requireNamespace("quollr", quietly = TRUE)) {
  remotes::install_github("JayaniLakshika/quollr")
}


```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(quollr)
library(tibble)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(patchwork)
library(colorspace)
library(kableExtra)
library(conflicted)
library(ggbeeswarm)
library(scales)

conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm'),
     plot.margin = margin(0, 0, 0, 0)
   )
)
```

```{r}
#| label: import-scripts
source("scripts/additional_functions.R")
```

```{r}
#| label: code-setup
set.seed(20240110)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{2.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data ($k < p$). Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2019], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
#| label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a", c(0.08, 0.9), cex = 1.3)

nldr1c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a", c(0.08, 0.9))

# umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.8.rds")
nldr2 <- umap_pbmc |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b", c(0.08, 0.9), cex = 1.3)

nldr2c <- umap_pbmc |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("b", c(0.08, 0.9))


umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c", cex = 1.3)

nldr3c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d", cex = 1.3)

nldr4c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("d") 

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e", c(0.08, 0.9), cex = 1.3)

nldr5c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("e", c(0.08, 0.9)) 

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f", cex = 1.3)

nldr6c <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("f") 

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g", cex = 1.3)

nldr7c <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("g") 

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h", cex = 1.3)

nldr8c <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#f781bf') +
  interior_annotation("h") 
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Eight different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
#| fig-pos: H
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 30), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

The paper is organized as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD{}   representations of \pD{} data have been computed using multidimensional scaling (MDS) [@kruskal1964], which includes principal components analysis (PCA) [@jolliffe2011] as a special case. (A contemporary comprehensive guide to MDS can be found in @borg2005.) The \kD{}   representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{}   layout is constructed by minimizing a stress function that differences distances between points in \pD{}   with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}   minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is \gD{}, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in \gD{}    represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@cfa69] and MDS. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)
<!--### Notation -->

<!-- @tbl-notation summarises the notation used to explain the new methodology. The observed data is denoted as $\mathbfit{x}_{n \times p}$ where $x_{ij}$ would indicate the $i^{th}$ observation on the $j^{th}$ variable sampled from a population $\mathbfit{X}$. To refer to variable $j$, we would use $X_j$.--> 

<!--
$X_{n \times p} = \begin{bmatrix} \textbf{x} _{1} & \textbf{x}_ {2} & \cdots & \textbf{x}_{n} \\  \end{bmatrix}^\top$

$Y_{n \times d} = \begin{bmatrix} \textbf{y} _{1} & \textbf{y}_ {2} & \cdots & \textbf{y}_{n} \\  \end{bmatrix}^\top$

$C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$

$C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$ $p$-D mappings of 2D hexagon bin centroids of the $k^{th}$ hexagon

-->

```{r}
#| label: tbl-notation
#| tbl-cap: "Summary of notation for describing new methodology."
# Notation used in the paper

notation_df <- read_csv("misc/notation.csv")

# Create the table
kable(notation_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("3cm", "12cm"))
```

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with (hyper-)parameters $\mathbfit{\theta}$. The parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. <!--Any input requirements for the data (such as normalization, or preprocessing through the use of PCA or similar) is considered part of the function $g$.-->

With our goal being to make a representation of this \gD{} layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbour information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. We recommend using a hexagon grid because it better reflects the data distribution and has less artifacts than a rectangular grid. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-two-curvy, and documented below.

```{r}
#| label: two-curvy-training
training_data_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_data.rds") |>
  mutate(ID = row_number())
```

```{r}
#| label: two-curvy-true-proj-data

true_model_df <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_true_model.rds")
wireframe_true_model <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_true_model_connections.rds")

data_two_curvy <- training_data_two_curvy |>
  select(-ID) |>
  mutate(type = "data")

true_model_two_curvy <- true_model_df |> 
  mutate(type = "true model")
```

<!--tSNE applied for Two curvy data-->
```{r}
#| label: tsne-two-curvy
tsne_two_curvy <- read_rds(file = "data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_30.rds") |>
  mutate(ID = row_number()) |>
  rename(c("emb1" = "tSNE1",
           "emb2" = "tSNE2"))
```

```{r}
#| label: fit_model-two-curvy

num_bins_x_two_curvy <- 24

algo_obj_two_curvy <- fit_highd_model(
  highd_data = training_data_two_curvy, 
  nldr_data = tsne_two_curvy, 
  bin1 = num_bins_x_two_curvy, 
  q = 0.1, 
  benchmark_highdens = 1)

tsne_two_curvy_scaled <- algo_obj_two_curvy$nldr_obj$scaled_nldr
tr_from_to_df_two_curvy <- algo_obj_two_curvy$trimesh_data
df_bin_centroids_two_curvy <- algo_obj_two_curvy$model_2d
df_bin_two_curvy <- algo_obj_two_curvy$model_highd
hex_grid <- algo_obj_two_curvy$hb_obj$hex_poly
counts_df <- algo_obj_two_curvy$hb_obj$std_cts

hex_grid_with_counts <- left_join(hex_grid, counts_df, by = c("hex_poly_id" = "hexID"))

hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids_two_curvy$hexID)

bin_width <- algo_obj_two_curvy$hb_obj$a1

sc_ltr_pos <- c(0.08, 0.9)
sc_xlims <- c(-0.3, 1.2)
sc_ylims <- c(-0.17, 1.25)
```

<!--Full hexagon grid with UMAP data-->

```{r}
#| label: hexbin-two-curvy

nldr_two_curvy_original <- ggplot(tsne_two_curvy_scaled, 
                                  aes(x = emb1, y = emb2)) +
  geom_point(alpha = 0.5, color = clr_choice, size = 0.5) 

# First plot with xlim and ylim
nldr_two_curvy <- nldr_two_curvy_original +
  interior_annotation("a", sc_ltr_pos, cex = 2) +
  xlim(sc_xlims) +
  ylim(sc_ylims)

# Second plot (identical to base except for xlim/ylim)
nldr_two_curvy2 <- nldr_two_curvy_original +
  interior_annotation("a", sc_ltr_pos, cex = 1.5)

# Third plot with aspect ratio and different annotation
nldr_two_curvy_original2 <- nldr_two_curvy_original +
  theme(aspect.ratio = 1) +
  interior_annotation("a1", sc_ltr_pos)

hex_grid_two_curvy <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(color = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = tsne_two_curvy_scaled, 
             aes(x = emb1, y = emb2), 
             alpha = 0.5, size = 0.5, color = clr_choice) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos, cex = 2)

hex_grid_coloured_two_curvy1 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts, 
    aes(x = x, y = y,
      group = hex_poly_id, 
      fill = std_counts), color = "grey70", linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
             aes(x = emb1, y = emb2),
             colour = clr_choice,
             alpha = 0.3,
             size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "E") +
  interior_annotation("b", sc_ltr_pos, cex = 2) + 
  xlim(sc_xlims) + ylim(sc_ylims) 
```

<!--Non-empty bins with bin centroids-->

```{r}
#| label: empty-bin-two-curvy

# df_bin_centroids_two_curvy <- df_bin_centroids_two_curvy |>
#   dplyr::filter(std_counts > 0)

hex_grid_nonempty_two_curvy <- ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(color = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids_two_curvy, 
             aes(x = c_x, y = c_y), 
             color = "#000000",
             size = 1) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos, cex = 2) 
```

<!--2D model-->
```{r}
#| label: triangulate-two-curvy

trimesh_two_curvy <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = emb1,
               y = emb2
             ),
             color = "#636363",
             alpha = 0.5,
             size = 0.5
  ) +
  coord_equal() +
  interior_annotation("a1", sc_ltr_pos)

trimesh_removed_two_curvy <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  coord_equal() +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos, cex = 2)

trimesh_two_curvy_removed1_tsne_with_data <- ggplot() +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = emb1,
               y = emb2
             ),
             color = clr_choice,
             alpha = 0.5,
             size = 0.5) +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  interior_annotation("a", sc_ltr_pos, cex = 1.2) +
  theme(aspect.ratio = 1)
```


```{r}
#| label: compute-error-tsne

## Compute error
error_df_two_curvy_abs <- augment(
  highd_data = training_data_two_curvy,
  model_2d = df_bin_centroids_two_curvy,
  model_highd = df_bin_two_curvy)

error_df_two_curvy_abs <- error_df_two_curvy_abs |>
  mutate(sqrt_row_wise_total_error = sqrt(row_wise_total_error))

error_df_two_curvy_abs <- error_df_two_curvy_abs |>
  bind_cols(tsne_two_curvy_scaled |>
              select(-ID))

error_plot_two_curvy <- error_df_two_curvy_abs |>
  ggplot(aes(x = emb1,
             y = emb2,
             colour = sqrt_row_wise_total_error)) +
  geom_point(alpha=0.5) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  xlab("tSNE1") +
  ylab("tSNE2") +
  theme_bw() +
  theme(
    aspect.ratio = 1,
    legend.position = "none"
  )

# error_plot_two_curvy_hist <- 
#   ggplot(error_df_two_curvy_abs) +
#   geom_histogram(aes(x=sqrt_row_wise_total_error, y=..density..)) +
#   geom_density(aes(x=sqrt_row_wise_total_error, y=..density..), colour="red") +
#   #xlab(expression(group("|", e[hj], "|"))) +
#   xlab(expression(e[hj])) +
#   ylab("") +
#   theme_bw() 

error_plot_two_curvy_quasi <- ggplot(
  error_df_two_curvy_abs, 
  aes(x = sqrt_row_wise_total_error, 
      y = 0, 
      colour = sqrt_row_wise_total_error)) +
  geom_quasirandom(alpha = 0.5) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  xlab(expression(e[hj])) +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "none")
```

```{r}
#| label: prep-data-for-proj-tsne-best

df_bin_two_curvy <- df_bin_two_curvy |>
  select(-hexID) |>
  mutate(type = "model")

# Apply the scaling
df_model_data_two_curvy <- bind_rows(data_two_curvy, true_model_two_curvy, df_bin_two_curvy)

scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |>
  as_tibble()

scaled_two_curvy_data <- scaled_two_curvy |>
  filter(type == "data") |>
  select(-type)

scaled_two_curvy_data_true_model <- scaled_two_curvy |>
  filter(type == "true model") |>
  select(-type)

scaled_two_curvy_data_model <- scaled_two_curvy |>
  filter(type == "model") |>
  select(-type)

```

```{r}
#| label: langevitour-true-two-curvy-proj
#| eval: false
df_model_data_two_curvy_filtered <- bind_rows(true_model_two_curvy, data_two_curvy)

langevitour::langevitour(df_model_data_two_curvy_filtered[1:(length(df_model_data_two_curvy_filtered)-1)],
                         lineFrom = wireframe_true_model$from,
                         lineTo = wireframe_true_model$to,
                         group = factor(df_model_data_two_curvy_filtered$type,
                                        c("data", "true model")),
                         levelColors = c(clr_choice, "#000000"))
```

```{r}
#| label: langevitour-best-fit-two-curvy-proj-tsne
#| eval: false

df_model_data_two_curvy_filtered <- bind_rows(df_bin_two_curvy, data_two_curvy)

langevitour::langevitour(df_model_data_two_curvy_filtered[1:(length(df_model_data_two_curvy_filtered)-1)],
                         lineFrom = tr_from_to_df_two_curvy$from,
                         lineTo = tr_from_to_df_two_curvy$to,
                         group = factor(df_model_data_two_curvy_filtered$type,
                                        c("data", "model")),
                         levelColors = c(clr_choice, "#000000"))
```


```{r}
#| label: best-fit-two-curvy-proj-tsne

## First projection
model_prj1 <- cbind(
  c(0.37847,-0.06517,0.04231,0.01655),
  c(-0.06466,-0.27052,0.22131,-0.15235))

proj_obj1 <- quollr::get_projection(projection = model_prj1, 
               proj_scale = 1, 
               highd_data = scaled_two_curvy_data, 
               model_highd = scaled_two_curvy_data_model, 
               trimesh_data = tr_from_to_df_two_curvy, 
               axis_param = list(limits = 0.5, 
                                 axis_scaled = 2,
                                 axis_pos_x = -0.5, 
                                 axis_pos_y = -0.5, 
                                 threshold = 0.03))

## For true model

projection_scaled <- model_prj1 * 1

projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% projection_scaled

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")

proj_obj1_true <- proj_obj1
proj_obj1_true[["model_df"]] <- true_model_df_proj

two_curvy_proj_true <- plot_proj(
  proj_obj = proj_obj1_true, 
  point_param = c(0.5, 0.5, clr_choice), # size, alpha, color
  line_param = c(0.5, 0.5, "black"), # linewidth, alpha
  plot_limits = c(-0.6, 0.6), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "a", cex = 1.5) +
  theme(aspect.ratio=1, 
        legend.position = "none") 

# Changed the axis parameters
axis_obj <- gen_axes(
    proj = model_prj1 * 2,
    limits = 0.9,
    axis_pos_x = -0.4,
    axis_pos_y = -0.4,
    axis_labels = names(scaled_two_curvy_data),
    threshold = 0.05)

axes <- axis_obj$axes
circle <- axis_obj$circle

proj_obj1[["axes"]] <- axes
proj_obj1[["circle"]] <- circle

two_curvy_proj_tsne_first_model1 <- plot_proj(
    proj_obj = proj_obj1, 
    point_param = c(0.5, 0.2, clr_choice), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-0.6, 0.6), 
    axis_text_size = 2, 
    is_category = FALSE) +
  interior_annotation(label = "b", position = sc_ltr_pos, cex = 1.2) +
  theme(aspect.ratio = 1,
        legend.position = "none") 

## Second projection
model_prj2 <- cbind(
  c(-0.38281,-0.04870,0.00619,-0.02451),
  c(-0.01303,0.00634,-0.37421,0.09648))

proj_obj2 <- get_projection(projection = model_prj2, 
                            proj_scale = 1, 
                            highd_data = scaled_two_curvy_data, 
                            model_highd = scaled_two_curvy_data_model, 
                            trimesh_data = tr_from_to_df_two_curvy, 
                            axis_param = list(limits = 0.35, 
                                              axis_scaled = 2,
                                              axis_pos_x = -0.35, 
                                              axis_pos_y = -0.35, 
                                              threshold = 0.01))


projected_true_model <- as.matrix(scaled_two_curvy_data_true_model) %*% model_prj2

projected_true_model_df <- projected_true_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

true_model_df_proj <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_model_df, 
  by = c("from" = "ID"))

names(true_model_df_proj)[3:NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_from")

true_model_df_proj <- dplyr::left_join(true_model_df_proj, projected_true_model_df, by = c("to" = "ID"))
names(true_model_df_proj)[(2 + NCOL(projected_true_model_df)):NCOL(true_model_df_proj)] <- paste0(names(projected_true_model_df)[-NCOL(projected_true_model_df)], "_to")

proj_obj2_true <- proj_obj2
proj_obj2_true[["model_df"]] <- true_model_df_proj

two_curvy_proj_true2 <- plot_proj(
    proj_obj = proj_obj2_true, 
    point_param = c(0.5, 0.5, clr_choice), # size, alpha, color
    line_param = c(0.5, 0.5, "black"), # linewidth, alpha
    plot_limits = c(-0.42, 0.42), 
    axis_text_size = 4, 
    is_category = FALSE) +
  interior_annotation(label = "b", cex = 1.5) +
  theme(aspect.ratio = 1, 
        legend.position = "none")

#Changed the axis parameters
axis_obj <- gen_axes(
    proj = model_prj2 * 2,
    limits = 0.7,
    axis_pos_x = -0.35,
    axis_pos_y = -0.35,
    axis_labels = names(scaled_two_curvy_data),
    threshold = 0.02)

axes <- axis_obj$axes
circle <- axis_obj$circle

proj_obj2[["axes"]] <- axes
proj_obj2[["circle"]] <- circle

two_curvy_proj_tsne_all_model1 <- plot_proj(
  proj_obj = proj_obj2, 
  point_param = c(0.1, 0.5, clr_choice), # size, alpha, color
  line_param = c(0.5, 0.5, "black"), # linewidth, alpha
  plot_limits = c(-0.5, 0.5), 
  axis_text_size = 2, 
  is_category = FALSE) +
  interior_annotation(label = "c", position = sc_ltr_pos, cex = 1.2) +
  theme(aspect.ratio = 1) #+
  #geom_segment(
  #  data = model_df, 
  #  aes(
  #    x = proj1_from, 
  #    y = proj2_from, 
  #    xend = proj1_to, 
  #    yend = proj2_to), 
  #  color = "#000000")

```


To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "non-linear clusters". It is constructed by simulating two clusters, each consisting of $1000$ observations. The C-shaped cluster is generated from $\theta \sim U(-3\pi/2, 0)$, $X_1 = \sin(\theta)$, $X_2 \sim U(0, 2)$ (adding thickness to the C), $X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)$, $X_4 = \cos(\theta)$. The other cluster is from $X_1 \sim U(0, 2)$, $X_2 \sim U(0, 3)$, $\gamma \sim U(0, 0.5)$, $X_3 = -(X_1^3 + X_2) + \gamma$, and $X_4 \sim U(0, 2)$. We would consider $T=(X_1, X_2, X_3, X_4)$ to be the geometric structure (true model) that we hope to capture.

<!-- The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance.  -->

```{r}
#| label: fig-two-curvy-true-proj
#| fig-cap: "Two \\gD{} projections from $4\\text{-}D$, for the two non-linear clusters data (blue points) and true model (black lines) are shown. The top cluster in (a) shows uniformly distributed data along a C-shape, while in the bottom cluster, the data is more concentrated in one corner of the curvy shape. Video of the langevitour animations are available at <>." 
#| fig-pos: H
#| fig-width: 10
#| fig-height: 5
#| eval: false

two_curvy_proj_true + two_curvy_proj_true2 +
  plot_layout(ncol = 2)
```

```{r}
#| label: fig-NLDR-two-curvy
#| echo: false
#| fig-cap: "Key steps for constructing the model on the tSNE layout ($k=2$): (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids. The two non-linear clusters data is shown."
#| fig-width: 12
#| fig-height: 4
#| out-width: 100%
 
nldr_two_curvy + hex_grid_two_curvy + 
  hex_grid_nonempty_two_curvy + 
  trimesh_removed_two_curvy +
  plot_layout(ncol = 4)
```

### Algorithm to represent the model in \gD{}  

#### Scale the data

Because we are working with distances between points, starting with data having a standard scale, e.g. [0, 1], is recommended. The default should take the aspect ratio produced by the NLDR $(r_1, r_2, ..., r_k)$ into account. When $k=2$, as in hexagon binning, the default range is $[0, y_{i,\text{max}}], i=1,2$, where $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = r_2/r_1$ (@fig-NLDR-two-curvy). If the NLDR aspect ratio is ignored then set $y_ {2,\text{max}} = 1.$ <!-- \times \frac{2}{\sqrt{3}}$. (The $\frac{2}{\sqrt{3}}$ accounts for the different height ($a_1$) and width ($a_2$) of a regular hexagon.) The scaling of data should take the size of the hexagons into account, but choice of number of bins should. -->

#### Computing hexagon grid configuration

Although there are several implementations of hexagon binning [@carr1987], and a published paper [@dan2023], surprisingly, none has sufficient detail or components that produce everything needed for this project. So we described the process used here. @fig-hex-param illustrates the notation used. 

The \gD{} hexagon grid is defined by its bin centroids. Each hexagon, $H_h$ ($h = 1, \dots, b$) is uniquely described by centroid, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. The number of bins in each direction is denoted as $(b_1, b_2)$, with  $b = b_1 \times b_2$ being the total number of bins. We expect the user to provide just $b_1$ and we calculate $b_2$ using the NLDR ratio, to compute the grid. 

To ensure that the grid covers the range of data values a buffer parameter ($q$) is set as a proportion of the range. By default,  $q=0.1$. The buffer should be extending a full hexagon width ($a_1$) and height ($a_2$) beyond the data, in all directions. The lower left position where the grid starts is defined as $(s_1, s_2)$, and corresponds to the centroid of the lowest left hexagon, $C_{1}^{(2)} = (c_{11}, c_{12})$. This must be smaller than the minimum data value. Because it is one buffer unit, $q$ below the minimum data values, $s_1 = -q$ and $s_2 = -qr_2$. 

The value for $b_2$ is computed by fixing $b_1$. Considering the upper bound of the first NLDR component, $a_1 > (1+2q)/(b_1 -1)$. Similarly, for the second NLDR component, 

$$
a_2 > \frac{r_2 + q(1 + r_2)}{(b_2 - 1)}.
$$

Since $a_2 = \sqrt{3}a_1/2$ for regular hexagons,

$$
a_1 > \frac{2[r_2 + q(1 + r_2)]}{\sqrt{3}(b_2 - 1)}.
$$

This is a linear optimization problem. Therefore, the optimal solution must occur on a vertex. Therefore,

$$
b_2 = \Big\lceil1 +\frac{2[r_2 + q(1 + r_2)](b_1 - 1)}{\sqrt{3}(1 + 2q)}\Big\rceil.
$$

```{r}
#| label: code-illustration
# Code to draw illustration for notation
## hexagon binning to have regular hexagons
hb_obj_notation <- hex_binning(
  nldr_obj = algo_obj_two_curvy$nldr_obj, 
  bin1 = 7, 
  q = 0.1)

a1_temp <- hb_obj_notation$a1
a2_temp <- hb_obj_notation$a2
l_temp <- quad(a=3, b = 2 * a2_temp, c = -(a2_temp^2 + a1_temp^2))

## Data set with all centroids
all_centroids_df_temp <- hb_obj_notation$centroids
hex_grid_temp <- hb_obj_notation$hex_poly

hex_grid_temp40 <- hex_grid_temp |> 
  filter(hex_poly_id == 40)

start_pt <- all_centroids_df_temp |> 
  filter(hexID == 1)
d_rect <- tibble(x1min = 0, 
                 x1max = 1,
                 x2min = 0,
                 x2max = diff(algo_obj_two_curvy$nldr_obj$lim2)/diff(algo_obj_two_curvy$nldr_obj$lim1))

# To move the rectangle to ignore the overlap with the centroids
# rect_adj <- tibble(x1 = 0.03, x2 = 0.03)
rect_adj <- tibble(x1 = -0.03, x2 = 0.03)


a1 <- tibble(x = all_centroids_df_temp$c_x[4],
             xend = all_centroids_df_temp$c_x[5],
             y = all_centroids_df_temp$c_y[21],
             yend = all_centroids_df_temp$c_y[21],
             label = expression(a[1]))
a2 <- tibble(x = all_centroids_df_temp$c_x[25],
             xend = all_centroids_df_temp$c_x[25],
             y = all_centroids_df_temp$c_y[25],
             yend = all_centroids_df_temp$c_y[33],
             label = expression(a[2]))
l <- tibble(x = hex_grid_temp40$x[2],
            xend = hex_grid_temp40$x[3],
            y = hex_grid_temp40$y[2],
            yend = hex_grid_temp40$y[3],
            label = expression(l))

hex_param_vis <- ggplot() + 
    geom_polygon(data = hex_grid_temp, 
                        aes(x = x, 
                            y = y, 
                            group = hex_poly_id),
                 fill = "white", 
                 color = "#bdbdbd") +
    geom_point(data = all_centroids_df_temp, aes(
      x = c_x, 
      y = c_y), 
      color = "#31a354", size = 0.9) +
    geom_point(data = start_pt, aes(x = c_x, 
                                    y = c_y), 
               color = "black") + 
    geom_rect(data=d_rect, 
              aes(xmin = x1min - rect_adj$x1,# - rect_adj$s1, 
                  xmax = x1max - rect_adj$x1,# - rect_adj$s1, 
                  ymin = x2min - rect_adj$x2,# - rect_adj$s2, 
                  ymax = x2max - rect_adj$x2),# - rect_adj$s2), 
              fill = "white", 
              color = "black", 
              alpha = 0, 
              linewidth = 0.7) +
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1max - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2max - rect_adj$x2)) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,0)", 
             hjust=-0.1, vjust=-0.3, size = 8) + 
    annotate("text", x=d_rect$x1max - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,1)", 
             hjust=1.1, vjust=-0.3, size = 8) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2max - rect_adj$x2,
                     label = expression(group("(", 
                        list(0, y[2][max]),")")), 
            hjust=-0.1, vjust=1.2, size = 8) + 
    geom_segment(data=d_rect, aes(
      x = x1min  - rect_adj$x1, # 0 - 0.03, 
      y = -0.31, 
      xend = x1max - rect_adj$x1, #1 - 0.03, 
      yend = -0.31), #-0.35),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
                 color = "black")+
    annotate("text", x=0.5, y=-0.36, 
             label = expression(r[1]), color = "black", size = 8) +
    geom_segment(data=d_rect, aes(
      x = -0.25, 
      y = x2min - rect_adj$x2, #0 - 0.05, 
      xend = -0.25, 
      yend = x2max - rect_adj$x2), #r2 - 0.05),
      arrow = arrow(length = unit(0.03, "npc"),
                       ends = "both"), 
                 color = "black")+ 
    annotate("text", x=-0.3, y=0.4, 
             label = expression(r[2]), color = "black", size = 8) +
    geom_segment(data = a1, aes(
      x = x, #-0.1 + 0.2087578, 
      y = y, #-0.15, 
      xend = xend, #-0.1 + 0.2087578*2, 
      yend = yend), #-0.15),
      arrow = arrow(length = unit(0.03, "npc"),
        ends = "both"), 
        color = "black")+ # a1 = 0.2087578
    annotate("text", 
             x=(a1$x+a1$xend)/2, 
             y=a1$y, 
             label = expression(a[1]), 
             color = "black",
             vjust = 1.2, size = 8) +
    geom_segment(data = a2, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + # a2 = 0.1807896
    annotate("text", x=a2$x, y=(a2$y+a2$yend)/2, 
             label = expression(a[2]), 
             color = "black", hjust=-0.2, size = 8) +
    annotate("text", x=-0.18, y=-0.24, 
      label = expression(group("(", list(s[1], s[2]), ")")),
      color = "black", size = 8) +
  geom_segment(data = l, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + 
    annotate("text", x=l$x + 0.03, y=(l$y+l$yend)/2, 
             label = expression(l), 
             color = "black", hjust=-0.2, size = 8) +
  coord_equal()
```

```{r}
#| label: fig-hex-param
#| fig-cap: "The components of the hexagon grid illustrating notation."
#| out-width: 30%
#| fig-pos: H
 
hex_param_vis
```

<!-- Number of bins is set by fixing $b_1$, which determines the binwidth accounting the offset $q_1$ and breaks on $x_1$, is calculated as  -->

<!-- $$ -->
<!-- a_1 = \frac{r_1 + q_1}{b_1}. -->
<!-- $$ -->

<!-- The computed binwidth then determines the number of vertical bins, $b_2$, and vertical binwidth, which are computed based on $r_2$, the offset $q_2$, and the height/width ratio of a regular hexagon, $\frac{2}{\sqrt{3}}$ as -->

<!-- $$ -->
<!-- a_2 = \frac{r_2 + q_2}{0.75 \times b_2}. -->
<!-- $$ -->

<!-- To define a hexagon grid across the \kD{} space, with hexagons of fixed height ($a_1$), width ($a_2$),  it is necessary to determine how many hexagons should be represented along each axis in the \kD{} space.

When $k=2$, the number of bins along the $x$ and $y$ axes, $b_1$ and $b_2$, is computed by considering the scaling factors ${r_1, r_2}$, along with the offset along the axes ($q_1$, $q_2$), and the height ($a_1$) and width ($a_2$) of a regular hexagon (@fig-NLDR-two-curvy (b)). ($b_1 = \frac{r_1 + q_1}{a_2}$, and $b_2 = \frac{r_2 + q_2}{0.75 \times a_1}$) where $0.75 \times a_1$ accommodate to have hexagons fill the entire 2D space without leaving any gaps between them.)-->

<!-- XXX We don't need parameters for the regular hexagon, these are fixed constants. -->


#### Binning the data

<!-- Points are allocated to the bin they fall into based on the nearest centroid. In situations where a point is equidistant from multiple centroids, tie-breaking rules are applied. If multiple centroids are in the same row, the point is assigned to the leftmost centroid. If multiple centroids are in different rows, the point is assigned to the bottom centroid. -->

<!-- $\{ i \in H_h, h = 1, \dots, b, \text{ and } i = 1, \dots, n\}$ -->

Observations are grouped into bins based on their nearest centroid. This produces a reduction in size of the data from $n$ to $m$, where $m\leq b$ (total number of bins). This can be defined using the function $u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}$, where
$u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2}$, mapping observation $i$ into $H_h = \{i| u(i) = h\}$. 

By default, the bin centroid is used for describing a hexagon (as done in @fig-NLDR-two-curvy (c)), but any measure of center, such as a mean or weighted mean of the points within each hexagon, could be used. The bin centers, and the binned data, are the two important components needed to render the model representation in high dimensions.  

<!-- XXX How are you doing this? Do you check the bounds of the hexagon? Or do you use distance to centroid? -->

<!--
Define a hexagon grid across the \kD{} space, using hexagons with fixed height ($a_2$), width ($a_1$). Each of the \kD{} points will belong to a hexagon bin. That is, for each $y \in \mathbfit{Y}$, we can (uniquely) identify the hexagon that the point belongs to. This identification is done finding the nearest bin centroid for the \kD{} points by considering the \kD{} Euclidean distance.    

When $k=2$, the starting coordinates $(s_1, s_2)$ mark the lower left of the grid. This is the bottom left bin centroid. By starting from there, points are generated to fill the grid accounting $b_1$, $b_2$, $a_1$, and $a_2$. 
-->

<!--We deliberately separate out the creation of the hexagon grid from the mapping of points on the grid.-->

#### Indicating neighborhood

Delaunay triangulation [@lee1980;@alb2024] is used to connect points so that edges indicate neighbouring observations, in both the NLDR layout (@fig-NLDR-two-curvy (d)) and the \pD{} model representation. When the data has been binned the triangulation connectd centroids. The edges preserve the neighborhood information when the model is lifted into \pD{}. 

<!-- When $k = 2$ Delaunay triangulation on $C^{(2)}$ generates the model in \gD{} space, which is a triangular mesh (@fig-NLDR-two-curvy (d)). It generates convex hulls of $C^{(2)}$ such that the circumcircle of every triangle in the triangulation contains no other points from $C^{(2)}$. -->

When shapes are non-linear in the NLDR layout, some edges could be long. It can also happen that distant centroids can be connected, particularly if clustering is present, which can result in long line segments. In order to generate a smooth surface in \gD{}, these long line segments should be removed when tuning the model fit.

<!--need to add what is meant by a long edge-->

### Rendering the model in \pD{}

The last step is to lift the \kD{} model into \pD{} by computing \pD{} vectors that represent bin centroids. We use the \pD{} mean of the points in $H_h$ to map the centroid $C_{h}^{(2)} = (c_{h1}, c_{h2})$ to a point in \pD{}. Let the \pD{} mean be

$$C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.$$
Furthermore, line segments that exist in the \kD{} model generate line segments in \pD{} by connecting the \pD{} means of the corresponding \kD{} bin centroids. If additional long edges need to be removed, compute the edges in \pD{} and pruned any detected long edges to improve the accuracy. Once pruned, re-plot the \gD{} view to ensure it accurately captures the data.

```{r}
#| label: hexbin-regular-two-curvy2

num_bins_x_two_curvy <- 15

algo_obj_two_curvy2 <- fit_highd_model(
  highd_data = training_data_two_curvy, 
  nldr_data = tsne_two_curvy, 
  bin1 = num_bins_x_two_curvy, 
  q = 0.1, 
  benchmark_highdens = 5)

tr_from_to_df_two_curvy2 <- algo_obj_two_curvy2$trimesh_data
df_bin_centroids_two_curvy2 <- algo_obj_two_curvy2$model_2d
df_bin_two_curvy2 <- algo_obj_two_curvy2$model_highd

hex_grid_two_curvy2 <- algo_obj_two_curvy2$hb_obj$hex_poly
counts_df_two_curvy2 <- algo_obj_two_curvy2$hb_obj$std_cts

hex_grid_with_counts_two_curvy2 <- left_join(hex_grid_two_curvy2, counts_df_two_curvy2, by = c("hex_poly_id" = "hexID"))

hex_grid_coloured_two_curvy2 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_two_curvy2, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", 
        linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
           aes(x = emb1, y = emb2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos, cex = 2)

trimesh_two_curvy2 <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy2,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = tSNE1,
               y = tSNE2
             ),
             color = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  interior_annotation("a1", sc_ltr_pos) 

```

```{r}
#| label: hexbin-regular-two-curvy3

num_bins_x_two_curvy <- 48

algo_obj_two_curvy3 <- fit_highd_model(
    highd_data = training_data_two_curvy, 
    nldr_data = tsne_two_curvy, 
    bin1 = num_bins_x_two_curvy, 
    q = 0.1, 
    benchmark_highdens = 5)

tr_from_to_df_two_curvy3 <- algo_obj_two_curvy3$trimesh_data
df_bin_centroids_two_curvy3 <- algo_obj_two_curvy3$model_2d
df_bin_two_curvy3 <- algo_obj_two_curvy3$model_highd

hex_grid_two_curvy3 <- algo_obj_two_curvy3$hb_obj$hex_poly
counts_df_two_curvy3 <- algo_obj_two_curvy3$hb_obj$std_cts

hex_grid_with_counts_two_curvy3 <- left_join(hex_grid_two_curvy3, counts_df_two_curvy3, by = c("hex_poly_id" = "hexID"))

hex_grid_coloured_two_curvy3 <-  ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_two_curvy3, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
           aes(x = emb1, y = emb2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "D") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos, cex = 2)
```


<!-- XXX You need to have consistent colouring. In langevitour the points corresponding to observations are purple. They should be purple in the \gD{} view too.  -->

<!-- XXX Should there be a second long edge removal? You can compute the edges in \pD{} now, and if there are long edges maybe these should be pruned, and we re-plot the \gD{} view.  -->

<!-- langevitour with pD model for two-curvy clusters -->
```{r}
#| label: best-fit-tsne
#| fig-cap: "Lifting the fitted model into \\pD{}. Starting from a default tSNE layout (a) of non-linear clusters data ($n =  2000$ and $p = 4$), from which the \\gD{} wireframe is constructed (a) and then lifted to $4\\text{-}D$ (b, c). The fit is reasonbly tight with the data, although it does not fully spread the full width. This highlights a key characteristic of tSNE, which compresses the data when applying tSNE. Furthermore, the sparse space observed in the middle of the tSNE layout is a result of (hyper-)parameter choice. Video of the langevitour animation is available at <>."
#| fig-width: 8
#| fig-height: 2

#nldr_two_curvy2 + 
  trimesh_two_curvy_removed1_tsne_with_data +
  two_curvy_proj_tsne_first_model1 +
  two_curvy_proj_tsne_all_model1 +
  plot_layout(ncol = 3)
```


### Measuring the fit {#sec-summary}
 <!-- Fitted values,  Error calculation-->

The model here is similar to a confirmatory factor analysis model [@brown2015], $\widehat{T}(X_1, X_2, X_3, X_4) + \Epsilon$. The difference between the fitted model and observed values would be considered to be residuals, and for this problem are $4\text{-}D$. 

<!--#### Fitted values-->

Observations are associated with their bin center, $C_{h}^{(p)}$, which are also considered to be the *fitted values*. These can also be denoted as $\widehat{X}$. <!--The fitted values of the points in $H_h$ refers to the \pD{} mapping $C_{h}^{(p)}$ of the corresponding \kD{} model point $C_{h}^{(2)}$.-->

<!--#### Error-->

The error is computed by taking the squared \pD{} Euclidean distance, corresponding to computing the root mean squared error (RMSE) as:

$$\sqrt{\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2}$${#eq-equation1} 

where $n$ is the number of observations, $b$ is the number of bins, $n_h$ is the number of observations in $h^{th}$ bin, $p$ is the number of variables, $\mathbfit{x}_{hij}$ is the $j^{th}$ dimensional data of $i^{th}$ observation in $h^{th}$ hexagon. We can consider $e_{hj} = \sqrt{\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2}$ to be the residual for each observation.

```{r}
#| label: error-two-curvy

error_two_curvy_umap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_tsne.rds")

## Find the minimum MSE when have duplicate a1
error_two_curvy_umap <- error_two_curvy_umap |>
  group_by(a1) |>
  filter(bin1 == min(bin1)) |>
  ungroup() |>
  mutate(prop_dens = 1/(b*side_length^2)) 

base_line_prop_dens <- error_two_curvy_umap |>
  filter(a1 == min(a1)) |>
  pull(prop_dens)

error_two_curvy_umap <- error_two_curvy_umap |>
  mutate(prop_comp = prop_dens/base_line_prop_dens)

mse_two_curvy_b <- ggplot(error_two_curvy_umap, 
                     aes(x = a1, 
                         y = sqrt(MSE))) +
  geom_vline(xintercept = 0.09,
           color = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             color = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) + 
  geom_point(size = 1) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = "RMSE") +
  interior_annotation("a", position = c(0.9, 0.15), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))


prop_dens_a1 <- ggplot(error_two_curvy_umap,
                     aes(x = a1,
                         y = prop_comp)) +
  geom_vline(xintercept = 0.09,
           color = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             color = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = paste("Relative proportion density")) +
  interior_annotation("d", position = c(0.9, 0.9), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))

error_two_curvy_umap <- error_two_curvy_umap |>
  mutate(prop_bins = b_non_empty/b)

a1_m_two_curvy <- ggplot(error_two_curvy_umap,
                     aes(x = a1,
                         y = prop_bins)) +
  geom_vline(xintercept = 0.09,
           color = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             color = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             color = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = expression("prop. non-empty bins " * bgroup("(", m / b, ")"))) +
  interior_annotation("b", position = c(0.9, 0.15), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))
```

```{r}
#| label: fig-p-d-error-in-2d-two-curvy
#| fig-cap: "The $4\\text{-}D$ model error in \\gD{} layout. Color indicates error ($e_{hj}$), dark colour indicating high error and light indicates low error. Most large errors are distributed near the sparse end of the non-linear cluster with dense corner."
#| out-width: 80%
#| fig-pos: H

error_plot_two_curvy + error_plot_two_curvy_quasi +
  plot_layout(ncol=2)
```

### Prediction into \gD{}

A new benefit of this fitted model is that it allows us to now predict a new observation's value in the NLDR, for any method. The steps are to determine the closest bin centroid in \pD{}, $C^{(p)}_{h}$ and predict it to be the centroid of this bin in \gD{}, $C^{(2)}_{h}$. This can be written as, let $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$, then the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. 

<!--prediction for true-model of two-curvy clust-->
```{r}
#| label: model-prediction-umap-original

predict_tsne_two_curvy <- read_rds(file = "data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_predict_true.rds")

predict_two_curvy_obj <- gen_scaled_data(
    nldr_data = predict_tsne_two_curvy)

predict_tsne_two_curvy_scaled <- predict_two_curvy_obj$scaled_nldr

plot_predict_umap_fun_two_curvy <- ggplot(
  data = predict_tsne_two_curvy_scaled, 
  aes(x = UMAP1, 
      y = UMAP2)) +
  geom_point(alpha = 0.5) +
  interior_annotation("b", c(0.08, 0.93), cex = 1.5)
```

```{r}
#| label: model-prediction-points-line
#| eval: false

predict_tsne_two_curvy_scaled <- predict_tsne_two_curvy_scaled |>
  mutate(ID = row_number()) |>
  mutate(type = "predict_from_umap") 

jittered_points_df <- jittered_points_df |>
  mutate(type = "predict_from_model") 

predict_point_df <- bind_rows(
  predict_tsne_two_curvy_scaled,
  jittered_points_df
)

plot_predict_umap_connect_two_curvy <- ggplot(
  data = predict_point_df, 
  aes(x = UMAP1, 
      y = UMAP2, 
      group = ID)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.3) +
  interior_annotation("d", c(0.08, 0.93), cex = 1.5)
```



```{r}
#| echo: false
#| label: fig-predict-two-curvy
#| fig-pos: H
#| fig-width: 10
#| fig-height: 10
#| out-width: 80%
#| fig-cap: "Comparison of prediction generated using the exiting `umap` R package's prediction method and our method: (a) A view of the true model in projection from $7\\text{-}D$, (b) predicted data from the `umap` prediction method, and (c) predicted data from our method."
#| eval: false

proj_plot_two_curvy_true + plot_predict_umap_fun_two_curvy +plot_predict_tsne_two_curvy +
  plot_layout(ncol = 3)
```


<!--The prediction approach involves finding the nearest \kD{} model point for a new \pD{} point. We define the function $z: \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{m\times p}$, where $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$ maps each \pD{} point to its nearest \pD{} mapping of the model. Therefore, the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$ be the predicted values.-->  



<!-- ::: {#fig-two_curvy-pred-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/two_curvy/sc_true_only.png){width="150" fig-align="center"} -->

<!-- ![](figures/two_curvy/pred_true_view.png){width="150" fig-align="center"} -->


<!-- A view of the true model in projections from $7\text{-}D$, and predictions of the true model in \gD{}, for the S-curve data. The predictions fits the UMAP layout which means that it capture the geometry of S-curve with UMAP. -->
<!-- ::: -->

<!-- XXX Prediction should have a jitter option, to spread the predicted points out fully in the hexagons. -->

### Tuning
<!-- removal of low density bins, removing long edges, choice of bins-->

The model fitting can be adjusted using these parameters: 

- hexagon bin parameters
    - bottom left bin position $(s_1, \ s_2)$, 
    - the total number of bins ($b$), 
- bin density cutoff, to remove low-density hexagons.

Default values are provided for each of these, but it is expected that the user will examine the MSE for a range of choices. Choosing these parameters according to MSE can be automated but it is recommended that the user examine the resulting model representation by overlaying it on the data in \pD{}. The next few subsections describe the calculation of default values, and the effect that different choices have on the model fit.

#### Hexagon bin parameters

The values $(s_1, \ s_2)$ define the position of the centroid of the bottom left hexagon. By default, this is at $s_1 = -q, s_2 = -qr_2$, where $q$ is the buffer bound the data. The choice of these values can have some effect on the distribution of bin counts. @fig-bins-two-curvy illustrates this. The distribution of bin counts for $s_1$ varying between $-0.1-0.0$ is shown. Generally, a more uniform distribution among these possibilities would indicate that the bins are reliably capturing the underlying distribution of observations. 

<!--The starting position of the hexagonal grid is important because different starting points result in different distributions of data across bins, even with the same total number of bins. This variation affects the model due to the differing number of non-empty bins. Therefore, it is necessary to evaluate various starting points with different total numbers of bins to determine which configurations are more effective at capturing the structure and fitting the model.--> 

```{r}
#| echo: false
#| label: fig-bins-two-curvy
#| fig-pos: H
#| fig-cap: "Hexbin density plots of tSNE layout of the non-linear cluster data, using three different bin inputs: (a) $b = 240/98 \\text{ } (15, \\text{ }16)$, (b) $b = 720/215 \\text{ } (24, \\text{ }30)$, and (c) $b = 2496/549 \\text{ } (48, \\text{ }52)$. Color indicates standardized counts, dark indicating high count and light indicates low count. At the smallest bin size, the data structure is discontinuous, suggesting that there are too many bins. Using the MSE of the model fit in $7\\text{-}D$ helps decide on a useful choice of number of bins."
#| fig-width: 9
#| fig-height: 3

hex_grid_coloured_two_curvy2 +
  hex_grid_coloured_two_curvy1 +
  hex_grid_coloured_two_curvy3 +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 12))
``` 


The default number of bins $b=b_1\times b_2$ is computed based on the sample size, by setting $b_1=n^{1/3}$, consistent with the Diaconis-Freedman rule [@freedman1981]. The value of $b_2$ is determined analytically by $b_1, q, r_2$. Values of $b_1$ between $2$ and $b_1 = \sqrt{n/r_2}$ are allowed. @fig-param-two-curvy (a) shows the effect of different choices of $b_1$ on the MSE of the fitted model.

<!-- To determine the effective $b$, candidate values are selected based on the range between the minimum and approximate maximum $b_1$, because $b_2$ is computed from $b_1$. The minimum $b_1$ is set to $2$, while the maximum number is estimated by taking the square root of $\frac{n}{2}$. By evaluating MSE across varying $b$ within this range for different $q$, helps to determine an appropriate values for $b$ and $q$ (@fig-param-two_curvy (a)).--> 

<!--add MSE vs total number of error plot-->

<!--To generate errors for different total number of bins-->

#### Measurement of capturing the data shape in 2-D 

The area of a hexagon is defined as $A = 3\sqrt{3}l^2/2$ where $l$ is the side length of the hexagon. If we know $a_1$ and $a_2$, $l$ can be computed (see appendix). The density of a hexagon grid is calculated as $\sum^{h}_{i=1}n_h/A$ and the proportion is $\sum^{h}_{i=1}n_h/Ab$. The baseline proportion is the proportion density at the smallest possible value of $a_1$. The relative proportion density is the ratio of the observed proportion density to the baseline proportion density.

#### Removal of low density bins

By default, when assessing the choice of $b_1$, the total number of bins is measured by the number of **non-empty** bins. This more accurately reflects the hexagon grid relative the MSE than the full number of bins in the grid. It may also be beneficial to remove low count bins also, in the situation where data is clustered or stringy, where the observed data is sparse. In order to decide if this is necessary, you would examine the distribution of bin counts, or the density which puts the counts on a standard scale. If there is something of a gap at low values, this would suggest a potential value to use as a cutoff. Alternatively, one could choose to remove based on a percentile, the bins with density in the lowest 5% of all bins, for example. @fig-param-two-curvy (c) illustrates the effect on the model representation of removing bins below different percentages. Generally, we would urge caution in removing low count bins. 

<!-- Once setting up the hexagon grid with an appropriate number of bins, some hexagon bins may have few or no data points within them (@fig-bins-two_curvy (b)). To ensure comprehensive coverage of the NLDR data, it is necessary to select hexagon bins with a considerable number of data points. This involves calculating the number of points within each hexagon. Then, the standard count is computed by dividing the number of points within each hexagon by the maximum number of points in the grid. Next, bins with a standard count less than a benchmark value are removed (@fig-param-two_curvy (c)). There is no specific rule for selecting a benchmark value. However, the following steps can help determine a suitable value for removing low-density hexagons:

1. Plot the distribution of the standardized counts (@fig-param-two_curvy (b)).
2. Examine the distribution of counts.
3. Select the first quantile value if the distribution is skewed.
-->



The benchmark value for removing low-density hexagons ranges between $0$ and $1$. When analyzing how these benchmark values influence model performance, it's essential to observe the change in MSE as the benchmark value increases (@fig-param-two-curvy (c)). The MSE shows a gradual decrease as the benchmark value goes from $1$ to $0$. Evaluating this rate of increase is important. If the increment is not considerable, the decision might lean towards retaining low-density hexagons.

<!--add MSE vs density (0 to 1)-->



<!--base line -->

<!-- Furthermore, selecting the benchmark value for removing low-density hexagons is important. Removing unnecessary bins may lead to the formation of long edges and an uneven \gD{} model. Hence, rather than solely relying on the benchmark value to identify hexagons for removal, it's essential to consider the standard number of points in the neighboring hexagons of the identified low-density bins (see @fig-lwd-two_curvy (b)). If neighboring bins also show low counts, only those bins will be removed. The remaining bins are used to construct the \gD{} model.    -->

```{r}
#| label: rm-lwd-bin-error

error_rm_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_rm_lwd_diff_bin.rds") |>
  mutate(bin1  = as.factor(bin1))

mse_two_curvy_lwd <- ggplot(error_rm_two_curvy, 
                     aes(x = benchmark_rm_lwd, 
                         y = RMSE,
                         color = a1,
                         linetype = a1)) + 
  geom_point(
    size = 1
    ) +
  geom_line(
    linewidth = 0.5
    ) + 
  scale_x_continuous(breaks = sort(unique(error_rm_two_curvy$benchmark_rm_lwd))[c(1, 3, 5, 7, 9, 11, 13, 15, 17)]) +
  scale_color_manual(values=c("#d95f02", "#1f78b4", "#1b9e77"),
                     labels = parse_format()) +
  scale_linetype_manual(values = c("dashed", "solid", "dotted"),
                        labels = parse_format()) +
  xlab("threshold to remove low-density hexagons") +
  ylab("RMSE") +
  interior_annotation("c", position = c(0.1, 0.9), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))
```


```{r}
#| echo: false
#| fig-cap: "Various plots to help assess best hexagon bin parameters (a, b, d) and thresholds to remove low-density bins (c). Both (a) and (c) show RMSE, against binwidth ($a_1$) and threshold. A good benchmark value for these parameters is when the RMSE drops and then flattens out. Three binwidth choices were made: $0.03$ (orange dashed), $0.05$ (blue solid), and $0.09$ (green dotted) to investigate. As the binwidth increases, the proportion of non-empty bins also increases (b). The relative proportion density decreases and levels off (d). Binwidth $0.05$ is chosen as the initial best binwidth for further analysis. There is no need to remove the low-density hexagons because as shown in (b), there is no considerable drop in RMSE."
#| label: fig-param-two-curvy
#| out-width: 80%
#| fig-width: 12
#| fig-height: 10
#| fig-pos: H

mse_two_curvy_b + a1_m_two_curvy +
  mse_two_curvy_lwd + prop_dens_a1 +
  plot_layout(ncol=2, guides = "collect")
```

<!-- #### Removing long edges -->

<!-- Edges define the neighbourhood structure, in order to provide a smooth \gD{} representation of the fitted model. @fig-two-curvy-true-proj shows a wire frame of the true model that was used to generate the non-linear clusters example data. The ideal is that the representation of the fitted model, at least for this example where we know the true model, should look similar to this.  -->

<!-- The Delaunay triangulation will ensure that all centroids are connected into a triangular mesh. For some structures, like clustered data, or highly non-linear shapes, breaks in the mesh are meaningful. When separated clusters are present the mesh should be broken across the gaps. For non-linear structures like the C-shaped curvilinear, the mesh should run unbroken along the C, but there should be no edges connecting the top of the C directly to the bottom of the C. For these reasons it is necessary to remove edges from the mesh in some applications. -->

<!-- The decision on edge length removal is made based on the distribution of edge lengths. In particular, a gap between values, where there a concentration of small values and then a few larger values, likely suggests a cutoff for edge removal. Because the triangulation is typically done on the hexagon centroids, there are particular discrete edge lengths, based on bin widths. @fig-rm-lg illustrates edge length distributions.  -->

<!-- There is an additional step that is needed. When the model is lifted into \pD{}, if the fit is good all the edges should be relatively small in this space, too. If this is not the case, then there are several possible actions: (1) re-do the NLDR to get a more representative layout; (2) identify the edge and remove it from the model, in \gD{} and \pD{}; (3) consider different values for the model fit, number of bins, initial bin position or removing low density bins. -->

<!-- ```{r} -->
<!-- #| label: model-two-curvy-umap -->

<!-- umap_two_curvy <- read_rds(file = "data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_n-neigbors_15_min-dist_0.1.rds") -->

<!-- num_bins_x_two_curvy <- 42 -->

<!-- algo_obj_two_curvy3 <- gen_nldr_vis_algo_obj( -->
<!--   high_d_data = training_data_two_curvy,  -->
<!--   nldr_data = umap_two_curvy,  -->
<!--   num_x_bins = num_bins_x_two_curvy) -->

<!-- umap_two_curvy_scaled <- algo_obj_two_curvy3$nldr_scaled -->
<!-- distance_two_curvy <- algo_obj_two_curvy3$distance_df -->
<!-- tr_from_to_df_two_curvy <- algo_obj_two_curvy3$tr_from_to_df -->
<!-- benchmark_two_curvy <- algo_obj_two_curvy3$benchmark -->
<!-- df_bin_centroids_two_curvy <- algo_obj_two_curvy3$df_bin_centroids -->
<!-- df_bin_two_curvy <- algo_obj_two_curvy3$df_bin -->
<!-- hex_grid_with_counts <- algo_obj_two_curvy3$hex_grid_with_counts -->
<!-- a1 <- algo_obj_two_curvy3$a1 -->

<!-- trimesh_two_curvy_umap <- ggplot() + -->
<!--   geom_segment(data = tr_from_to_df_two_curvy, -->
<!--                aes( -->
<!--                  x = x_from, -->
<!--                  y = y_from, -->
<!--                  xend = x_to, -->
<!--                  yend = y_to), -->
<!--                colour = "#000000", -->
<!--                alpha = 0.5, -->
<!--                linewidth = 0.5) + -->
<!--   geom_point(data = umap_two_curvy_scaled, -->
<!--              aes( -->
<!--                x = UMAP1, -->
<!--                y = UMAP2 -->
<!--              ), -->
<!--              color = "#636363", -->
<!--              alpha = 0.5, -->
<!--              size = 0.5 -->
<!--   ) + -->
<!--   coord_equal() + -->
<!--   xlim(c(0, 1)) + -->
<!--   ylim(c(0, 1)) + -->
<!--   interior_annotation("a1", sc_ltr_pos, cex = 2) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # With the computed benchmark value -->
<!-- # distance_df_small_edges_two_curvy <- distance_two_curvy |> -->
<!-- #   filter(distance < benchmark_two_curvy) -->

<!-- ####With diff benchmark values -->

<!-- ## Benchmark 2 -->

<!-- benchmark_two_curvy <- 8 * a1 -->

<!-- distance_df_small_edges_two_curvy3 <- distance_two_curvy |> -->
<!--   filter(distance < benchmark_two_curvy) -->

<!-- tr_from_to_df_two_curvy4 <- inner_join( -->
<!--   tr_from_to_df_two_curvy, distance_df_small_edges_two_curvy3, -->
<!--   by = c("from", "to")) -->

<!-- trimesh_two_curvy_removed2_umap <- ggplot() + -->
<!--   geom_segment(data = tr_from_to_df_two_curvy4, -->
<!--                aes( -->
<!--                  x = x_from, -->
<!--                  y = y_from, -->
<!--                  xend = x_to, -->
<!--                  yend = y_to), -->
<!--                colour = "#000000", -->
<!--                alpha = 0.5, -->
<!--                linewidth = 0.5)+ -->
<!--   geom_point(data = umap_two_curvy_scaled, -->
<!--              aes( -->
<!--                x = UMAP1, -->
<!--                y = UMAP2 -->
<!--              ), -->
<!--              color = "#636363", -->
<!--              alpha = 0.5, -->
<!--              size = 0.5 -->
<!--   ) + -->
<!--   coord_equal() + -->
<!--   xlim(c(0, 1)) + -->
<!--   ylim(c(0, 1)) + -->
<!--   interior_annotation("b1", sc_ltr_pos, cex = 2) -->

<!-- ## Benchmark 1 -->

<!-- benchmark_two_curvy <- 2.5 * bin_width -->

<!-- distance_df_small_edges_two_curvy2 <- distance_two_curvy |> -->
<!--   filter(distance < benchmark_two_curvy) ## 0.231 -->

<!-- tr_from_to_df_two_curvy3 <- inner_join( -->
<!--   tr_from_to_df_two_curvy, distance_df_small_edges_two_curvy2, -->
<!--   by = c("from", "to")) -->

<!-- trimesh_two_curvy_removed1_umap <- ggplot() + -->
<!--   geom_segment(data = tr_from_to_df_two_curvy3, -->
<!--                aes( -->
<!--                  x = x_from, -->
<!--                  y = y_from, -->
<!--                  xend = x_to, -->
<!--                  yend = y_to), -->
<!--                colour = "#000000", -->
<!--                alpha = 0.5, -->
<!--                linewidth = 0.5)+ -->
<!--   geom_point(data = umap_two_curvy_scaled, -->
<!--              aes( -->
<!--                x = UMAP1, -->
<!--                y = UMAP2 -->
<!--              ), -->
<!--              color = "#636363", -->
<!--              alpha = 0.5, -->
<!--              size = 0.5 -->
<!--   ) + -->
<!--   coord_equal() + -->
<!--   xlim(c(0, 1)) + -->
<!--   ylim(c(0, 1)) + -->
<!--   interior_annotation("c1", sc_ltr_pos, cex = 2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| label: compare-2d-and-4d-dist -->

<!-- ### Compute highD distance -->
<!-- dist_vec <- proxy::dist(x = df_bin_two_curvy[, -1], method = "Euclidean") |> as.vector() -->

<!-- from_vec <- c() -->
<!-- to_vec <- c() -->
<!-- num_obs <- 1:(NROW(df_bin_two_curvy) - 1) -->

<!-- for (obs in num_obs) { -->

<!--   from_val <- rep(obs, (NROW(df_bin_two_curvy) - obs)) -->
<!--   if ((obs + 1) <= NROW(df_bin_two_curvy)) { -->
<!--     to_val <- (obs + 1):NROW(df_bin_two_curvy) -->
<!--   } -->
<!--   from_vec <- append(from_vec, from_val) -->
<!--   to_vec <- append(to_vec, to_val) -->

<!-- } -->

<!-- dist_highd <- tibble::tibble(from = from_vec, to = to_vec, dist_highd = dist_vec) -->


<!-- ## To plot the distribution of distances -->

<!-- # Define bin width and starting point -->
<!-- start_point <- a1/2 -->

<!-- # Create bins and calculate the mean value for each bin range -->
<!-- bins <- seq(start_point, max(distance_two_curvy$distance) + a1, -->
<!--             by = a1) -->
<!-- bin_labels <- bins[-length(bins)] + a1 / 2  # mean of each bin range -->

<!-- distance_two_curvy_dist <- distance_two_curvy |> -->
<!--   mutate(bin = cut(distance, breaks = bins, include.lowest = TRUE, labels = bin_labels)) -->

<!-- distance_two_curvy_dist <- left_join(distance_two_curvy_dist, dist_highd, by = c("from", "to")) -->

<!-- text_df <- tibble::tibble( -->
<!--   x = c(0.12, 0.43), -->
<!--   y = c(10, 10), -->
<!--   text = c("c", "b") -->
<!-- ) -->

<!-- distance_points_umap <- ggplot( -->
<!--   distance_two_curvy_dist, -->
<!--   aes(x = distance, -->
<!--       y = dist_highd)) + -->
<!--   geom_point(alpha = 0.5) + -->
<!--   geom_text( -->
<!--     data=text_df, -->
<!--     aes(x=x, y=y, -->
<!--         label=text), -->
<!--     colour="#bdbdbd", -->
<!--     size = 10) + -->
<!--   geom_vline(xintercept = 8 * bin_width, -->
<!--              linetype="solid", -->
<!--              color = "#bdbdbd", -->
<!--              linewidth=1) + -->
<!--   geom_vline(xintercept = 2.5 * bin_width, -->
<!--              linetype="solid", -->
<!--              color = "#bdbdbd", -->
<!--              linewidth=1) + -->
<!--   ylab(expression(d^{(4)})) + -->
<!--   xlab(expression(d^{(2)})) + -->
<!--   theme_minimal() + -->
<!--   theme(aspect.ratio = 0.75, -->
<!--         panel.border = element_rect(fill = 'transparent'), -->
<!--         plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5), -->
<!--         panel.grid.major.x = element_blank(), -->
<!--         axis.ticks.x = element_line(), -->
<!--         axis.ticks.y = element_line(), -->
<!--         axis.title = element_text(size = 15), -->
<!--         axis.text = element_text(size = 12)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| label: proj-tsne-model -->

<!-- df_bin_two_curvy <- df_bin_two_curvy |> -->
<!--   select(-hexID) |> -->
<!--   mutate(type = "model") -->

<!-- # Apply the scaling -->
<!-- df_model_data_two_curvy <- bind_rows(data_two_curvy, df_bin_two_curvy) -->

<!-- scaled_two_curvy <- scale_data_manual(df_model_data_two_curvy, "type") |> -->
<!--   as_tibble() -->

<!-- scaled_two_curvy_data <- scaled_two_curvy |> -->
<!--   filter(type == "data") |> -->
<!--   select(-type) -->

<!-- scaled_two_curvy_data_model <- scaled_two_curvy |> -->
<!--   filter(type == "model") |> -->
<!--   select(-type) -->


<!-- ## First projection -->
<!-- projection <- cbind( -->
<!--   c(0.37847,-0.06517,0.04231,0.01655), -->
<!--   c(-0.06466,-0.27052,0.22131,-0.15235)) -->

<!-- proj_obj1 <- get_projection(projection = projection,  -->
<!--                             proj_scale = 1,  -->
<!--                             scaled_data = scaled_two_curvy_data,  -->
<!--                             scaled_data_model = scaled_two_curvy_data_model,  -->
<!--                             distance_df_small_edges = distance_df_small_edges_two_curvy2,  -->
<!--                             axis_param = list(limits = 0.6,  -->
<!--                                               axis_scaled = 2, -->
<!--                                               axis_pos_x = -0.5,  -->
<!--                                               axis_pos_y = -0.5,  -->
<!--                                               threshold = 0.04)) -->

<!-- ## Projection with Delauany triangulation -->

<!-- projected_model <- as.matrix(scaled_two_curvy_data_model) %*% projection_scaled -->

<!-- projected_model_df <- projected_model |> -->
<!--   tibble::as_tibble(.name_repair = "unique") |> -->
<!--   dplyr::rename(c("proj1" = "...1", -->
<!--                   "proj2" = "...2")) |> -->
<!--   dplyr::mutate(ID = dplyr::row_number()) -->

<!-- model_df1 <- dplyr::left_join( -->
<!--   distance_two_curvy |> select(-distance), -->
<!--   projected_model_df, -->
<!--   by = c("from" = "ID")) -->

<!-- names(model_df1)[3:NCOL(model_df1)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from") -->

<!-- model_df1 <- dplyr::left_join(model_df1, projected_model_df, by = c("to" = "ID")) -->
<!-- names(model_df1)[(2 + NCOL(projected_model_df)):NCOL(model_df1)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to") -->

<!-- ## With benchmark option 2 -->

<!-- model_df2 <- dplyr::left_join( -->
<!--   distance_df_small_edges_two_curvy3 |> select(-distance), -->
<!--   projected_model_df, -->
<!--   by = c("from" = "ID")) -->

<!-- names(model_df2)[3:NCOL(model_df2)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from") -->

<!-- model_df2 <- dplyr::left_join(model_df2, projected_model_df, by = c("to" = "ID")) -->
<!-- names(model_df2)[(2 + NCOL(projected_model_df)):NCOL(model_df2)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to") -->

<!-- two_curvy_proj_first_model1_umap <- plot_proj( -->
<!--     proj_obj = proj_obj1,  -->
<!--     point_param = c(0.5, 0.2, "#636363"), # size, alpha, color -->
<!--     line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha -->
<!--     plot_limits = c(-0.6, 0.6),  -->
<!--     title = "c2",  -->
<!--     cex = 2,  -->
<!--     position = sc_ltr_pos, -->
<!--     axis_text_size = 4,  -->
<!--     is_category = FALSE) + -->
<!--     theme( -->
<!--         legend.position = "none" -->
<!--     ) -->

<!-- ## Projection with Delauany triangulation -->

<!-- proj_obj1_temp1 <- proj_obj1 -->
<!-- proj_obj1_temp2 <- proj_obj1 -->
<!-- proj_obj1_temp1[["model_df"]] <- model_df1 -->

<!-- two_curvy_proj_model_delaunay_umap <- plot_proj( -->
<!--     proj_obj = proj_obj1_temp1,  -->
<!--     point_param = c(0.5, 0.2, "#636363"), # size, alpha, color -->
<!--     line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha -->
<!--     plot_limits = c(-0.6, 0.6),  -->
<!--     title = "a2",  -->
<!--     cex = 2,  -->
<!--     position = sc_ltr_pos, -->
<!--     axis_text_size = 4,  -->
<!--     is_category = FALSE) + -->
<!--     theme(aspect.ratio = 1,  -->
<!--           legend.position = "none") -->

<!-- proj_obj1_temp2[["model_df"]] <- model_df2 -->

<!-- two_curvy_proj_model_benchmark2_umap <- plot_proj( -->
<!--     proj_obj = proj_obj1_temp2,  -->
<!--     point_param = c(0.5, 0.2, "#636363"), # size, alpha, color -->
<!--     line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha -->
<!--     plot_limits = c(-0.6, 0.6),  -->
<!--     title = "b2",  -->
<!--     cex = 2,  -->
<!--     position = sc_ltr_pos, -->
<!--     axis_text_size = 4,  -->
<!--     is_category = FALSE) + -->
<!--     theme(aspect.ratio = 1,  -->
<!--           legend.position = "none") -->

<!-- ``` -->

<!-- ```{r} -->
<!-- #| label: fig-rm-lg -->
<!-- #| fig-cap: "The choice of long edge removal significantly impacts the model's accuracy. \\gD{} Vs $7\\text{-}D$ distances between the bin centroids, where the model is fitted with UMAP for the non-linear clusters data. The two benchmarks for long edge removal: (b) benchmark = 8$a_1$ and (c) benchmark = 2.5$a_1$, where $a_1$ = 0.03 are made. The wireframes in \\gD{} and a projection from $7\\text{-}D$ are shown with Delaunay triangulation (a1, a2) and the two benchmark values (b1, b2, c1, c2). With the benchmark set at 6.5$a_1$, long edges exist within the C-shaped cluster, which distrupts the model. In contrast, using 2$a_1$ is a better option, as it effectively captures the non-linear shapes of the clusters. To generate an accurate model that captures the data structure, the choice of long edge removal does matter." -->
<!-- #| fig-height: 15 -->
<!-- #| fig-width: 15 -->

<!-- free(distance_points_umap) +  -->
<!--     wrap_plots(trimesh_two_curvy_umap, trimesh_two_curvy_removed2_umap, -->
<!--                trimesh_two_curvy_removed1_umap, two_curvy_proj_model_delaunay_umap, -->
<!--                two_curvy_proj_model_benchmark2_umap, two_curvy_proj_first_model1_umap, -->
<!--                ncol = 3) + -->
<!--     plot_layout(heights = c(0.8, 1)) -->
<!-- ``` -->

### Linked plots

Diagnosing the model while locating points in the \gD{} layout and displaying the generated model overlaid on data in the \pD{} is important.

The \gD{} layout and the langevitour view with the model are linked together via rectangular brushes; when a brush is active, points will be highlighted in the adjacent view. Because the langevitour is dynamic, brush events that become active will pause the animation, so that a user can interrogate the current view. The interface is constructed as a **browsable HTML widget** specifically designed for interactive data analysis.

To understand how well the model fits the points whether it fits well, works better in some positions, or fails to match the overall pattern. It is important to link and brush the points with high model error in the \pD{} error plot, \gD{} layout, and the generated model overlay on the data in \pD{}.

<!-- Brushing the high model error in the \gD{} layout, with dense C-shaped data explained in @sec-effect-dens, helps in locating the positions of the related \pD{} points in langevitour view. This process helps in identifying the high model errors that occur due to the sparse end (add the video). -->

<!-- Furthermore, when examining the digit 1 from the MNIST dataset (@sec-mnist), brushing the small cluster in the \gD{} layout helps in locating the positions of the related \pD{} points in langevitour view. This process helps in identifying how close the small cluster to the big cluster (add the video). We can also explore which data corresponds to the twisted pattern of the big cluster (add the video).  -->

## Best fit, or at least avoiding an inaccurate representation

@fig-toy-mse shows a summary of plots that can be used to help assess the strength of the fit, and compare the fits for different representations. What does it mean to be a best fit for this problem? There probably isn't a best fit, but there are wrong representations. The goal is to help users decide on a useful and appropriate low-dimensional representation of the high-dimensional data. 

Deciding on the best fit relies on several elements: 

- the choice of NLDR method, and the parameters used to create it, and
- model fit parameters: bin size, low density bin removal.

Comparing the MSE to obtain the best fit is suitable if one starts from the same NLDR representation. In theory, because the MSE is computed on \pD{} measuring the fit between model and data it might still be useful to compare different NLDR representations. A good NLDR representation should produce a good fit, producing a low MSE if the model fits the data well. However, it technically might be quite variable.

```{r}
#| label: read-two-curvy-clust-nldr
# Read a variety of different NLDR representations of two_non_linear_diff_shaped_close_clusters
# and plot them on same aspect ratio
tsne_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_30.rds")

nldr_two_curvy1 <- tsne_two_curvy |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a") +
  theme(aspect.ratio = 1) 

tsne_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_62.rds")

nldr_two_curvy6 <- tsne_two_curvy |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("f") +
  theme(aspect.ratio = 1) 

umap_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_umap_n-neigbors_15_min-dist_0.1.rds")

nldr_two_curvy2 <- umap_two_curvy |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#636363') +
  interior_annotation("b", c(0.08, 0.9)) +
  theme(aspect.ratio = 1) 

phate_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_phate_knn_5.rds")

nldr_two_curvy3 <- phate_two_curvy |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("c") +
  theme(aspect.ratio = 1) 

trimap_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_trimap_n-inliers_12_n-outliers_4_n-random_3.rds")

nldr_two_curvy4 <- trimap_two_curvy |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("d") +
  theme(aspect.ratio = 1)

pacmap_two_curvy <- read_rds("data/two_non_linear_diff_shaped_close_clusters/two_non_linear_diff_shaped_close_clusters_pacmap_n-neighbors_10_init_random_MN-ratio_0.5_FP-ratio_2.rds")

nldr_two_curvy5 <- pacmap_two_curvy |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e", c(0.08, 0.9)) +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-two_curvy

error_two_curvy_umap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_umap.rds")
error_two_curvy_tsne <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_tsne.rds")
error_two_curvy_phate <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_phate.rds")
error_two_curvy_trimap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_trimap.rds")
error_two_curvy_pacmap <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_pacmap.rds")
error_two_curvy_tsne2 <- read_rds("data/two_non_linear_diff_shaped_close_clusters/error_two_non_linear_diff_shaped_close_clusters_tsne2.rds")

error_two_curvy_tsne2 <- error_two_curvy_tsne2 |>
  mutate(method = "tSNE2")

error_two_curvy <- bind_rows(error_two_curvy_umap, 
                         error_two_curvy_tsne,
                         error_two_curvy_phate,
                         error_two_curvy_trimap,
                         error_two_curvy_pacmap,
                         error_two_curvy_tsne2)

error_two_curvy <- error_two_curvy |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-two_curvy

error_plot_two_curvy <-
  plot_mse(error_two_curvy) +
  scale_x_continuous(breaks =
    sort(unique(error_two_curvy$a1))[
      seq(1, length(
        unique(error_two_curvy$a1)), 
        by = 5)]) +
  scale_color_manual(
    values=c('#e41a1c','#ff7f00','#4daf4a', 
             "#a65628",'#636363', '#984ea3')) +
  theme(aspect.ratio = 1.5)
```

```{r}
#| fig-cap: "Assessing which of the 6 NLDR layouts on the two non-linear clusters data is the better representation using RMSE for varying binwidth ($a_1$). Colour used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-f). Layout d is universally poor. Layouts a, b, e that show two close clusters are universally suboptimal. Layout b with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. Layout e has small separation with oddly shaped clusters. Layout a is the best choice."
#| label: fig-toy-mse
#| fig-pos: H
#| out-width: 100%
#| fig-height: 6
#| fig-width: 6

error_plot_two_curvy + wrap_plots(nldr_two_curvy1, nldr_two_curvy2, nldr_two_curvy3, 
                                    nldr_two_curvy4, nldr_two_curvy5,
                                    nldr_two_curvy6, ncol = 2, 
                                    widths = c(50, 50))
```

## Curiosities

<!-- XXX THIS IS NOT INTERESTING IN ITS CURRENT FORM. THIS IS ONLY ANY INTERESTING EXAMPLE WHEN THE FILLED OUT VS FLAT SHAPES ARE DISCUSSED. -->

With the drawing of the model in the data, several interesting differences between NLDR methods can be observed.

### Ordering of points

<!-- DHC: Will need to have more bins in each of the methods to illustrate this properly. -->

To illustrate a difference in how the methods organise points in the NLDR layout, simulated $4\text{-}D$ data having five Gaussian clusters is used. @fig-five-gau-projs a1, b1, c1 show the \gD{} layouts for (a) tSNE, (b) UMAP, and (c) PaCMAP, respectively. The default hyper-parameters are used. All three methods show the five clusters, with varying degrees of separation.

The models are fitted to these layouts, but we focus on a single cluster to illustrate the curious detail. @fig-five-gau-projs a2, b2, c2 show the fitted models in a projection of the $4\text{-}D$ space. In this projection the difference between methods can be seen. These clusters are fully $4\text{-}D$ in nature, so we would expect the model to be a *crumpled \gD{} sheet* that stretches in all four dimensions. This is what is observed for tSNE and UMAP. The curious detail is that the model for PaCMAP is closer to a *pancake* in shape! What this means is that there has to be some ordering of points in the \gD{} PaCMAP layout that induces the flat model, likely that the points are organised by the global principal components. So the layout of the model in $4\text{-}D$ doesn't reflect the dimension of each cluster. This can be happen because PaCMAP balances three components (near, mid-range, and far) rather than focusing solely on near neighbors. Mid-range neighbors ensure that transitions between clusters or groups are preserved and less concern with local preservation.

<!-- DHC: is there something in the documentation that might help to explain what has happened? -->

<!-- This pattern can also be observed by examining the error from the model fit (@fig-five-gau-projs a3, b3, c3). With tSNE and UMAP the error is relatively uniformly distributed, but with PaCMAP there is more error in the center of the \gD{} layout, reflecting that there are more points in the middle that are further from the fitted model. -->

<!--Projections-->
```{r}
#| label: five-gau-proj-tsne-model

training_data_gau <- read_rds("data/five_gau_clusters/data_five_gau.rds")

data_gau <- training_data_gau |> 
  select(-ID) |>
  mutate(type = "data")

tsne_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau_71.rds")

## Compute hexbin parameters
num_bins_x_gau1 <- 18

algo_obj_gau1 <- fit_highd_model(
  highd_data = training_data_gau, 
  nldr_data = tsne_data_gau, 
  bin1 = num_bins_x_gau1, 
  q = 0.1, 
  benchmark_highdens = 5)

tsne_gau_scaled <- algo_obj_gau1$nldr_obj$scaled_nldr
tr_from_to_df_gau1 <- algo_obj_gau1$trimesh_data
df_bin_centroids_gau1 <- algo_obj_gau1$model_2d
df_bin_gau1 <- algo_obj_gau1$model_highd

trimesh_removed_gau_tsne <- ggplot() + 
  geom_point(
    data = tsne_gau_scaled,
    aes(
      x = tSNE1,
      y = tSNE2
    ),
    colour = clr_choice,
    alpha = 0.1) + 
  geom_segment(
    data = tr_from_to_df_gau1,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    color = "#000000",
    #alpha = 0.4,
    linewidth = 0.5) +
  interior_annotation("a1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

# df_bin_centroids_gau1 <- df_bin_centroids_gau1 |>
#   dplyr::filter(std_counts > 0)

df_b <- df_bin_gau1 |>
  dplyr::filter(hexID %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hexID),] |>
  dplyr::select(-hexID) 

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

```

```{r}
#| label: langevitour-five-gau-tsne-proj
#| eval: false

df_model_data_n <- bind_rows(df_b, data_gau)

langevitour::langevitour(df_model_data_n[1:(length(df_model_data_n)-1)],
                         lineFrom = tr_from_to_df_gau1$from,
                         lineTo = tr_from_to_df_gau1$to,
                         group = factor(df_model_data_n$type,
                                        c("data", "model")),
                         levelColors = c(clr_choice, "#000000"))
```


```{r}
#| label: five-gau-tsne-projs
projection <- cbind(
  c(0.10394,0.08379,-0.17868,0.32960),
  c(0.35111,-0.01137,-0.09657,-0.16018))

proj_obj1 <- get_projection(projection = projection, 
               proj_scale = 2.5, 
               highd_data = scaled_gau_data, 
               model_highd = scaled_gau_data_model, 
               trimesh_data = tr_from_to_df_gau1, 
               axis_param = list(limits = 1,
                                 axis_scaled = 2,
                                 axis_pos_x = -0.72, 
                                 axis_pos_y = -0.72, 
                                 threshold = 0.1))

projected_df <- proj_obj1$projected_df
model_df <- proj_obj1$model_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle

five_gau_proj_tsne_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_point(
    #size = 0.5,
    alpha = 0.05,
    color = clr_choice) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    alpha = 0.4,
    linewidth = 0.8) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2),
    label=rownames(axes), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.89, 0.89)) +
  ylim(c(-0.89, 0.89)) +
  interior_annotation("a2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
projection <- cbind(
  c(0.30943,-0.08452,-0.23005,-0.05284),
  c(0.22463,-0.06682,0.32106,0.02452))

proj_obj2 <- get_projection(projection = projection, 
                            proj_scale = 3, 
                            highd_data = scaled_gau_data, 
                            model_highd = scaled_gau_data_model, 
                            trimesh_data = tr_from_to_df_gau1, 
                            axis_param = list(limits = 1.5,
                                              axis_scaled = 2,
                                              axis_pos_x = -1, 
                                              axis_pos_y = -1, 
                                              threshold = 0.05))

five_gau_proj_tsne_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "a3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none") 
```

```{r}
#| label: five-gau-proj-umap-model

umap_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")

## Compute hexbin parameters
num_bins_x_gau1 <- 63

algo_obj_gau1 <- fit_highd_model(
  highd_data = training_data_gau, 
  nldr_data = umap_data_gau, 
  bin1 = num_bins_x_gau1, 
  q = 0.1, 
  benchmark_highdens = 5)

umap_gau_scaled <- algo_obj_gau1$nldr_obj$scaled_nldr
tr_from_to_df_gau1 <- algo_obj_gau1$trimesh_data
df_bin_centroids_gau1 <- algo_obj_gau1$model_2d
df_bin_gau1 <- algo_obj_gau1$model_highd

trimesh_removed_gau_umap <- ggplot() + 
  geom_point(
    data = umap_gau_scaled,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    colour = clr_choice,
    alpha = 0.05) + 
  geom_segment(
    data = tr_from_to_df_gau1,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    color = "#000000",
    #alpha = 0.4,
    linewidth = 0.5) +
  interior_annotation("b1", 
                      position = c(0.95, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

# df_bin_centroids_gau1 <- df_bin_centroids_gau1 |>
#   dplyr::filter(std_counts > 0)

df_b <- df_bin_gau1 |>
  dplyr::filter(hexID %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hexID),] |>
  dplyr::select(-hexID)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

```

```{r}
#| label: langevitour-five-gau-umap-proj
#| eval: false

df_model_data_n <- bind_rows(df_b, data_gau)

langevitour::langevitour(df_model_data_n[1:(length(df_model_data_n)-1)],
                         lineFrom = tr_from_to_df_gau1$from,
                         lineTo = tr_from_to_df_gau1$to,
                         group = factor(df_model_data_n$type,
                                        c("data", "model")),
                         levelColors = c(clr_choice, "#000000"))
```


```{r}
#| label: five-gau-umap-projs
projection <- cbind(
  c(0.10394,0.08379,-0.17868,0.32960),
  c(0.35111,-0.01137,-0.09657,-0.16018))

proj_obj1 <- get_projection(projection = projection, 
               proj_scale = 2.5, 
               highd_data = scaled_gau_data, 
               model_highd = scaled_gau_data_model, 
               trimesh_data = tr_from_to_df_gau1, 
               axis_param = list(limits = 1,
                                 axis_scaled = 2,
                                 axis_pos_x = -0.72, 
                                 axis_pos_y = -0.72, 
                                 threshold = 0.1))

projected_df <- proj_obj1$projected_df
model_df <- proj_obj1$model_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle

five_gau_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_point(
    alpha = 0.05,
    color = clr_choice) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#000000",
    alpha = 0.4,
    linewidth = 0.8) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.89, 0.89)) +
  ylim(c(-0.89, 0.89)) +
  interior_annotation("b2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
projection <- cbind(
  c(0.30943,-0.08452,-0.23005,-0.05284),
  c(0.22463,-0.06682,0.32106,0.02452))

proj_obj2 <- get_projection(projection = projection, 
                            proj_scale = 3, 
                            highd_data = scaled_gau_data, 
                            model_highd = scaled_gau_data_model, 
                            trimesh_data = tr_from_to_df_gau1, 
                            axis_param = list(limits = 1.5,
                                              axis_scaled = 2,
                                              axis_pos_x = -1, 
                                              axis_pos_y = -1, 
                                              threshold = 0.05))

five_gau_proj_umap_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "b3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none")
```

```{r}
#| label: five-gau-proj-pacmap-model

pacmap_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")

## Compute hexbin parameters
num_bins_x_gau1 <- 28

algo_obj_gau1 <- fit_highd_model(
  highd_data = training_data_gau, 
  nldr_data = pacmap_data_gau, 
  bin1 = num_bins_x_gau1, 
  q = 0.1, 
  benchmark_highdens = 5)

pacmap_gau_scaled <- algo_obj_gau1$nldr_obj$scaled_nldr
tr_from_to_df_gau1 <- algo_obj_gau1$trimesh_data
df_bin_centroids_gau1 <- algo_obj_gau1$model_2d
df_bin_gau1 <- algo_obj_gau1$model_highd

trimesh_removed_gau_pacmap <- ggplot() + 
  geom_point(
    data = pacmap_gau_scaled,
    aes(
      x = PaCMAP1,
      y = PaCMAP2),
      colour = clr_choice,
    alpha = 0.1) +
  geom_segment(
    data = tr_from_to_df_gau1,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    color = "#000000",
    #alpha = 0.4,
    linewidth = 0.5) +
  interior_annotation("c1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

# df_bin_centroids_gau1 <- df_bin_centroids_gau1 |>
#   dplyr::filter(std_counts > 0)

df_b <- df_bin_gau1 |>
  dplyr::filter(hexID %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hexID),] |>
  dplyr::select(-hexID)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)
```

```{r}
#| label: langevitour-five-gau-pacmap-proj
#| eval: false

df_model_data_n <- bind_rows(df_b, data_gau)

langevitour::langevitour(df_model_data_n[1:(length(df_model_data_n)-1)],
                         lineFrom = tr_from_to_df_gau1$from,
                         lineTo = tr_from_to_df_gau1$to,
                         group = factor(df_model_data_n$type,
                                        c("data", "model")),
                         levelColors = c(clr_choice, "#000000"))
```


```{r}
#| label: five-gau-pacmap-projs
## First projection
projection <- cbind(
  c(0.10394,0.08379,-0.17868,0.32960),
  c(0.35111,-0.01137,-0.09657,-0.16018))

proj_obj1 <- get_projection(projection = projection, 
               proj_scale = 2.5, 
               highd_data = scaled_gau_data, 
               model_highd = scaled_gau_data_model, 
               trimesh_data = tr_from_to_df_gau1, 
               axis_param = list(limits = 1,
                                 axis_scaled = 2,
                                 axis_pos_x = -0.72, 
                                 axis_pos_y = -0.72, 
                                 threshold = 0.1))

projected_df <- proj_obj1$projected_df
model_df <- proj_obj1$model_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle

five_gau_proj_pacmap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1,
      y = proj2)) +
  geom_point(
    alpha = 0.05,
    color = clr_choice) +
  geom_segment(
    data = model_df,
    aes(
      x = proj1_from,
      y = proj2_from,
      xend = proj1_to,
      yend = proj2_to),
    color = "#000000",
    alpha = 0.4,
    linewidth = 0.8) +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2, label=rownames(axes)),
    colour="grey50",
    size = 5) +
  geom_path(
    data=circle,
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.89, 0.89)) +
  ylim(c(-0.89, 0.89)) +
  interior_annotation("c2",
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
projection <- cbind(
  c(0.30943,-0.08452,-0.23005,-0.05284),
  c(0.22463,-0.06682,0.32106,0.02452))

proj_obj2 <- get_projection(projection = projection, 
                            proj_scale = 3, 
                            highd_data = scaled_gau_data, 
                            model_highd = scaled_gau_data_model, 
                            trimesh_data = tr_from_to_df_gau1, 
                            axis_param = list(limits = 1.5,
                                              axis_scaled = 2,
                                              axis_pos_x = -1, 
                                              axis_pos_y = -1, 
                                              threshold = 0.05))

five_gau_proj_pacmap_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "c3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none")

```

```{r}
#| label: fig-five-gau-projs
#| fig-height: 10
#| fig-width: 15
#| fig-pos: H
#| fig-cap: "NLDR's organise points in the \\gD{} layout in different ways, possibly misleadingly, illustrated using three layouts: (a) tSNE, (b) UMAP, (c) PaCMAP. The data has five Gaussian cluster in $4\\text{-}D$. The bottom row of plots shows a \\gD{} projection from a tour on $4\\text{-}D$ revealing the differences generated by the layouts on the model fits.  We would expect the model fit to be like that in (a2) where it is distinctly is separate for each cluster but like a hairball in each. This would indicate the distinct clusters, each being fully $4\\text{-}D$. With (c2), the curiousity is that the model is a \\gD{} pancake shape in $4\\text{-}D$, indicating that there is some ordering of points done by PaCMAP, posisbly along some principal component axes.  Videos of the langevitour animations are available at <https://youtu.be/oQxEb4wRdHI>, <https://youtu.be/JW49csPpDx4>, and XXX respectively."

trimesh_removed_gau_tsne + trimesh_removed_gau_umap + trimesh_removed_gau_pacmap +
five_gau_proj_tsne_model1 + five_gau_proj_umap_model1 + five_gau_proj_pacmap_model1 + 
  #five_gau_proj_tsne_model2 +
  #five_gau_proj_umap_model2 + five_gau_proj_pacmap_model2 +
  plot_layout(guides = "collect", nrow = 2) &
  theme(legend.position='none')
```

### The effect of density {#sec-effect-dens}

To illustrate how tSNE organize different number of points within the structure in the tSNE layout, simulated $4\text{-}D$ data having C-shaped structure is used. @fig-one-dens_clust-error b shows \gD{} layouts for tSNE. The default hyper-parameters are used. The \gD{} layout is colored according to the $4\text{-}D$ model error (@fig-one-dens_clust-error a). In @fig-one-dens_clust-error a, the large model error points are clustered in the top corner, forming a separate cluster with high model errors.   

The model is fitted to this layout. @fig-one-dens_clust-error c shows the $2\text{-}D$ projection of the fitted model. In this projection, the distribution of $4\text{-}D$ model errors can be observed. The dense C-shaped structure shows high model errors at the sparse end. In the dense C-shaped structure, the high model errors occur because there are fewer data points positioned within the bins at the sparse end of the structure.

```{r}
#| label: model-one-dens-clust

one_c_shaped_data <- read_rds(here::here("data/one_c_shaped_dens_structure/one_c_shaped_dens_data.rds"))
one_c_shaped_data <- one_c_shaped_data |>
  mutate(ID = row_number())

tsne_one_c_shaped <- read_rds(file = "data/one_c_shaped_dens_structure/one_c_shaped_dens_structure_tsne_perplexity_52.rds") |>
  mutate(ID = row_number())

num_bins_x_one_c_shaped <- 20

algo_obj_dens_clust <- fit_highd_model(
  highd_data = one_c_shaped_data, 
  nldr_data = tsne_one_c_shaped, 
  bin1 = num_bins_x_one_c_shaped, 
  q = 0.1, 
  benchmark_highdens = 1)

tsne_one_c_shaped_scaled <- algo_obj_dens_clust$nldr_obj$scaled_nldr
tr_from_to_df_dens_clust <- algo_obj_dens_clust$trimesh_data
df_bin_centroids_dens_clust <- algo_obj_dens_clust$model_2d
df_bin_dens_clust <- algo_obj_dens_clust$model_highd
```

```{r}
#| label: error-model-one-dens-clust
## Compute error
error_df_one_curvy_abs <- augment(
  highd_data = one_c_shaped_data,
  model_2d = df_bin_centroids_dens_clust,
  model_highd = df_bin_dens_clust)

error_df_one_curvy_abs <- error_df_one_curvy_abs |>
  bind_cols(tsne_one_c_shaped_scaled |>
              select(-ID))

error_df_one_curvy_abs <- error_df_one_curvy_abs |>
  mutate(sqrt_row_wise_total_error = sqrt(row_wise_total_error))

# Compute density
density_data <- density(error_df_one_curvy_abs$sqrt_row_wise_total_error)
density_df <- data.frame(x = density_data$x, y = density_data$y)

# Add density values to the original dataset
error_df_one_curvy_abs <- error_df_one_curvy_abs %>%
  mutate(density = approx(density_df$x, density_df$y, xout = sqrt_row_wise_total_error)$y)

error_df_one_curvy_abs <- error_df_one_curvy_abs |>
  mutate(error_cat_n = if_else(tSNE1 <= 0.25 & tSNE2 <= 0.25, "selected", "deselected")) |> ## high_error points
  mutate(error_cat_n2 = if_else(tSNE2 >= 1.2, "selected", "deselected")) ## corner points

error_df_one_curvy_abs_selected11 <- error_df_one_curvy_abs |>
  dplyr::filter(error_cat_n == "selected")

error_df_one_curvy_abs_deselected11 <- error_df_one_curvy_abs |>
  dplyr::filter(error_cat_n == "deselected")

error_plot_one_curvy_hist_selected <- ggplot(error_df_one_curvy_abs_deselected11, 
      aes(x = sqrt_row_wise_total_error, 
          y = density)) +
  geom_point(alpha=0.7, color = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected11, 
      aes(x = sqrt_row_wise_total_error, 
          y = density),
      alpha=0.7, color = "#800026", size = 3) +
  xlab(expression(e[hj])) +
  ylab("") +
  interior_annotation("b1",
                      position = c(0.95, 0.9),
                      cex = 2) +
  theme_bw() +
  theme(aspect.ratio = 1,
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))

error_df_one_curvy_abs_selected12 <- error_df_one_curvy_abs |>
  dplyr::filter(error_cat_n2 == "selected")

error_df_one_curvy_abs_deselected12 <- error_df_one_curvy_abs |>
  dplyr::filter(error_cat_n2 == "deselected")


error_plot_one_curvy_hist <- ggplot(error_df_one_curvy_abs_deselected12, 
      aes(x = sqrt_row_wise_total_error, 
          y = density)) +
  geom_point(alpha=0.7, color = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected12, 
      aes(x = sqrt_row_wise_total_error, 
          y = density),
      alpha=0.7, color = "#800026", size = 3) +
  xlab(expression(e[hj])) +
  ylab("") +
  interior_annotation("a1",
                      position = c(0.95, 0.9),
                      cex = 2) +
  theme_bw() +
  theme(aspect.ratio = 1,
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))
```

```{r}
#| label: link-model-one-dens-clust

plot_tsne_dens <- error_df_one_curvy_abs_deselected12 |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.7, color = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected12,
             aes(x = tSNE1, y = tSNE2),
             alpha=0.7, color = "#800026", size = 3) +
  theme(aspect.ratio = 1) +
  interior_annotation("a2",
                      position = c(0.08, 0.9),
                      cex = 2)

plot_tsne_dens_selected <- error_df_one_curvy_abs_deselected11 |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.7, color = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected11,
             aes(x = tSNE1, y = tSNE2),
             alpha=0.7, color = "#800026", size = 3) +
  theme(aspect.ratio = 1) +
  interior_annotation("b2",
                      position = c(0.08, 0.9),
                      cex = 2)
```

```{r}
#| label: prep-proj-one-dens-clust

# Apply the scaling

data_c_shaped <- one_c_shaped_data |>
  select(-ID) |>
  mutate(type = "data")

df_bin_dens_clust <- df_bin_dens_clust |>
  select(-hexID) |>
  mutate(type = "model")

df_model_data <- bind_rows(data_c_shaped, df_bin_dens_clust)
scaled_c_shaped <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_c_shaped_data <- scaled_c_shaped |>
  filter(type == "data") |>
  select(-type)

scaled_c_shaped_data_model <- scaled_c_shaped |>
  filter(type == "model") |>
  select(-type)
```

```{r}
#| label: langevitour-c-shaped-tsne-proj
#| eval: false

data_c_shaped_n <- data_c_shaped |>
  select(-type) |>
  mutate(type = factor(error_df_one_curvy_abs$error_cat_n2,
                          levels=c("deselected", "selected")))

df_model_data_n <- bind_rows(df_bin_dens_clust, data_c_shaped_n)

langevitour::langevitour(df_model_data_n[1:(length(df_model_data_n)-1)],
                         lineFrom = tr_from_to_df_dens_clust$from,
                         lineTo = tr_from_to_df_dens_clust$to,
                         group = factor(df_model_data_n$type,
                                        c("deselected", "selected", "model")),
                         levelColors = c('#d9d9d9', '#800026', "#000000"),
                         pointSize = c(1.5, 3, 0))
```

```{r}
#| label: proj-one-dens-clust

## First projection
projection <- cbind(
  c(0.05096,0.15399,0.19736,0.05110),
  c(0.14608,-0.16929,0.11291,-0.07160))

proj_obj1 <- get_projection(projection = projection,
                            proj_scale = 1,
                            highd_data = scaled_c_shaped_data,
                            model_highd = scaled_c_shaped_data_model,
                            trimesh_data = tr_from_to_df_dens_clust,
                            axis_param = list(limits = 0.4,
                                              axis_scaled = 3,
                                              axis_pos_x = -0.28,
                                              axis_pos_y = -0.28,
                                              threshold = 0.02))

proj_obj1[["cluster"]] <- factor(error_df_one_curvy_abs$error_cat_n2,
                          levels=c("deselected", "selected"))

projected_df <- proj_obj1$projected_df
model_df <- proj_obj1$model_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle

projected_df <- projected_df |>
      dplyr::mutate(cluster = proj_obj1$cluster)

five_c_shaped_proj_tsne_model1 <- ggplot() +
      geom_point(
        data = projected_df,
        aes(
          x = proj1,
          y = proj2,
          colour = factor(cluster, levels = c("deselected", "selected")),
          size = factor(cluster, levels = c("deselected", "selected"))),
        alpha = 0.5) +
      geom_segment(
        data = model_df,
        aes(
          x = proj1_from,
          y = proj2_from,
          xend = proj1_to,
          yend = proj2_to),
        color = "#000000",
        linewidth = 0.8,
        alpha = 0.4) +
      geom_segment(
          data=axes,
          aes(x=x1, y=y1, xend=x2, yend=y2),
          colour="grey70") +
        geom_text(
          data=axes,
          aes(x=x2, y=y2),
          label=rownames(axes),
          colour="grey50",
          size = 4.5) +
        geom_path(
          data=circle,
          aes(x=c1, y=c2), colour="grey70") +
        xlim(c(-0.35, 0.35)) +
        ylim(c(-0.35, 0.35)) +
        interior_annotation(label = "a3", position = c(0.08, 0.9), cex = 2) +
        scale_color_manual(values = c('#d9d9d9', '#800026')) +
        scale_size_manual(values = c('deselected' = 1.5, 'selected' = 3)) +
        theme(aspect.ratio = 1,
              legend.position = "none")


proj_obj1[["cluster"]] <- factor(error_df_one_curvy_abs$error_cat_n,
                          levels=c("deselected", "selected"))

projected_df <- proj_obj1$projected_df
model_df <- proj_obj1$model_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle

projected_df <- projected_df |>
      dplyr::mutate(cluster = proj_obj1$cluster)

five_c_shaped_proj_tsne_model1_selected <- ggplot() +
      geom_point(
        data = projected_df,
        aes(
          x = proj1,
          y = proj2,
          colour = factor(cluster, levels = c("deselected", "selected")),
          size = factor(cluster, levels = c("deselected", "selected"))),
        alpha = 0.5) +
      geom_segment(
        data = model_df,
        aes(
          x = proj1_from,
          y = proj2_from,
          xend = proj1_to,
          yend = proj2_to),
        color = "#000000",
        linewidth = 0.8,
        alpha = 0.4) +
      geom_segment(
          data=axes,
          aes(x=x1, y=y1, xend=x2, yend=y2),
          colour="grey70") +
        geom_text(
          data=axes,
          aes(x=x2, y=y2),
          label=rownames(axes),
          colour="grey50",
          size = 4.5) +
        geom_path(
          data=circle,
          aes(x=c1, y=c2), colour="grey70") +
        xlim(c(-0.35, 0.35)) +
        ylim(c(-0.35, 0.35)) +
  interior_annotation(label = "b3", position = c(0.08, 0.9), cex = 2) +
  scale_color_manual(values = c('#d9d9d9', '#800026')) +
  scale_size_manual(values = c('deselected' = 1.5, 'selected' = 3)) +
  theme(aspect.ratio = 1,
        legend.position = "none")
```

```{r}
#| label: fig-one-dens_clust-error
#| fig-width: 15
#| fig-height: 10
#| fig-pos: H
#| out-width: 100%
#| fig-cap: "The sparse end of the structure shows a high model error, while the dense end shows a low model error. The tSNE layout of the C-shaped structure, with dense points (a1-a3) are colored according to their $4\\text{-}D$ model error. Darker colors represent a high error, while lighter colors indicate a low error. a2, a3 and b2, b3 present the results from linked plots that brushed the high $4\\text{-}D$ error points in \\gD{} and $4\\text{-}D$. It helps in identifying the high $4\\text{-}D$ model errors that occur due to the sparse end. Videos of the linked plots are available at <>."

error_plot_one_curvy_hist + plot_tsne_dens + 
  five_c_shaped_proj_tsne_model1 +
  error_plot_one_curvy_hist_selected + plot_tsne_dens_selected +
  five_c_shaped_proj_tsne_model1_selected +
    plot_layout(guides = "collect", nrow = 2) &
    theme(legend.position='none')
```

## Applications {#sec-applications}

<!-- XXX WHY THESE EXAMPLES. NEEDS AN OVERVIEW PARAGRAPH -->

To showcase the robustness of our method when applied to data with different characteristics, this section presents two case studies. The first application focuses on single-cell data, a domain where the high dimensionality, sparsity, and heterogeneity of gene expression measurements pose significant analytical challenges. Our approach facilitates the identification of the most accurate NLDR layout, enabling the discovery of cell populations with similar expression profiles. The second case study explores the MNIST hand-written digits dataset, which serves as a benchmark for evaluating the preservation of local structures, specifically with digit 1.  

### Single-cell gene expression {#sec-pbmc}
<!--
- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use our method to get a better representation
-->

<!-- XXX What is single cell data???? -->

Single-cell data refers to the characteristics of individual cells within a population (@haque2017). These data also provides insights at the level of individual cells, enabling deeper understanding of gene expression. 

Clustering of single-cell data is used to identify groups of cells with similar expression profiles. NLDR often used to summarise the discovered clusters, and help to understand the results. The purpose of this example is to *illustrate how to use our method to help decide on an appropriate NLDR layout that accurately represents the data*. The cluster results of Human Peripheral Blood Mononuclear Cells (PBMC3k) in @chen2024 are examined. There are $2622$ single cells, with $1000$ gene expressions. Following their pre-processing, UMAP was performed using nine principal components. @fig-NLDR-variety (a) is the reproduction of the published plot. The question is whether this accurately represents the cluster structure in the data. Our method can help here, and also help to provide a better \gD{} layout, as needed. 

<!-- UMAP layout with author's suggested parameter choice-->
```{r}
#| label: published-pbmc
## Import data
training_data_pbmc <- read_rds("data/pbmc3k/pbmc_pca_50.rds")
names(training_data_pbmc) <- paste0("x", 1:50)

training_data_pbmc <- training_data_pbmc[, 1:9] |>
  mutate(ID = 1:NROW(training_data_pbmc))

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")
pbmc_scaled_obj <- gen_scaled_data(
  nldr_data = umap_pbmc)
umap_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr
```

The @fig-NLDR-variety (a) shows three well-separated clusters with big separations. However, as shown in @fig-model-pbmc-author-proj (a2), there is no big separation between three clusters in $9\text{-}D$. Therefore, the suggested UMAP representation (@fig-NLDR-variety (a)) does not accurately represent the structure of PBMC3k dataset.

As a result, it is necessary to find an appropriate layout for the dataset. MSE for different binwidths ($a_1$) using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with various (hyper-)~parameter settings were computed (@fig-pbmc-mse). Layouts c, d, and e, which show small separations between clusters, are universally optimal. However, layout d performs well with smaller binwidths and poorly with larger binwidths. On the other hand, layout e performs well with larger binwidths. Layout c was selected for further analysis due to its better (hyper-)parameter selection for the same method of the published plot.

The visualization of the selected layout in the $9\text{-}D$ shows edges between the clusters (@fig-model-pbmc-author-proj (b2)). This supports the presence of small separations between clusters. Additionally, the data points are not uniformly distributed across the clusters, resulting in dense areas (@fig-model-pbmc-author-proj (b2)). Furthermore, the clusters shows non-linear shapes (@fig-model-pbmc-author-proj (b3)).

<!-- As shown in @fig-NLDR-variety, layout c, d, and e that show small separations between clusters are universally optimal. But layout d with little separations perform well at tiny binwidth and poorly as binwidth increases. Also, layout e performs well at large binwidths. The layout c is chosen as the best, since it used the UMAP with a different (hyper-) parameter settings. -->

<!-- The visualization of the model generated for the selected layout in $9\text{-}D$ shows edges between the clusters (@fig-mnist-tri-proj (b2)). This provides evidence for the small separations between clusters. Furthermore, the data does not uniformally distributed along the clusters and therefore shows densed points (@fig-mnist-tri-proj (b2). Also, the clusters are non-linear shaped (@fig-mnist-tri-proj (b3).   -->

<!-- To determine whether the UMAP representation with the specific (hyper-)parameter choice suggested by @chen2023 preserves the data structure present in $9\text{-}D$ (@fig-model-pbmc-author-proj (b)), we visualize the model constructed with UMAP overlaid on the $9\text{-}D$ data.  -->

<!-- But, when visualizing the *model-in-the-data-space* some unobserved structures can be seen. Some clusters have non-linear continuity patterns and high-density patches (@fig-model-pbmc-author-proj). -->

<!-- As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)). This demonstrates that tSNE accurately captures the data structure of the PBMC3k dataset, which UMAP did not achieve. -->


<!-- In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $b_1$ using UMAP with different (hyper-)parameter settings and tSNE with default (hyper-)parameter setting (@fig-pbmc-mse) were calculated. After analyzing the results, it was found that tSNE with default (hyper-)parameter setting (perplexity: $30$) achieved the lowest error. Therefore, tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. -->


<!--Fit the best model for author suggestion and compute error-->
```{r}
#| label: hexbin-pbmc
## Compute hexbin parameters
num_bins_x_pbmc <- 22

algo_obj_pbmc <- fit_highd_model(
  highd_data = training_data_pbmc, 
  nldr_data = umap_pbmc, 
  bin1 = num_bins_x_pbmc, 
  q = 0.1, 
  benchmark_highdens = 5)

umap_pbmc_scaled <- algo_obj_pbmc$nldr_obj$scaled_nldr
tr_from_to_df_pbmc <- algo_obj_pbmc$trimesh_data
df_bin_centroids_pbmc <- algo_obj_pbmc$model_2d
df_bin_pbmc <- algo_obj_pbmc$model_highd

umap_pbmc_scaled_with_cluster <- umap_pbmc_scaled |>
  mutate(cluster = if_else(UMAP1 <= 0.25, "cluster1", if_else(UMAP1 >= 0.625, "cluster2", "cluster3"))) |>
  mutate(cluster = factor(cluster, levels = c("cluster1", "cluster2", "cluster3")))

trimesh_removed_pbmc <- ggplot() + 
  geom_point(
    data = umap_pbmc_scaled_with_cluster,
    aes(
      x = UMAP1,
      y = UMAP2,
      color = cluster
    ),
    alpha = 0.2,
    size = 0.1#,
    #color = '#a65628'
  )  +
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada")) +
  interior_annotation("a1", cex = 2) +
  theme(
    aspect.ratio = 1
  )
```

```{r}
#| label: prep-pbmc-umap-model-proj

data_pbmc <- training_data_pbmc |> 
  select(-ID) |>
  mutate(type = "data")

# df_bin_centroids_pbmc <- df_bin_centroids_pbmc |>
#   dplyr::filter(std_counts > 0)

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hexID %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hexID),] |>
  dplyr::select(-hexID) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)
```

```{r}
#| label: langevitour-pbmc-umap-author-proj
#| eval: false

data_pbmc_n <- data_pbmc |>
  select(-type) |>
  mutate(type = as.character(umap_pbmc_scaled_with_cluster$cluster))

df_model_data_pbmc_n <- bind_rows(df_b_pbmc, data_pbmc_n)

langevitour::langevitour(df_model_data_pbmc_n[1:(length(df_model_data_pbmc_n)-1)],
                         lineFrom = tr_from_to_df_pbmc$from,
                         lineTo = tr_from_to_df_pbmc$to,
                         group = factor(df_model_data_pbmc_n$type,
                                        c("cluster1", "cluster2", "cluster3", "model")),
                         levelColors = c("#8dd3c7", "#fdb462", "#bebada", "#000000"))
```

```{r}
#| label: pbmc-umap-model-proj

## First projection
projection <- cbind(
    c(-0.4545,-0.5533,-0.1235,0.3984,-0.2065,-0.3586,-0.0088,0.3572,-0.1368),
    c(0.4576,-0.3157,0.4708,0.1410,-0.4479,-0.1685,0.3668,-0.2692,0.1332))

proj_obj1 <- get_projection(projection = projection, 
               proj_scale = 1.23, 
               highd_data = scaled_pbmc_data, 
               model_highd = scaled_pbmc_data_model, 
               trimesh_data = tr_from_to_df_pbmc, 
               axis_param = list(limits = 1,
                                 axis_scaled = 1, 
                                 axis_pos_x = -0.48, 
                                 axis_pos_y = -0.48, 
                                 threshold = 0.082))

proj_obj1[["cluster"]] <- as.character(umap_pbmc_scaled_with_cluster$cluster)

pbmc_proj_umap_model1 <- plot_proj(
  proj_obj = proj_obj1, 
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-0.65, 0.65), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "a2", cex = 2) + 
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

## Second projection
projection <- cbind(
    c(0.0734,0.6138,-0.4671,0.2589,0.1189,0.0778,-0.0819,0.0308,-0.5561),
    c(0.6625,0.0034,0.2412,0.3360,0.2463,0.1070,-0.2903,-0.4705,0.1293))

proj_obj2 <- get_projection(projection = projection, 
               proj_scale = 1.25, 
               highd_data = scaled_pbmc_data, 
               model_highd = scaled_pbmc_data_model, 
               trimesh_data = tr_from_to_df_pbmc, 
               axis_param = list(limits = 1.35, 
                                 axis_scaled = 1, 
                                 axis_pos_x = -0.75, 
                                 axis_pos_y = -0.75, 
                                 threshold = 0.12))

proj_obj2[["cluster"]] <- as.character(umap_pbmc_scaled_with_cluster$cluster)

pbmc_proj_umap_model2 <- plot_proj(
    proj_obj = proj_obj2, 
    point_param = c(1.5, 0.2), # size, alpha, color
    plot_limits = c(-1, 0.75), 
    axis_text_size = 4,
    is_category = TRUE) +
  interior_annotation(label = "a3", cex = 2) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

```

<!-- ::: {#fig-pbmc1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/umap_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_1.png) -->

<!-- ![](figures/pbmc3k/sc_2.png) -->

<!-- ![](figures/pbmc3k/sc_3.png) -->

<!-- Model in \gD{}, on the UMAP layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.041)$, $b = 870 \  (30, \ 29)$, $m = 135$, benchmark value to remove large edges is $0.099$) (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/VqqWuE0Jj6A>). -->
<!-- ::: -->
<!--compute absolute error for different parameter choices-->

```{r}
#| label: combine-data-pbmc

error_pbmc_umap <- read_rds("data/pbmc3k/error_pbmc_umap_30_min_dist_0.3.rds")
error_pbmc_umap2 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.01.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_12_min_dist_0.99.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_3_min_dist_0.6.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_4_min_dist_0.6.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.8.rds")
error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.8.rds")
error_pbmc_tsne <- read_rds("data/pbmc3k/error_pbmc_tsne_5.rds")
error_pbmc_tsne2 <- read_rds("data/pbmc3k/error_pbmc_tsne_30.rds")
error_pbmc_phate <- read_rds("data/pbmc3k/error_pbmc_phate_5.rds")
error_pbmc_trimap <- read_rds("data/pbmc3k/error_pbmc_trimap_12_4_3.rds")
error_pbmc_pacmap <- read_rds("data/pbmc3k/error_pbmc_pacmap_30_random_0.9_5.rds")

error_pbmc <- bind_rows(error_pbmc_umap, 
                        error_pbmc_umap2,
                        error_pbmc_umap3,
                        error_pbmc_tsne,
                        error_pbmc_tsne2,
                        error_pbmc_phate,
                        error_pbmc_trimap,
                        error_pbmc_pacmap)

error_pbmc <- error_pbmc |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()

error_pbmc <- error_pbmc |>
  mutate(method = factor(method,
                         levels = c("UMAP_5_min_dist_0.8", "UMAP_30_min_dist_0.3", "UMAP_5_min_dist_0.01","tsne_5", "tsne_30", "phate_5", "trimap_12_4_3", "pacmap")))
```

```{r}
#| label: error-comp-pbmc

error_plot_pbmc <- plot_mse(error_pbmc) +
  scale_x_continuous(breaks = sort(unique(error_pbmc$a1))[c(1, 5, 9, 13, 17, 21, 26)]) +
  scale_color_manual(values=c('#999999','#a65628','#e41a1c','#377eb8','#4daf4a','#ff7f00','#984ea3','#f781bf')) 

```

```{r}
#| fig-cap: "Assessing which of the 8 NLDR layouts on the PBMC3k data  (shown in @fig-NLDR-variety) is the better representation using RMSE for varying binwidth ($a_1$). Colour  used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-h). Layout f is universally poor. Layouts a, c, g, h that show large separations between clusters are universally suboptimal. Layout d with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. The choice of best is between layouts b and e, that have small separations between oddly shaped clusters. Layout e is the best choice."
#| label: fig-pbmc-mse
#| fig-pos: H
#| out-width: 100%

design <- "
AABB
AABB
AABB
"
error_plot_pbmc + 
  wrap_plots(nldr1c, nldr2c, nldr3c, nldr4c,
             nldr5c, nldr6c, nldr7c, nldr8c, ncol = 2) +
  plot_layout(design = design)
```

<!-- The MSE for different binwidths ($a_1$) using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with different (hyper-)parameter settings (@fig-pbmc-mse) were calculated.  -->

<!-- It is found that tSNE (@fig-mnist-mse (a)) provide the most reasonable representation for the digit 1 dataset, showing universally best. However, \gD{} representation shows a big non-linear cluster and a small cluster with a small gap (@fig-tsne-best).  -->

<!-- In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $a_1$ using UMAP, tSNE, PHATE, PaCMAP, and TriMAP with different (hyper-)parameter settings (@fig-pbmc-mse) were calculated. tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)). -->
<!-- The visualization of the *model-in-the-data-space* provides evidence for this (@fig-pbmc2-sc). Therefore, tSNE with perplexity $30$ accurately captures the data structure of the PBMC3k dataset, which is better than UMAP. -->

<!--best choice-->

```{r}
#| label: tsne-pbmc-read-best
tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")
```

<!-- We then fit the model for tSNE, and visualize the resultant model in the $9\text{-}D$ data space. The model shows a quirk, as shown in @fig-pbmc2-sc. All three clusters are connected by an edge except the small and large clusters. Because the clusters are so close in \gD{}, they attempt to maintain the structure in \pD{} as well. This is evident that tSNE with perplexity $30$ provides a reasonable representation of PBMC3k data. -->

<!--Fit the best model and compute error-->
```{r}
#| label: tsne-num-bins-pbmc

## Compute hexbin parameters
num_bins_x_tsne_pbmc <- 22

algo_obj_pbmc <- fit_highd_model(
  highd_data = training_data_pbmc, 
  nldr_data = tsne_pbmc, 
  bin1 = num_bins_x_tsne_pbmc, 
  q = 0.1, 
  benchmark_highdens = 1)

tsne_pbmc_scaled_best <- algo_obj_pbmc$nldr_obj$scaled_nldr
tr_from_to_df_pbmc <- algo_obj_pbmc$trimesh_data
df_bin_centroids_pbmc <- algo_obj_pbmc$model_2d
df_bin_pbmc <- algo_obj_pbmc$model_highd

tsne_pbmc_scaled_best_with_cluster <- tsne_pbmc_scaled_best |>
  mutate(cluster = umap_pbmc_scaled_with_cluster$cluster)

trimesh_removed_pbmc_best <- ggplot() + 
  geom_point(
    data = tsne_pbmc_scaled_best_with_cluster,
    aes(
      x = tSNE1,
      y = tSNE2,
      color = cluster
    ),
    alpha = 0.3#,
    #color = "#999999"
  ) +
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada")) +
  interior_annotation("b1", cex = 2, c(0.08, 0.9)) +
  theme(
    aspect.ratio = 1
  )
```

```{r}
#| label: prep-pbmc-tsne-model-proj-best

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hexID %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hexID),] |>
  dplyr::select(-hexID) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)
```

```{r}
#| label: langevitour-pbmc-tsne-best-proj
#| eval: false

df_model_data_pbmc_n <- bind_rows(df_b_pbmc, data_pbmc_n)

langevitour::langevitour(df_model_data_pbmc_n[1:(length(df_model_data_pbmc_n)-1)],
                         lineFrom = tr_from_to_df_pbmc$from,
                         lineTo = tr_from_to_df_pbmc$to,
                         group = factor(df_model_data_pbmc_n$type,
                                        c("cluster1", "cluster2", "cluster3", "model")),
                         levelColors = c("#8dd3c7", "#fdb462", "#bebada", "#000000"))
```


```{r}
#| label: pbmc-tsne-model-proj-best

## First projection
projection <- cbind(
  c(-0.4545,-0.5533,-0.1235,0.3984,-0.2065,-0.3586,-0.0088,0.3572,-0.1368),
  c(0.4576,-0.3157,0.4708,0.1410,-0.4479,-0.1685,0.3668,-0.2692,0.1332))

proj_obj1 <- get_projection(projection = projection, 
                            proj_scale = 1.23, 
                            highd_data = scaled_pbmc_data, 
                            model_highd = scaled_pbmc_data_model, 
                            trimesh_data = tr_from_to_df_pbmc, 
                            axis_param = list(limits = 1, 
                                              axis_scaled = 1,
                                              axis_pos_x = -0.48, 
                                              axis_pos_y = -0.48, 
                                              threshold = 0.082))

proj_obj1[["cluster"]] <- as.character(umap_pbmc_scaled_with_cluster$cluster)

pbmc_proj_tsne_model1_best <- plot_proj(
  proj_obj = proj_obj1,
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-0.65, 0.65), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "b2", cex = 2) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

## Second projection
projection <- cbind(
  c(0.0734,0.6138,-0.4671,0.2589,0.1189,0.0778,-0.0819,0.0308,-0.5561),
  c(0.6625,0.0034,0.2412,0.3360,0.2463,0.1070,-0.2903,-0.4705,0.1293))

proj_obj2 <- get_projection(projection = projection, 
                            proj_scale = 1.25, 
                            highd_data = scaled_pbmc_data, 
                            model_highd = scaled_pbmc_data_model, 
                            trimesh_data = tr_from_to_df_pbmc, 
                            axis_param = list(limits = 1.35, 
                                              axis_scaled = 1,
                                              axis_pos_x = -0.75, 
                                              axis_pos_y = -0.75, 
                                              threshold = 0.12))

proj_obj2[["cluster"]] <- as.character(umap_pbmc_scaled_with_cluster$cluster)

pbmc_proj_tsne_model2_best <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-1, 0.75), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "b3", cex = 2) + 
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))
```

<!-- <!--bin1 = 15, bin2 = 20, b = 300, non_empty = 136--> 
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| fig-cap: "(a) Model generated in $2\\text{-}D$ with tSNE, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows three well-separated distant clusters. The $p\\text{-}D$ model errors are distributed along clusters, but most low $p\\text{-}D$ model errors present in the large cluster." -->
<!-- #| label: fig-model-pbmc -->
<!-- #| fig-pos: H -->
<!-- #| out-height: 30% -->

<!-- trimesh_removed_pbmc + error_plot_pbmc +  -->
<!--   plot_layout(guides='collect', ncol=2) & -->
<!--   theme(legend.position='bottom') -->
<!-- ``` -->

<!-- XXX Number of bins for b is inadequate. Caption needs to start with what this figure is for XXX -->

```{r}
#| echo: false
#| fig-cap: "Compare the published \\gD{} layout (@fig-pbmc-mse a) and the \\gD{} layout selected (@fig-pbmc-mse e) by MSE plot (@fig-pbmc-mse) from the UMAP and tSNE with default hyper-parameters. The PBMC3k data ($n =  2622$) has three close clusters in $9\\text{-}D$. Two \\gD{} projection from a tour on $9\\text{-}D$ of the model fit with @fig-pbmc-mse a ($a_1 = 0.06$, $b = 616/86 (22, 28)$) shows three-well separated clusters with big separations. On the other hand, the model fit with @fig-pbmc-mse e ($a_1 = 0.06$, $b = 704/227 (22, 32)$) shows three-well separated clusters with small separations. Therefore, @fig-pbmc-mse e is more reasonable than @fig-pbmc-mse a. Also, the fitted models in $9\\text{-}D$ help to see some unobserved patterns of the data: (i) dense points, and (ii) non-linear clusters. Videos of the langevitour animations are available at <https://youtu.be/0cKX_HG_n0k> and <https://youtu.be/KhJvsRtaX04> respectively."
#| label: fig-model-pbmc-author-proj
#| fig-pos: H
#| fig-width: 10
#| fig-height: 15
#| out-width: 90%

free(trimesh_removed_pbmc) + free(trimesh_removed_pbmc_best) +
  pbmc_proj_umap_model1 + pbmc_proj_tsne_model1_best +
  pbmc_proj_umap_model2 + pbmc_proj_tsne_model2_best +
  plot_layout(nrow=2, byrow=FALSE) &
  theme(legend.position='none')
```

<!-- ::: {#fig-pbmc2-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/tsne_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_4.png) -->

<!-- ![](figures/pbmc3k/sc_5.png) -->

<!-- ![](figures/pbmc3k/sc_6.png) -->

<!-- Model in \gD{}, on the tSNE layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.058)$, $b = 300 \  (15, \ 20)$, $m = 136$, benchmark value to remove large edges is $0.133$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/5Y1hE4i7N2k>). -->
<!-- ::: -->

### Hand-written digits {#sec-mnist}
<!--
- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 
-->

<!--add different NLDR layouts-->

The digit 1 of the MNIST dataset consists of $7877$ grayscale images of handwritten digits [@lecun2010]. Before further analysis, PCA was used to preprocess the data, where the first $10$ principal components, explaining 83% of the total variation, were selected. The objective is to select a reasonable \gD{} layout, representing the non-linear structure of the digit 1 dataset in $10\text{-}D$ (@fig-clust-mnist (a)). 


<!-- @yingfan2021 used this dataset to evaluate how PaCMAP preserves local structures in \gD{}. Because of the large number of images, the focus was specifically on the handwritten digit 1, which exhibits a non-linear structure in \gD{} (@fig-pacmap-author). There are $7877$ images of the digit 1 in the dataset. Additionally, PCA was applied as a preprocessing step and the first $10$ principal components were selected. The objective is to assess whether PaCMAP effectively preserves the non-linear structure in \gD{}.  -->

```{r}
#| label: read-mnist-nldr
# Read a variety of different NLDR representations of mnist
# and plot them on same aspect ratio
tsne_mnist <- read_rds("data/mnist/mnist_tsne30.rds")

nldr_mnist1 <- tsne_mnist |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("a", c(0.08, 0.93)) +
  theme(aspect.ratio = 1)

tsne_mnist2 <- read_rds("data/mnist/mnist_tsne89.rds")

nldr_mnist6 <- tsne_mnist2 |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("f", c(0.08, 0.93)) +
  theme(aspect.ratio = 1)

umap_mnist <- read_rds("data/mnist/mnist_umap.rds")

nldr_mnist2 <- umap_mnist |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("b") +
  theme(aspect.ratio = 1)

phate_mnist <- read_rds("data/mnist/mnist_phate.rds")

nldr_mnist3 <- phate_mnist |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("c", position = c(0.94, 0.92)) +
  theme(aspect.ratio = 1)

trimap_mnist <- read_rds("data/mnist/mnist_trimap.rds")

nldr_mnist4 <- trimap_mnist |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("d", position = c(0.86, 0.92)) +
  theme(aspect.ratio = 1)

pacmap_mnist <- read_rds("data/mnist/mnist_pacmap.rds")

nldr_mnist5 <- pacmap_mnist |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e") +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-mnist

error_mnist_umap <- read_rds("data/mnist/error_mnist_umap.rds")
error_mnist_tsne <- read_rds("data/mnist/error_mnist_tsne.rds")
error_mnist_tsne2 <- read_rds("data/mnist/error_mnist_tsne2.rds")
error_mnist_phate <- read_rds("data/mnist/error_mnist_phate.rds")
error_mnist_trimap <- read_rds("data/mnist/error_mnist_trimap.rds")
error_mnist_pacmap <- read_rds("data/mnist/error_mnist_pacmap.rds")

error_mnist <- bind_rows(error_mnist_umap, 
                        error_mnist_tsne,
                        error_mnist_tsne2,
                        error_mnist_phate,
                        error_mnist_trimap,
                        error_mnist_pacmap)

error_mnist <- error_mnist |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup() |>
  mutate(method = factor(method,
                         levels = c("UMAP", "tSNE", "tSNE2", "PHATE", "TriMAP", "PaCMAP")))
```

```{r}
#| label: error-comp-mnist

error_plot_mnist <- plot_mse(error_mnist) +
  scale_x_continuous(breaks = sort(
    unique(error_mnist$a1))[append(seq(1, length(unique(error_mnist$a1)), 
                                       by = 5), 24)]) +
  scale_color_manual(
    values=c('#a65628', '#984ea3', '#999999', 
             '#377eb8', '#ff7f00', '#e41a1c')) +
  theme(aspect.ratio=1.5)

```


```{r}
#| fig-cap: "Assessing which of the 6 NLDR layouts on the MNIST digit 1 data is the better representation using RMSE for varying binwidth ($a_1$). Colour used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-f). All the layouts appear to be very similar. Layout c is universally poor. Layouts a, f that show two close clusters are universally suboptimal. Layout a with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. Layout f is the best choice."
#| label: fig-mnist-mse
#| fig-pos: H
#| out-width: 92%

error_plot_mnist + wrap_plots(nldr_mnist1, nldr_mnist2, nldr_mnist3, 
                                    nldr_mnist4, nldr_mnist5, nldr_mnist6, ncol = 2)
```

<!-- PaCMAP preserves non-linear structure in $784\text{-}D$. To evaluate whether PaCMAP provides a reasonable representation of the data, the \gD{} embedding of the handwritten digit 1 was selected. As shown in @fig-pacmap-author, the angle of the digit 1 images varies along the \gD{} structure. -->

The MSE for different binwidths ($a_1$) using UMAP, PHATE, PaCMAP, TriMAP with default (hyper-)parameter setting and tSNE with default (hyper-)parameter setting and perplexity $89$ (@fig-mnist-mse) were calculated. It is found that tSNE with perplexity $89$ (@fig-mnist-mse (f)) provide the most reasonable representation for the digit 1 dataset, showing universally best. However, \gD{} representation shows a big non-linear cluster and a small cluster with a small gap. 

In the case of digit 1 data, there should not be any clusters unless anomalies exist, indicating different digit 1 patterns. The angle of the digit 1 images varies along this non-linear clustering structure (@fig-model-error-mnist), while the small cluster contains the digit 1 images with different patterns of the digit 1, unlike the usual (@fig-model-error-mnist (c)). This provides the evidence for two close clusters.

By visualizing the model generated for tSNE in $10\text{-}D$ helps to assess the \gD{} layout. As shown in @fig-clust-mnist (a1), the model provides the evidence for the non-linear structure of the digit 1 data in $10\text{-}D$. The model shows some quirks. The model's twisted pattern provides evidence for the $10\text{-}D$ data structure, which is not observed by \gD{} layout (@fig-clust-mnist (a2)). 


<!-- PaCMAP layout with author's suggested parameter choice-->
```{r}
#| label: read-mnist
## Import data
training_data_mnist <- read_rds("data/mnist/mnist_10_pcs_of_digit_1.rds")
names(training_data_mnist) <- paste0("x", 1:NCOL(training_data_mnist))

training_data_mnist <- training_data_mnist |>
  mutate(ID = 1:NROW(training_data_mnist))

data_mnist <- training_data_mnist |>
  select(-ID) |>
  mutate(type = "data")
```

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->

<!-- Fit the model and compute error-->
```{r}
#| label: hexbin-mnist

num_bins_x_mnist <- 30

algo_obj_mnist <- fit_highd_model(
  highd_data = training_data_mnist, 
  nldr_data = tsne_mnist2, 
  bin1 = num_bins_x_mnist, 
  q = 0.1, 
  benchmark_highdens = 1)

tsne_minst_scaled <- algo_obj_mnist$nldr_obj$scaled_nldr
tr_from_to_df_mnist <- algo_obj_mnist$trimesh_data
df_bin_centroids_mnist <- algo_obj_mnist$model_2d
df_bin_mnist <- algo_obj_mnist$model_highd

sc_ltr_pos_mnist <- c(0.96, 0.96)

tsne_minst_scaled <- tsne_minst_scaled |>
  mutate(cluster = if_else((tSNE1 <= 1) & (tSNE1 >= 0.78) & (tSNE2 <= 0.62) & (tSNE2 >= 0.39), "small_clust", "big_clust"))

trimesh_removed_mnist <- ggplot() + 
  geom_point(data = tsne_minst_scaled,
             aes(
               x = tSNE1,
               y = tSNE2,
               colour = cluster
             ),
             alpha=0.2)+
    geom_segment(data = tr_from_to_df_mnist, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000") +
  scale_color_manual(values=c("#999999",'#ff7f00')) +
  interior_annotation("a", c(0.08, 0.93), cex = 2) +
  theme(legend.position = "none") 

tsne_plot_mnist_clust <- ggplot() + 
  geom_point(data = tsne_minst_scaled,
             aes(
               x = tSNE1,
               y = tSNE2,
               colour = cluster
             ),
             alpha=0.2)+
  scale_color_manual(values=c('#d9d9d9', '#ff7f00')) +
  interior_annotation("a", c(0.08, 0.93), cex = 2.5) +
  theme(legend.position = "none") 

## Compute error
error_df <- augment(
  highd_data = training_data_mnist,
  model_2d = df_bin_centroids_mnist,
  model_highd = df_bin_mnist) 

## To join embedding
error_df <- error_df |>
  bind_cols(tsne_minst_scaled |> 
              select(-ID))

## To transform the data
## ggplot(data = error_df, aes(x=row_wise_abs_error)) + geom_density() ## right skewed

error_df <- error_df |>
  mutate(sqrt_row_wise_tot_error = sqrt(row_wise_total_error)) |>
  mutate(sqrt_row_wise_tot_error = standardize(sqrt_row_wise_tot_error))
  
highd_error_2d_mnist <- error_df |>
  ggplot(aes(x = tSNE1,
             y = tSNE2,
             colour = sqrt_row_wise_tot_error)) +
  geom_point(alpha=0.2) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  geom_rect(aes(xmin = 0.82, xmax = 0.92, ymin = 0.38, ymax = 0.58),
               fill = "transparent", color = "#000000", linewidth = 1) +
  geom_rect(aes(xmin = 0.9, xmax = 1, ymin = 1.25, ymax = 1.45),
               fill = "transparent", color = "#375cb1", linewidth = 1)  +
  geom_rect(aes(xmin = 0.35, xmax = 0.45, ymin = 0.55, ymax = 0.7),
               fill = "transparent", color = "#004616", linewidth = 1)  +
  geom_rect(aes(xmin = 0.7, xmax = 0.8, ymin = 0.25, ymax = 0.4),
               fill = "transparent", color = "#4c389a", linewidth = 1) +
  theme(legend.position = "none",
        aspect.ratio = 1,
        plot.margin = margin(0, 0, 0, 0),
        panel.spacing = unit(0, "pt"),
        axis.ticks.length = unit(0, "pt"),
        strip.text = element_blank(),   # Optional, if you're using facets
        axis.title = element_blank()   # Optional
        )
```

<!-- As a result of visualizing the *model-in-the-data-space*, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data (@fig-mnist-tri-proj). It also shows some quirks like twisted patterns and long edges (@fig-mnist-tri-proj). Furthermore, the results help to identify anomalies (with large model errors) within and the outside the non-linear structure, displaying different patterns of the digit 1 (@fig-model-error-mnist).  -->

<!-- According to @fig-mnist-tri-proj (b), the non-linear structure observed in the \gD{} layout of PaCMAP (@fig-pacmap-author) is also visible when visualizing the model overlaid on the data space. This indicates that PaCMAP accurately captures the non-linear structure of the $10\text{-}D$ data. Additionally, the model shows a twisted pattern within the non-linear structure in $10\text{-}D$ space (@fig-mnist-tri-proj (c)), which is an additional pattern not visible in the \gD{} representation (@fig-pacmap-author). Furthermore, as shown in @fig-mnist-tri-proj (d), some long edges exist in the $10\text{-}D$ space that are not recognized as long edges in the \gD{} representation. However, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data.  -->

<!--bin1 = 22, bin2 = 17, b = 374, non_empty = 140-->

```{r}
#| label: data-prep-mnist-model-proj

hexID_mnist <- df_bin_centroids_mnist |>
  #dplyr::filter(std_counts > 0) |>
  dplyr::pull(hexID)

df_b_mnist <- df_bin_mnist |>
  dplyr::filter(hexID %in% hexID_mnist) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_mnist <- df_b_mnist[match(hexID_mnist, df_b_mnist$hexID),] |>
  dplyr::select(-hexID) 

# Apply the scaling
df_model_data_mnist <- bind_rows(data_mnist, df_b_mnist)
scaled_mnist <- scale_data_manual(df_model_data_mnist, "type") |>
  as_tibble()

scaled_mnist_data <- scaled_mnist |>
  filter(type == "data") |>
  select(-type)

scaled_mnist_data_model <- scaled_mnist |>
  filter(type == "model") |>
  select(-type)
```

```{r}
#| label: langevitour-best-fit-mnist-proj-tsne
#| eval: false

data_mnist_cluster <- data_mnist |>
  select(-type) |>
  mutate(type = tsne_minst_scaled$cluster)

df_model_data_mnist_n <- bind_rows(df_b_mnist, data_mnist_cluster)

langevitour::langevitour(df_model_data_mnist_n[1:(length(df_model_data_mnist_n)-1)],
                         lineFrom = tr_from_to_df_mnist$from,
                         lineTo = tr_from_to_df_mnist$to,
                         group = factor(df_model_data_mnist_n$type,
                                        c("big_clust", "small_clust", "model")),
                         levelColors = c("#999999",'#ff7f00', "#000000"))
```

```{r}
#| label: mnist-model-proj

projection <- cbind(
  c(-0.04295,0.42361,-0.36702,0.13103,-0.19678,-0.29621,0.00471,0.09323,0.18822,-0.18484),
  c(-0.36644,0.05058,0.18200,0.21860,0.13583,-0.03511,0.35339,0.15821,-0.26945,-0.27926))

proj_obj1 <- get_projection(projection = projection, 
               proj_scale = 2.5, 
               highd_data = scaled_mnist_data, 
               model_highd = scaled_mnist_data_model, 
               trimesh_data = tr_from_to_df_mnist, 
               axis_param = list(limits = 1, 
                                 axis_scaled = 1.1, 
                                 axis_pos_x = -0.92, 
                                 axis_pos_y = -0.92, 
                                 threshold = 0.065))

proj_obj1[["cluster"]] <- tsne_minst_scaled$cluster

mnist_proj_tsne_model1 <- plot_proj(
  proj_obj = proj_obj1, 
  point_param = c(0.5, 0.5), # size, alpha, color
  line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
  plot_limits = c(-1.1, 0.9), 
  axis_text_size = 4, 
  is_category = TRUE) +
  interior_annotation(label = "b", cex = 2) +
  scale_color_manual(values = c("#999999",'#ff7f00')) +
  theme(
    legend.position = "none"
  )

mnist_proj_tsne_model1_link <- plot_proj(
    proj_obj = proj_obj1,
    point_param = c(1.2, 0.5), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-1.1, 0.9), 
    axis_text_size = 5, 
    is_category = TRUE) +
  interior_annotation(label = "b", cex = 2.5) +
  scale_color_manual(values = c("#d9d9d9",'#ff7f00')) +
  theme(
      legend.position = "none"
  )

## Second projection
projection <- cbind(
    c(-0.33305,0.20223,0.06303,0.11124,0.13299,-0.30093,0.34082,0.15581,-0.08864,0.33573),
    c(0.53615,0.14969,-0.09393,-0.20053,0.07992,-0.34791,0.13057,-0.06814,-0.16737,0.03716))

proj_obj2 <- get_projection(projection = projection, 
               proj_scale = 2, 
               highd_data = scaled_mnist_data, 
               model_highd = scaled_mnist_data_model, 
               trimesh_data = tr_from_to_df_mnist, 
               axis_param = list(limits = 1, 
                                 axis_scaled = 1.1, 
                                 axis_pos_x = -0.83, 
                                 axis_pos_y = -0.83, 
                                 threshold = 0.065))

proj_obj2[["cluster"]] <- tsne_minst_scaled$cluster

mnist_proj_tsne_model2 <- plot_proj(
    proj_obj = proj_obj2,
    point_param = c(0.5, 0.3), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-1, 1), 
    axis_text_size = 3.5, 
    is_category = TRUE) +
  interior_annotation(label = "c", cex = 2) +
  scale_color_manual(values = c("#999999",'#ff7f00')) +
  theme(
      legend.position = "none"
  )

mnist_proj_tsne_model2_link <- plot_proj(
    proj_obj = proj_obj2, 
    point_param = c(1.2, 0.5), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-1, 1), 
    axis_text_size = 5, 
    is_category = TRUE) +
  interior_annotation(label = "c", cex = 2.5) +
  scale_color_manual(values = c("#d9d9d9",'#ff7f00')) +
  theme(
      legend.position = "none"
  )

```

<!--add langevitour screenshots and youtube animation link-->
```{r}
#| label: fig-mnist-tri-proj
#| fig-pos: H
#| fig-height: 4
#| fig-width: 12
#| out-width: 100%
#| fig-cap: "\\gD{} wireframe layout shows that there is a big non-linear cluster (grey) and a small cluster (orange) located very close to the one corner of the big cluster. The MNIST digit 1 data ($n =  7877$ and $p = 10$) has a non-linear structure in $10\\text{-}D$. Two \\gD{} projections from a tour on $10\\text{-}D$ reveal that the closeness of the clusters in $10\\text{-}D$ and the twisted pattern of the model fit with tSNE ($a_1 = 0.041$, $b = 659/585 \\text{ } (30, \\text{ }48)$). Video of the langevitour animation is available at <>."
#| eval: false

free(trimesh_removed_mnist) + mnist_proj_tsne_model1 +
  mnist_proj_tsne_model2 +
  plot_layout(ncol = 3)

```

```{r}
#| label: fig-clust-mnist
#| fig-pos: H
#| fig-height: 5
#| fig-width: 15
#| out-width: 100%
#| fig-cap: "The tSNE layout of the MNIST digit 1 data shows a big non-linear cluster (grey) and a small cluster (orange) located very close to the one corner of the big cluster in \\gD{} (a). The MNIST digit 1 data ($n =  7877$ and $p = 10$) has a non-linear structure in $10\\text{-}D$. Two \\gD{} projections from a tour on $10\\text{-}D$ reveal that the closeness of the clusters in $10\\text{-}D$ and the twisted pattern of the model fit with tSNE ($a_1 = 0.041$, $b = 659/585 \\text{ } (30, \\text{ }48)$). Video of the langevitour animation is available at <>. The brushing feature in the linked plots helps in visualizing the closeness of the small cluster to the big cluster. Videos of the linked plots are available at <>."

tsne_plot_mnist_clust + mnist_proj_tsne_model1_link + mnist_proj_tsne_model2_link +
  plot_layout(ncol = 3)
```

<!-- ::: {#fig-mnist1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/mnist/mnist_model_2d.png){#fig-mnist1-model} -->

<!-- ![](figures/mnist/sc_1.png){#fig-mnist1-sc1} -->

<!-- ![](figures/mnist/sc_2.png){#fig-mnist1-sc2} -->

<!-- ![](figures/mnist/sc_3.png){#fig-mnist1-sc3} -->

<!-- Model in \gD{}, on the PaCMAP layout, and three views of the fit in projections from $10\text{-}D$, for the digit 1 of MNIST data ($(s_1, \ s_2) = (-0.100, \ -0.059)$, $b = 374 \  (22, \ 17)$, $m = 140$, benchmark value to remove large edges is $0.094$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/zcg_GXBmqjA>). -->


<!-- ::: -->
<!-- need to update the youtube recording-->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

```{r}
#| label: img-diff-pos

## Data with pixel values
mnist_data <- read_rds("data/mnist/mnist_digit_1.rds")

mnist_data <- mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28)

img_right_top <- c(868, 1465, 1787, 2006, 4314)

pixels_gathered_within <-  mnist_data |>
  filter(instance %in% img_right_top)

right_top_img <- plot_digit_img(
  pixels_gathered_within, palette = "Blues 2", title_text = "a")

img_middle <- c(130, 2091, 7795, 163, 77)

pixels_gathered_within <-  mnist_data |>
  filter(instance %in% img_middle)

middle_img <- plot_digit_img(
  pixels_gathered_within, palette = "Greens 3", title_text = "b")

img_right_bottom <- c(2035, 7708, 7710, 4134, 5006)

pixels_gathered_within <-  mnist_data |>
  filter(instance %in% img_right_bottom)

right_bottom_img <- plot_digit_img(
  pixels_gathered_within, palette = "Purples 2", title_text = "c")

img_error_outside <- c(999, 3650, 4916, 2964, 5622)

pixels_gathered_outside <-  mnist_data |>
  filter(instance %in% img_error_outside)

imge_error_sample_outside <- plot_digit_img(
  pixels_gathered_outside, palette = "Grays", title_text = "d")

##### inside
img_error_inside <- c(5173, 6981, 7228, 6673, 3179) #, 6673, 3179, 7363, 7650, 5241, 4636

pixels_gathered_inside <-  mnist_data |>
  filter(instance %in% img_error_inside)

imge_error_sample_inside <- plot_digit_img(
  pixels_gathered_inside, palette = "Oranges", title_text = "e")

```

```{r}
#| label: fig-model-error-mnist
#| fig-cap: "The $10\\text{-}D$ model error in \\gD{} layout of the MNIST digit 1 data shows a pattern. Most low model errors are distributed along the big non-linear cluster, while most large model errors are distributed along the small cluster. The images associated with large model errors shows different patterns of digit 1, some inside (e) the non-linear structure and others outside (d). Along the non-linear cluster, the angle of digit 1 changes (a-c)."
#| fig-pos: H
#| fig-width: 10
#| fig-height: 10
#| out-width: 100%

# Define your current layout
top_layout <- right_top_img + middle_img + right_bottom_img + imge_error_sample_outside + imge_error_sample_inside + plot_layout(nrow = 5)

final_layout <- wrap_plots(highd_error_2d_mnist, top_layout)

final_layout
```

<!-- These anomalies can be classified into two types: those that are anomalies within the non-linear cluster and those that lie in the small cluster. The images associated with large model error points within the non-linear cluster display different patterns of the digit 1.  -->

<!-- There are certain data points that exhibit high error rates due to their deviation from the usual $10\text{-}D$ data structure, which makes them anomalies (@fig-model-mnist). These anomalies can be classified into two types: those that are anomalies within the non-linear structure and those that lie outside of it. The images associated with high model error points within the non-linear structure display different patterns of the digit 1, as shown in @fig-mnist1-within. However, when comparing these images to the ones found outside of the non-linear structure, it becomes evident that the latter display different patterns of the digit 1 (@fig-mnist1-out).  -->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

<!-- ::: {#fig-mnist-anomalies layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/mnist/img_error_sample_within.png){#fig-mnist1-within} -->

<!-- ![](figures/mnist/img_error_sample_out.png){#fig-mnist1-out} -->

<!-- Some images of handwritten digit 1 which occur high model error (a) within the non-linear structure, and (b) outside the non-linear structure. The images shows different patterns of digit 1. -->
<!-- ::: -->

## Discussion {#sec-discussion}

<!-- - Summarise contributions -->
<!-- - Explain where it is expected or not expected to work, eg higher dimensional relationships -->
<!-- - Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views (need to add) -->
<!-- - Predicting new observations in $k$-D -->
<!-- - Extending layouts beyond $k$-D, when 2D is clearly inadequate. -->
<!-- - Diagnostic app to explore differences in distances (need to add) -->
<!-- - What might be useful enhancements -->

We have developed an algorithm to assess the NLDR method and (hyper-)parameter choice(s) and choose the most accurate representation of the structure(s) present in the \pD{} data. Starting from \gD{} layout, the fitted model, as represented by the positions of points in \kD{}, and turn it into a \pD{} wireframe to overlay on the data, viewing it with a tour. This approach is defined as *model-in-data-space*. Viewing a model in the data space is an ideal way to examine the fit.

With the model fit, several observations can be made. The simulated $4\text{-}D$ data, which contains five Gaussian clusters, was analyzed. Both tSNE and UMAP demonstrated the fitted model as a *crumpled \gD{} sheet* that stretches across all four dimensions, highlighting its ability to preserve local structure. In contrast, the flat representation generated by PaCMAP indicates its failure to capture the variation within the clusters. This highlights how different NLDR methods organize the data points differently.

Another usability of our approach is how NLDR methods behave with different density of the data structure. The tSNE layout of simulated $4\text{-}D$ C-shaped data with different densities are used. The high $4\text{-}D$ model errors are scattered to a corner of the \gD{} layout of dense C-shaped structure. In the dense C-shaped structure, the high model errors occur because there are fewer data points positioned within the bins at the sparse end of the structure.  

Our algorithm can also be useful in practice to decide on a reasonable NLDR layout that accurately represents the data, as described with a single-cell gene expression data, **PBMC3k**. The published results of UMAP with default (hyper-)parameter setting by @chen2024 were evaluated. The data showed three close clusters. But the published layout showed three well-separated clusters. Therefore, by changing the NLDR method to tSNE with default (hyper-)parameters, we found a better layout, which is more representative the data. Additionally, the model discovers non-uniform data distribution and non-linear structures within the clusters that are not visible in the \gD{} layout, demonstrating the ability of our model in uncovering hidden data characteristics. Also, as explained in Appendix, with recently introduced approach, scDEED also proved the layout suggested by our layout is better than the published one.  

The digit 1 of **Hand-written digits** data shows how the model accurately capture the non-linear structure. Not only that, the $10\text{-}D$ model error help to find the anomalies of digit 1 images. Also, twisted pattern of the model evident that how tSNE tries to maintain the local structure by positioning the similar patterns of digit 1 close to each other. 

Diagnose is important to find where the \gD{} points located in \pD{}. One approach is to use interactivity. By linking the \gD{} layout with the fitted model in \pD{} and employing brushing techniques, we can identify which points fit well and which do not. Additionally, linking the \gD{} error plot with the fitted model in \pD{} can help pinpoint where mismatches occur.

Predicting new observations in \kD{} is particularly valuable due to the limitations of some NLDR methods, like tSNE, which don't provide a straightforward method for prediction. As a result, our approach offers a solution that capable of generating predicted \kD{} embedding regardless of the NLDR method employed, effectively addressing this functional gap.

Furthermore, to extend layouts beyond \kD{}, when \gD{} is clearly inadequate, k-means can be used as an alternative to bin centroids.

There are many avenues for future work. First, the development of
evaluation metrics is important for NLDR. We introduced a qualitative approach. But having a quantitative measure also will be useful for various application domains. Second, one could use our approach to create an approach for tuning (hyper-)parameters.  Another direction for future work is to introduce more approaches to diagnose the fitted model. Finally, one can think of designing a new NLDR method by following the binning in high-dimensions.

<!-- In conclusion, while our method effectively captures and represents high-dimensional data structures, further enhancements could involve introducing approaches to diagnose the fitted model. These improvements would help in creating a more accurate representation of the data when \gD{} layout is inadequate. -->

## Supplementary Materials

Appendix: The appendix includes more details about the hexagonal binning algorithm and scDEED method (appendix.pdf, Portable Document Format file).

R package `quollr`: The R package `quollr` containing codes to fit, and visualize the model. (need to add quollr .zip file, GNU zipped tar file)

Direct links to videos for viewing online are available in @tbl-links.

```{r}
#| label: tbl-links
#| tbl-cap: "Example videos."
#| tbl-pos: H

links_df <- tibble(
  data = c("PBMC3k"),
  URL = c("link1")
)


links_df |>
  kableExtra::kable(format = "latex", 
                    booktabs = TRUE, 
                    label = "tbllinks") |>
  kableExtra::kable_styling(latex_options = "scale_down")

```

## Acknowledgments

These `R` packages were used for the work: `tidyverse` (@hadley2019), `Rtsne` (@jesse2015), `umap` (@tomasz2023), `patchwork` (@thomas2024), `colorspace` (@achim2020), `langevitour` (@harisson2024), `conflicted` (@hadley2023), `reticulate` (@kevin2024), `kableExtra` (@hao2024). These `python` packages were used for the work: `trimap` (@amid2019) and `pacmap` (@yingfan2021). The article was created with `R` packages `quarto` (@jjallaire2024). The project's GitHub repository ([https://github.com/JayaniLakshika/paper-nldr-vis-algorithm](https://github.com/JayaniLakshika/paper-nldr-vis-algorithm)) contains all materials required to reproduce this article.


## References {.unnumbered}
  
::: {#refs}
:::
      
