---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P. Gamage
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: https://www.michaellydeamore.com/
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-linear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data (\pD{}) by applying a non-linear transformation. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of \pD{} distributions. The NLDR methods and hyper-parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the \pD{} data, we have developed an algorithm to show the \gD{} NLDR model in the \pD{} space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks. 
  
keywords: [high-dimensional data vizualization, non-linear dimension reduction, tour]
keywords-formatted: [high-dimensional data vizualization, non-linear dimension reduction, tour]

bibliography: paper.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
  \newcommand\fD{$4\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r include=FALSE}
#| label: install-libraries
#| warning: false
#| echo: false

options(repos = c(CRAN = "https://cran.rstudio.com")) # Setup mirror

packages_to_check <- c("remotes", "tidyverse", "patchwork", "colorspace", "kableExtra", "conflicted")

for (pkg in packages_to_check) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Installing package:", pkg))
    install.packages(pkg)
  } else {
    installed_version <- packageVersion(pkg)
    available_version <- tryCatch({
      utils::packageDescription(pkg)$Version
    }, error = function(e) NA) # Handle cases where package info isn't readily available

    if (!is.na(available_version) && installed_version < package_version(available_version)) {
      message(paste("A newer version of package", pkg, "is available. Updating..."))
      install.packages(pkg)
    } else {
      message(paste("Package", pkg, "is up-to-date (version", installed_version, ")."))
    }
  }
}

```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(tibble)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(patchwork)
library(colorspace)
library(kableExtra)
library(conflicted)
library(ggbeeswarm)
library(scales)

conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm'),
     plot.margin = margin(0, 0, 0, 0)
   )
)
```

```{r}
#| label: import-scripts
source("scripts/additional_functions.R")
```

```{r}
#| label: code-setup
set.seed(20240110)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{2.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data ($k < p$). Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2019], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). 

```{r}
#| label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a", c(0.08, 0.9), cex = 1.3)

nldr1c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a", c(0.08, 0.9))

# umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.8.rds")
nldr2 <- umap_pbmc |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b", c(0.08, 0.9), cex = 1.3)

nldr2c <- umap_pbmc |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("b", c(0.08, 0.9))


umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c", cex = 1.3)

nldr3c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d", cex = 1.3)

nldr4c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("d") 

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e", c(0.08, 0.9), cex = 1.3)

nldr5c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("e", c(0.08, 0.9)) 

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f", cex = 1.3)

nldr6c <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("f") 

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g", cex = 1.3)

nldr7c <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("g") 

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h", cex = 1.3)

nldr8c <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#f781bf') +
  interior_annotation("h") 
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Eight different NLDR representations of the same data. Different methods and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
#| fig-pos: H
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 30), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

The paper is organized as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. In @sec-bestfit, we describe how to assess the best fit and identify the most accurate \gD{} layout based on the proposed model diagnostics. Curiosities and unexpected patterns discovered in NLDR results by examining the model in the data space are discussed in @sec-curiosities. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, low-dimensional (\kD{}) representations of high-dimensional (\pD{}) data have been computed using multidimensional scaling (MDS) [@kruskal1964], which includes principal components analysis (PCA) [@jolliffe2011] as a special case. (A contemporary comprehensive guide to MDS can be found in @borg2005.) The \kD{} representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{} layout is constructed by minimizing a stress function that differences distances between points in \pD{} with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques: tSNE, UMAP, PHATE, TriMAP and PaCMAP. Both tNSE and UMAP can be considered to produce the \kD{} representation by minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is \gD{}, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in \gD{}    represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@joreskog1969] and MDS. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with hyper-parameters $\mathbfit{\theta}$. These parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. 

With our goal being to make a representation of this \gD{} layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbor information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. We recommend using a hexagon grid because it better reflects the data distribution and has less artifacts than a rectangular grid. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-two-curvy, and documented below.

```{r}
#| label: fit_model-two-curvy

tsne_two_curvy_scaled <- read_rds("data/two_nonlinear/tsne_two_curvy_scaled.rds")
hex_grid_with_counts <- read_rds("data/two_nonlinear/hex_grid_with_counts.rds")
hex_grid_nonempty <- read_rds("data/two_nonlinear/hex_grid_nonempty.rds")
tr_from_to_df_two_curvy <- read_rds("data/two_nonlinear/tr_from_to_df_two_curvy.rds")
df_bin_centroids_two_curvy <- read_rds("data/two_nonlinear/df_bin_centroids_two_curvy.rds")

sc_ltr_pos <- c(0.08, 0.9)
sc_xlims <- c(-0.3, 1.2)
sc_ylims <- c(-0.17, 1.25)
```

<!--Full hexagon grid with UMAP data-->

```{r}
#| label: hexbin-two-curvy

nldr_two_curvy_original <- ggplot(tsne_two_curvy_scaled, 
                                  aes(x = emb1, y = emb2)) +
  geom_point(alpha = 0.5, colour = clr_choice, size = 0.5) 

# First plot with xlim and ylim
nldr_two_curvy <- nldr_two_curvy_original +
  interior_annotation("a", sc_ltr_pos, cex = 2) +
  xlim(sc_xlims) +
  ylim(sc_ylims)

hex_grid_two_curvy <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(colour = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = tsne_two_curvy_scaled, 
             aes(x = emb1, y = emb2), 
             alpha = 0.5, size = 0.5, colour = clr_choice) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos, cex = 2)

hex_grid_coloured_two_curvy1 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts, 
    aes(x = x, y = y,
      group = hex_poly_id, 
      fill = std_counts), colour = "grey70", linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
             aes(x = emb1, y = emb2),
             colour = clr_choice,
             alpha = 0.3,
             size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "E") +
  interior_annotation("b", sc_ltr_pos, cex = 2) + 
  xlim(sc_xlims) + ylim(sc_ylims) 
```

<!--Non-empty bins with bin centroids-->

```{r}
#| label: empty-bin-two-curvy

hex_grid_nonempty_two_curvy <- ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(colour = "grey70", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids_two_curvy, 
             aes(x = c_x, y = c_y), 
             colour = "#000000",
             size = 1) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos, cex = 2) 
```

<!--2D model-->
```{r}
#| label: triangulate-two-curvy

trimesh_two_curvy <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = emb1,
               y = emb2
             ),
             colour = "#636363",
             alpha = 0.5,
             size = 0.5
  ) +
  coord_equal() +
  interior_annotation("a1", sc_ltr_pos)

trimesh_removed_two_curvy <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  coord_equal() +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos, cex = 2)

trimesh_two_curvy_removed1_tsne_with_data <- ggplot() +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = emb1,
               y = emb2
             ),
             colour = clr_choice,
             alpha = 0.5,
             size = 0.5) +
  geom_segment(data = tr_from_to_df_two_curvy,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  interior_annotation("a", cex = 1.2) +
  theme(aspect.ratio = 1)
```


```{r}
#| label: compute-error-tsne

## Compute error
error_df_two_curvy_abs <- read_rds("data/two_nonlinear/error_df_two_curvy_abs.rds")

error_plot_two_curvy <- error_df_two_curvy_abs |>
  ggplot(aes(x = emb1,
             y = emb2,
             colour = error_cat)) +
  geom_point(alpha=0.5) +
  scale_colour_manual(values = c('#ffffcc','#ffeda0','#fed976',
                                 '#feb24c','#fd8d3c','#fc4e2a',
                                 '#e31a1c','#bd0026','#800026')) +
  xlab("tSNE1") +
  ylab("tSNE2") +
  theme_bw() +
  theme(
    aspect.ratio = 1,
    legend.position = "none"
  )

error_plot_two_curvy_quasi <- ggplot(
  error_df_two_curvy_abs, 
  aes(x = sqrt_row_wise_total_error, 
      y = 0, 
      colour = error_cat)) +
  geom_quasirandom(alpha = 0.5) +
  scale_colour_manual(values = c('#ffffcc','#ffeda0','#fed976',
                                 '#feb24c','#fd8d3c','#fc4e2a',
                                 '#e31a1c','#bd0026','#800026')) +
  xlab(expression(e[hj])) +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "none",
        plot.margin = margin(0, 0, 0, 0))
```

```{r}
#| label: best-fit-two-curvy-proj-tsne

## First projection
proj_obj1 <- read_rds("data/two_nonlinear/two_nonlinear_proj_obj1.rds")

two_curvy_proj_tsne_first_model1 <- plot_proj(
    proj_obj = proj_obj1, 
    point_param = c(0.5, 0.2, clr_choice), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-0.6, 0.6), 
    axis_text_size = 2, 
    is_category = FALSE) +
  interior_annotation(label = "b", cex = 1.2) +
  theme(aspect.ratio = 1,
        legend.position = "none") 


projected_df <- proj_obj1$projected_df
axes <- proj_obj1$axes
circle <- proj_obj1$circle
cluster <- proj_obj1$cluster

projected_df <- projected_df |>
      dplyr::mutate(cluster = proj_obj1$cluster)

two_curvy_proj_tsne_first_model1_coloured <- projected_df |>
  ggplot(
    aes(
      x = proj1,
      y = proj2,
      colour = cluster)) +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  geom_segment(
    data=axes,
    aes(x=x1, y=y1, xend=x2, yend=y2),
    colour="grey70") +
  geom_text(
    data=axes,
    aes(x=x2, y=y2),
    label=rownames(axes),
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle,
    aes(x=c1, y=c2), colour="grey70") +
  scale_colour_manual(values = c('#ffffcc','#ffeda0','#fed976',
                                 '#feb24c','#fd8d3c','#fc4e2a',
                                 '#e31a1c','#bd0026','#800026')) +
  xlim(c(-0.6, 0.6)) +
  ylim(c(-0.6, 0.6)) +
  theme(aspect.ratio = 1,
        legend.position = "none") 

## Second projection

proj_obj2 <- read_rds("data/two_nonlinear/two_nonlinear_proj_obj2.rds")

two_curvy_proj_tsne_all_model1 <- plot_proj(
  proj_obj = proj_obj2, 
  point_param = c(0.1, 0.5, clr_choice), # size, alpha, color
  line_param = c(0.5, 0.5, "black"), # linewidth, alpha
  plot_limits = c(-0.5, 0.5), 
  axis_text_size = 2, 
  is_category = FALSE) +
  interior_annotation(label = "c", cex = 1.2) +
  theme(aspect.ratio = 1)

```


To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "nonlinear clusters". It is constructed by simulating two clusters, each consisting of $1000$ observations. The C-shaped cluster is generated from $\theta \sim U(\text{-}3\pi/2, 0)$, $X_1 = \sin(\theta)$, $X_2 \sim U(0, 2)$ (adding thickness to the C), $X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)$, $X_4 = \cos(\theta)$. The other cluster is from $X_1 \sim U(0, 2)$, $X_2 \sim U(0, 3)$, $\gamma \sim U(0, 0.5)$, $X_3 = \text{-}(X_1^3 + X_2) + \gamma$, and $X_4 \sim U(0, 2)$. We would consider $T=(X_1, X_2, X_3, X_4)$ to be the geometric structure (true model) that we hope to capture.

<!-- The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance.  -->

```{r}
#| label: fig-NLDR-two-curvy
#| echo: false
#| fig-cap: "Key steps for constructing the model on the tSNE layout ($k=2$): (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids. The two nonlinear clusters data is shown."
#| fig-width: 12
#| fig-height: 4
#| out-width: 100%
 
nldr_two_curvy + hex_grid_two_curvy + 
  hex_grid_nonempty_two_curvy + 
  trimesh_removed_two_curvy +
  plot_layout(ncol = 4)
```

### Algorithm to represent the model in \gD{}  

#### Scale the data

Because we are working with distances between points, starting with data having a standard scale, e.g. [0, 1], is recommended. The default should take the aspect ratio produced by the NLDR $(r_1, r_2, ..., r_k)$ into account. When $k=2$, as in hexagon binning, the default range is $[0, y_{i,\text{max}}], i=1,2$, where $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = r_2/r_1$ (@fig-NLDR-two-curvy). If the NLDR aspect ratio is ignored then set $y_ {2,\text{max}} = 1.$ 

#### Computing hexagon grid configuration

Although there are several implementations of hexagon binning [@carr1987], and a published paper [@dan2023], surprisingly, none has sufficient detail or components that produce everything needed for this project. So we described the process used here. @fig-hex-param illustrates the notation used. 

The \gD{} hexagon grid is defined by its bin centroids. Each hexagon, $H_h$ ($h = 1, \dots, b$) is uniquely described by centroid, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. The number of bins in each direction is denoted as $(b_1, b_2)$, with  $b = b_1 \times b_2$ being the total number of bins. We expect the user to provide just $b_1$ and we calculate $b_2$ using the NLDR ratio, to compute the grid. 

To ensure that the grid covers the range of data values a buffer parameter ($q$) is set as a proportion of the range. By default,  $q=0.1$. The buffer should be extending a full hexagon width ($a_1$) and height ($a_2$) beyond the data, in all directions. The lower left position where the grid starts is defined as $(s_1, s_2)$, and corresponds to the centroid of the lowest left hexagon, $C_{1}^{(2)} = (c_{11}, c_{12})$. This must be smaller than the minimum data value. Because it is one buffer unit, $q$ below the minimum data values, $s_1 = -q$ and $s_2 = -qr_2$. 

The value for $b_2$ is computed by fixing $b_1$. Considering the upper bound of the first NLDR component, $a_1 > (1+2q)/(b_1 -1)$. Similarly, for the second NLDR component, 

$$
a_2 > \frac{r_2 + q(1 + r_2)}{(b_2 - 1)}.
$$

Since $a_2 = \sqrt{3}a_1/2$ for regular hexagons,

$$
a_1 > \frac{2[r_2 + q(1 + r_2)]}{\sqrt{3}(b_2 - 1)}.
$$

This is a linear optimization problem. Therefore, the optimal solution must occur on a vertex. Therefore,

$$
b_2 = \Big\lceil1 +\frac{2[r_2 + q(1 + r_2)](b_1 - 1)}{\sqrt{3}(1 + 2q)}\Big\rceil.
$${#eq-bin2}

```{r}
#| label: code-illustration
# Code to draw illustration for notation
## hexagon binning to have regular hexagons

hex_grid_temp <- read_rds("data/two_nonlinear/hex_grid_temp.rds")
all_centroids_df_temp <- read_rds("data/two_nonlinear/all_centroids_df_tempp.rds")
start_pt <- read_rds("data/two_nonlinear/start_pt.rds")
d_rect <- read_rds("data/two_nonlinear/d_rect.rds")
a1 <- read_rds("data/two_nonlinear/a2_data.rds")
a2 <- read_rds("data/two_nonlinear/a1_data.rds")
l <- read_rds("data/two_nonlinear/l_data.rds")
rect_adj <- read_rds("data/two_nonlinear/rect_adj.rds")

hex_param_vis <- ggplot() + 
    geom_polygon(data = hex_grid_temp, 
                        aes(x = x, 
                            y = y, 
                            group = hex_poly_id),
                 fill = "white", 
                 colour = "#bdbdbd") +
    geom_point(data = all_centroids_df_temp, aes(
      x = c_x, 
      y = c_y), 
      colour = "#31a354", size = 0.9) +
    geom_point(data = start_pt, aes(x = c_x, 
                                    y = c_y), 
               colour = "black") + 
    geom_rect(data=d_rect, 
              aes(xmin = x1min - rect_adj$x1,# - rect_adj$s1, 
                  xmax = x1max - rect_adj$x1,# - rect_adj$s1, 
                  ymin = x2min - rect_adj$x2,# - rect_adj$s2, 
                  ymax = x2max - rect_adj$x2),# - rect_adj$s2), 
              fill = "white", 
              colour = "black", 
              alpha = 0, 
              linewidth = 0.7) +
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1max - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2max - rect_adj$x2)) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,0)", 
             hjust=-0.1, vjust=-0.3, size = 8) + 
    annotate("text", x=d_rect$x1max - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,1)", 
             hjust=1.1, vjust=-0.3, size = 8) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2max - rect_adj$x2,
                     label = expression(group("(", 
                        list(0, y[2][max]),")")), 
            hjust=-0.1, vjust=1.2, size = 8) + 
    geom_segment(data=d_rect, aes(
      x = x1min  - rect_adj$x1, # 0 - 0.03, 
      y = -0.31, 
      xend = x1max - rect_adj$x1, #1 - 0.03, 
      yend = -0.31), #-0.35),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
                 colour = "black")+
    annotate("text", x=0.5, y=-0.36, 
             label = expression(r[1]), colour = "black", size = 8) +
    geom_segment(data=d_rect, aes(
      x = -0.25, 
      y = x2min - rect_adj$x2, #0 - 0.05, 
      xend = -0.25, 
      yend = x2max - rect_adj$x2), #r2 - 0.05),
      arrow = arrow(length = unit(0.03, "npc"),
                       ends = "both"), 
                 colour = "black")+ 
    annotate("text", x=-0.3, y=0.4, 
             label = expression(r[2]), colour = "black", size = 8) +
    geom_segment(data = a1, aes(
      x = x, #-0.1 + 0.2087578, 
      y = y, #-0.15, 
      xend = xend, #-0.1 + 0.2087578*2, 
      yend = yend), #-0.15),
      arrow = arrow(length = unit(0.03, "npc"),
        ends = "both"), 
        colour = "black")+ # a1 = 0.2087578
    annotate("text", 
             x=(a1$x+a1$xend)/2, 
             y=a1$y, 
             label = expression(a[1]), 
             colour = "black",
             vjust = 1.2, size = 8) +
    geom_segment(data = a2, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      colour = "black") + # a2 = 0.1807896
    annotate("text", x=a2$x, y=(a2$y+a2$yend)/2, 
             label = expression(a[2]), 
             colour = "black", hjust=-0.2, size = 8) +
    annotate("text", x=-0.18, y=-0.24, 
      label = expression(group("(", list(s[1], s[2]), ")")),
      colour = "black", size = 8) +
  geom_segment(data = l, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      colour = "black") + 
    annotate("text", x=l$x + 0.03, y=(l$y+l$yend)/2, 
             label = expression(l), 
             colour = "black", hjust=-0.2, size = 8) +
  coord_equal()
```

```{r}
#| label: fig-hex-param
#| fig-cap: "The components of the hexagon grid illustrating notation."
#| out-width: 30%
#| fig-pos: H
 
hex_param_vis
```

#### Binning the data

Observations are grouped into bins based on their nearest centroid. This produces a reduction in size of the data from $n$ to $m$, where $m\leq b$ (total number of bins). This can be defined using the function $u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}$, where
$$u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2},$$ maps observation $i$ into $H_h = \{i| u(i) = h\}$. 

By default, the bin centroid is used for describing a hexagon (as done in @fig-NLDR-two-curvy (c)), but any measure of center, such as a mean or weighted mean of the points within each hexagon, could be used. The bin centers, and the binned data, are the two important components needed to render the model representation in high dimensions.  

#### Indicating neighborhood

Delaunay triangulation [@lee1980;@alb2024] is used to connect points so that edges indicate neighboring observations, in both the NLDR layout (@fig-NLDR-two-curvy (d)) and the \pD{} model representation. When the data has been binned the triangulation connects centroids. The edges preserve the neighborhood information from the \kD{} representation when the model is lifted into \pD{}. 

### Rendering the model in \pD{}

The last step is to lift the \gD{} model into \pD{} by computing \pD{} vectors that represent bin centroids. We use the \pD{} mean of the points in a given hexagon, $H_h$, denoted $C_{h}^{(p)}$, to map the centroid $C_{h}^{(2)} = (c_{h1}, c_{h2})$ to a point in \pD{}. Let the \pD{} mean be

$$C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.$$

```{r}
#| label: hexbin-regular-two-curvy2

hex_grid_with_counts_two_curvy2 <- read_rds("data/two_nonlinear/two_nonlinear_hex_grid_with_counts_two_curvy2.rds")
tr_from_to_df_two_curvy2 <- read_rds("data/two_nonlinear/two_nonlinear_tr_from_to_df_two_curvy2.rds")

hex_grid_coloured_two_curvy2 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_two_curvy2, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        colour = "grey70", 
        linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
           aes(x = emb1, y = emb2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos, cex = 2)

trimesh_two_curvy2 <- ggplot() +
  geom_segment(data = tr_from_to_df_two_curvy2,
               aes(
                 x = x_from,
                 y = y_from,
                 xend = x_to,
                 yend = y_to),
               colour = "#000000") +
  geom_point(data = tsne_two_curvy_scaled,
             aes(
               x = tSNE1,
               y = tSNE2
             ),
             colour = "#636363",
            alpha = 0.5,
            size = 0.5
            ) +
  coord_equal() +
  interior_annotation("a1", sc_ltr_pos) 

```

```{r}
#| label: hexbin-regular-two-curvy3

hex_grid_with_counts_two_curvy3 <- read_rds("data/two_nonlinear/two_nonlinear_hex_grid_with_counts_two_curvy3.rds")
tr_from_to_df_two_curvy3 <- read_rds("data/two_nonlinear/two_nonlinear_tr_from_to_df_two_curvy3.rds")

hex_grid_coloured_two_curvy3 <-  ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_two_curvy3, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        colour = "grey70", linewidth=0.2) +
  geom_point(data = tsne_two_curvy_scaled,
           aes(x = emb1, y = emb2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "D") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos, cex = 2)
```

```{r}
#| label: best-fit-tsne
#| fig-cap: "Lifting the \\gD{} fitted model into \\pD{}. Two projections of the \\pD{} fitted model overlaying the data are shown in b, c. The fit is reasonbly tight with the data in one cluster (top one in b), but slightly less so in the other cluster probably because it is in $3\\text{-}D$. Notice also that, in the \\gD{} layout the two clusters have internal gaps which creates a model with some holes. This lacy pattern happens regardless of the hyper-parameter choice, but this doen't severely impact the \\pD{} model representation."
#| fig-width: 8
#| fig-height: 2

trimesh_two_curvy_removed1_tsne_with_data +
  two_curvy_proj_tsne_first_model1 +
  two_curvy_proj_tsne_all_model1 +
  plot_layout(ncol = 3)
```


### Measuring the fit {#sec-summary}
 <!-- Fitted values,  Error calculation-->

The model here is similar to a confirmatory factor analysis model [@brown2015], $\widehat{T}(X_1, X_2, X_3, X_4) + \Epsilon$. The difference between the fitted model and observed values would be considered to be residuals, and for this problem are \fD{}. 

<!--#### Fitted values-->

Observations are associated with their bin center, $C_{h}^{(p)}$, which are also considered to be the *fitted values*. These can also be denoted as $\widehat{X}$. 
<!--#### Error-->

The error is computed by taking the squared \pD{} Euclidean distance, corresponding to computing the root mean squared error (RMSE) as:

$$\sqrt{\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2}$${#eq-equation1} 

where $n$ is the number of observations, $b$ is the number of bins, $n_h$ is the number of observations in $h^{th}$ bin, $p$ is the number of variables and $\mathbfit{x}_{hij}$ is the $j^{th}$ dimensional data of $i^{th}$ observation in $h^{th}$ hexagon. We can consider $e_{hj} = \sqrt{\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2}$ to be the residual for each observation.

```{r}
#| label: error-two-curvy

error_two_curvy_umap <- read_rds("data/two_nonlinear/error_two_curvy_umap.rds")

mse_two_curvy_b <- ggplot(error_two_curvy_umap, 
                     aes(x = a1, 
                         y = RMSE)) +
  geom_vline(xintercept = 0.09,
           colour = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             colour = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             colour = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) + 
  geom_point(size = 1) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = "RMSE") +
  interior_annotation("a", position = c(0.9, 0.1), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))


prop_dens_a1 <- ggplot(error_two_curvy_umap,
                     aes(x = a1,
                         y = prop_comp)) +
  geom_vline(xintercept = 0.09,
           colour = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             colour = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             colour = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = paste("Relative proportion density")) +
  interior_annotation("d", position = c(0.9, 0.9), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))

a1_m_two_curvy <- ggplot(error_two_curvy_umap,
                     aes(x = a1,
                         y = prop_bins)) +
  geom_vline(xintercept = 0.09,
           colour = "#d95f02", linetype="dashed", 
           linewidth=1) +
  geom_vline(xintercept = 0.05,
             colour = "#1f78b4", 
             linetype="solid",
             linewidth=1) +
  geom_vline(xintercept = 0.03, linetype="dotted",
             colour = "#1b9e77", linewidth=1) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 1) +
  # scale_y_continuous(breaks = sort(unique(error_two_curvy$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_two_curvy_umap$a1, 2)))[c(1, 4, 7, 11, 13, 14, 15, 16)]) +
  labs(x = expression(paste("binwidth (", a[1], ")")), y = expression("prop. non-empty bins " * bgroup("(", m / b, ")"))) +
  interior_annotation("b", position = c(0.9, 0.1), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))
```

```{r}
#| label: fig-p-d-error-in-2d-two-curvy
#| fig-cap: "The \\fD{} model error in \\gD{} layout. Color indicates error ($e_{hj}$), dark color indicating high error and light indicates low error. Most large errors are distributed near the sparse end of the nonlinear cluster and most small errors are distributed near the dense corners."
#| out-width: 80%
#| fig-pos: H

 error_plot_two_curvy_quasi + error_plot_two_curvy + two_curvy_proj_tsne_first_model1_coloured +
  plot_layout(ncol=3)
```

### Prediction into \gD{}

<!-- Does this really need an entire subsection? It's only 3 sentences long. -->

A new benefit of this fitted model is that it allows us to now predict a new observation's value in the NLDR, for any method. The steps are to determine the closest bin centroid in \pD{}, $C^{(p)}_{h}$ and predict it to be the centroid of this bin in \gD{}, $C^{(2)}_{h}$. This can be written as, let $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$, then the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. 

### Tuning

The model fitting can be adjusted using these parameters: 

- hexagon bin parameters
    - bottom left bin position $(s_1, \ s_2)$, 
    - the total number of bins ($b$), 
- bin density cutoff, to remove low-density hexagons.

Default values are provided for each of these, but it is expected that the user will examine the RMSE for a range of choices. Choosing these parameters according to RMSE can be automated but it is recommended that the user examine the resulting model representation by overlaying it on the data in \pD{}. The next few subsections describe the calculation of default values, and the effect that different choices have on the model fit.

#### Hexagon bin parameters

The values $(s_1, \ s_2)$ define the position of the centroid of the bottom left hexagon. By default, this is at $s_1 = -q, s_2 = -qr_2$, where $q$ is the buffer bound the data. The choice of these values can have some effect on the distribution of bin counts which is seen in @fig-bins-two-curvy. The distribution of bin counts for $s_1$ varying between $\text{-}0.1\,\text{--}\,0.0$. Generally, a more uniform distribution among these possibilities would indicate that the bins are reliably capturing the underlying distribution of observations. 

```{r}
#| echo: false
#| label: fig-bins-two-curvy
#| fig-pos: H
#| fig-cap: "Hexbin density plots of tSNE layout of the nonlinear cluster data, using three different bin inputs: (a) $b = 240/98 \\text{ } (15, \\text{ }16)$, (b) $b = 720/215 \\text{ } (24, \\text{ }30)$, and (c) $b = 2496/549 \\text{ } (48, \\text{ }52)$. Color indicates standardized counts, dark indicating high count and light indicates low count. At the smallest bin size, the data structure is discontinuous, suggesting that there are too many bins. Using the RMSE of the model fit in $7\\text{-}D$ helps decide on a useful choice of number of bins."
#| fig-width: 9
#| fig-height: 3

hex_grid_coloured_two_curvy2 +
  hex_grid_coloured_two_curvy1 +
  hex_grid_coloured_two_curvy3 +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 12))
``` 

The default number of bins $b=b_1\times b_2$ is computed based on the sample size, by setting $b_1=n^{1/3}$, consistent with the Diaconis-Freedman rule [@freedman1981]. The value of $b_2$ is determined analytically by $b_1, q, r_2$ (@eq-bin2). Values of $b_1$ between $2$ and $b_1 = \sqrt{n/r_2}$ are allowed. @fig-param-two-curvy (a) shows the effect of different choices of $b_1$ on the RMSE of the fitted model.

#### Measurement of capturing the data shape in \gD{} 

<!-- I'll be honest I've no idea what this section is -->
The area of a hexagon is defined as $A = 3\sqrt{3}l^2/2$ where $l$ is the side length of the hexagon. If we know $a_1$ and $a_2$, $l$ can be computed (see appendix). The density of a hexagon grid is calculated as $\sum^{h}_{i=1}n_h/A$ and the proportion is $\sum^{h}_{i=1}n_h/Ab$. The baseline proportion is the proportion density at the smallest possible value of $a_1$. The relative proportion density is the ratio of the observed proportion density to the baseline proportion density.

#### Removal of low density bins

By default, when assessing the choice of $b_1$, the total number of bins is measured by the number of **non-empty** bins. This more accurately reflects the hexagon grid relative the RMSE than the full number of bins in the grid. It may also be beneficial to remove low count bins also, in the situation where data is clustered or stringy, where the observed data is sparse. In order to decide if this is necessary, you would examine the distribution of bin counts, or the density which puts the counts on a standard scale. If there is something of a gap at low values, this would suggest a potential value to use as a cutoff. Alternatively, one could choose to remove based on a percentile, the bins with density in the lowest $5\%$ of all bins, for example. @fig-param-two-curvy (c) illustrates the effect on the model representation of removing bins below different percentages. Generally, we would urge caution in removing low count bins. 

The benchmark value for removing low-density hexagons ranges between $0$ and $1$. When analyzing how these benchmark values influence model performance, it's essential to observe the change in RMSE as the benchmark value increases (@fig-param-two-curvy (c)). The RMSE shows a gradual decrease as the benchmark value goes from $1$ to $0$. Evaluating this rate of increase is important. If the increment is not considerable, the decision might lean towards retaining low-density hexagons.

```{r}
#| label: rm-lwd-bin-error

error_rm_two_curvy <- read_rds("data/two_nonlinear/error_rm_lwd_diff_bin.rds") |>
  mutate(bin1  = as.factor(bin1))

mse_two_curvy_lwd <- ggplot(error_rm_two_curvy, 
                     aes(x = benchmark_rm_lwd, 
                         y = RMSE,
                         colour = a1,
                         linetype = a1)) + 
  geom_point(
    size = 1
    ) +
  geom_line(
    linewidth = 0.5
    ) + 
  scale_x_continuous(breaks = sort(unique(error_rm_two_curvy$benchmark_rm_lwd))[c(1, 3, 5, 7, 9, 11, 13, 15, 17)]) +
  scale_color_manual(values=c("#d95f02", "#1f78b4", "#1b9e77"),
                     labels = parse_format()) +
  scale_linetype_manual(values = c("dashed", "solid", "dotted"),
                        labels = parse_format()) +
  xlab("bin density cutoff") +
  ylab("RMSE") +
  interior_annotation("c", position = c(0.07, 0.9), cex = 2) +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 20, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        axis.title = element_text(size = 18),
axis.text = element_text(size = 15))
```


```{r}
#| echo: false
#| fig-cap: "Various plots to help assess best hexagon bin parameters (a, b, d) and thresholds to remove low-density bins (c). Both (a) and (c) show RMSE, against binwidth ($a_1$) and bin density cutoff. A good benchmark value for these parameters is when the RMSE drops and then flattens out. Three binwidth choices were made: $0.03$ (orange dashed), $0.05$ (blue solid), and $0.09$ (green dotted) to investigate. As the binwidth increases, the proportion of non-empty bins also increases (b). The relative proportion density decreases and levels off (d). Binwidth $0.05$ is chosen as the initial best binwidth for further analysis. There is no need to remove the low-density hexagons because as shown in (b), there is no considerable drop in RMSE."
#| label: fig-param-two-curvy
#| out-width: 80%
#| fig-width: 12
#| fig-height: 10
#| fig-pos: H

mse_two_curvy_b + a1_m_two_curvy +
  mse_two_curvy_lwd + prop_dens_a1 +
  plot_layout(ncol=2, guides = "collect")
```

### Linked plots

Diagnosing the model while locating points in the \gD{} layout and displaying the generated model overlaid on data in the \pD{} is important.

The \gD{} layout and the langevitour view with the model are linked together via rectangular brushes; when a brush is active, points will be highlighted in the adjacent view. Because the langevitour is dynamic, brush events that become active will pause the animation, so that a user can interrogate the current view. The interface is constructed as a **browsable HTML widget** specifically designed for interactive data analysis.

To understand how well the model fits the points whether it fits well, works better in some positions, or fails to match the overall pattern. It is important to link and brush the points with high model error in the \pD{} error plot, \gD{} layout, and the generated model overlay on the data in \pD{}.

## Assessing the best fit, and hence most accurate \gD{} layout {#sec-bestfit} 

@fig-toy-rmse illustrates the approach to compare the fits for different representations and assess the strength of any fit. What does it mean to be a best fit for this problem? Analysts use an NLDR layout to display the structure present in high-dimensional data in a convenient \gD{} display. It is a competitor to linear dimension reduction that can better represent nonlinear associations such as clusters. However, these methods can hallucinate, suggesting patterns that don't exist, and grossly exaggerate other patterns. Having a layout that best fits the high-dimensional structure is desirable but more important is to identify bad representations so they can be avoided. The goal is to help users decide on a the most useful and appropriate low-dimensional representation of the high-dimensional data. 

A particular pattern that we commonly see is that analysts tend to pick layouts with clusters that have big separations between them. When you examine their data in a tour, it is almost always that we see there are no big separations, and actually often the suggested clusters are not even present. While we don't expect that analysts include animated gifs of tours in their papers, we should expect that any \gD{} representation adequately indicates the clustering that is present, and honestly show lack of separation or lack of clustering when it doesn't exist. It is important for analysts to have tools to select the accurate representation not the pretty but wrong representation.

To compare and assess a range of representations an analyst needs:

- a selection of NLDR representations made with a range of parameter choices and possibly different methods (tSNE, UMAP, ...).
- a range of model fits made by varying bin size and low density bin removal.
- calculated RMSE for each layout, when it is transformed into the \pD{} space. 

Comparing the RMSE to obtain the best fit is appropraite if the same NLDR method is used. However, because the RMSE is computed on \pD{} data it measures the fit between model and data so it can also be used to compare the fit of different NLDR methods. A lower RMSE indicates a better NLDR representation.

```{r}
#| label: read-two-curvy-clust-nldr
# Read a variety of different NLDR representations of two_non_linear_diff_shaped_close_clusters
# and plot them on same aspect ratio
tsne_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_30.rds")

nldr_two_curvy1 <- tsne_two_curvy |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a") +
  theme(aspect.ratio = 1) 

tsne_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_tsne_perplexity_62.rds")

nldr_two_curvy6 <- tsne_two_curvy |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("f") +
  theme(aspect.ratio = 1) 

umap_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_umap_n-neigbors_15_min-dist_0.1.rds")

nldr_two_curvy2 <- umap_two_curvy |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#636363') +
  interior_annotation("b", c(0.08, 0.9)) +
  theme(aspect.ratio = 1) 

phate_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_phate_knn_5.rds")

nldr_two_curvy3 <- phate_two_curvy |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("c") +
  theme(aspect.ratio = 1) 

trimap_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_trimap_n-inliers_12_n-outliers_4_n-random_3.rds")

nldr_two_curvy4 <- trimap_two_curvy |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("d") +
  theme(aspect.ratio = 1)

pacmap_two_curvy <- read_rds("data/two_nonlinear/two_non_linear_diff_shaped_close_clusters_pacmap_n-neighbors_10_init_random_MN-ratio_0.5_FP-ratio_2.rds")

nldr_two_curvy5 <- pacmap_two_curvy |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e", c(0.08, 0.9)) +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-two_curvy

error_two_curvy_umap <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_umap.rds")
error_two_curvy_tsne <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_tsne.rds")
error_two_curvy_phate <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_phate.rds")
error_two_curvy_trimap <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_trimap.rds")
error_two_curvy_pacmap <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_pacmap.rds")
error_two_curvy_tsne2 <- read_rds("data/two_nonlinear/error_two_non_linear_diff_shaped_close_clusters_tsne2.rds")

error_two_curvy_tsne2 <- error_two_curvy_tsne2 |>
  mutate(method = "tSNE2")

error_two_curvy <- bind_rows(error_two_curvy_umap, 
                         error_two_curvy_tsne,
                         error_two_curvy_phate,
                         error_two_curvy_trimap,
                         error_two_curvy_pacmap,
                         error_two_curvy_tsne2)

error_two_curvy <- error_two_curvy |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(RMSE == min(RMSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-two_curvy

error_plot_two_curvy <-
  plot_mse(error_two_curvy) +
  scale_x_continuous(breaks =
    sort(unique(error_two_curvy$a1))[
      seq(1, length(
        unique(error_two_curvy$a1)), 
        by = 5)]) +
  scale_color_manual(
    values=c('#e41a1c','#ff7f00','#4daf4a', 
             "#a65628",'#636363', '#984ea3')) +
  theme(aspect.ratio = 1.5)
```

```{r}
#| fig-cap: "Assessing which of the 6 NLDR layouts on the two nonlinear clusters data is the better representation using RMSE for varying binwidth ($a_1$). Color used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-f). Layout d is universally poor. Layouts a, b, e that show two close clusters are universally suboptimal. Layout b with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. Layout e has small separation with oddly shaped clusters. Layout a is the best choice."
#| label: fig-toy-rmse
#| fig-pos: H
#| out-width: 100%
#| fig-height: 6
#| fig-width: 6

error_plot_two_curvy + wrap_plots(nldr_two_curvy1, nldr_two_curvy2, nldr_two_curvy3, 
                                    nldr_two_curvy4, nldr_two_curvy5,
                                    nldr_two_curvy6, ncol = 2, 
                                    widths = c(50, 50))
```

## Curiosities about NLDR results discovered by examining the model in the data space {#sec-curiosities}

With the drawing of the model in the data, several interesting differences between NLDR methods can be observed.

### Some methods appear to order points in the layout

When examining the \gD{} model representations it appears to be flatter or like a pancake with some methods, especially PACMAP, when the data is structure is higher than two dimensional. A simple example of this can be seen with data simulated to contain five \fD{} Gaussian clusters. Each cluster is essentially a ball in \fD{}, so there is no \gD{} representation, rather the model in each cluster should resemble a crumpled sheet of paper that fills out \fD{}.

@fig-five-gau-projs a1, b1, c1 show the \gD{} layouts for (a) tSNE, (b) UMAP, and (c) PaCMAP, respectively. The default hyper-parameters for each method are used. In each layout we can see an accurate representation where all five clusters are visible, although with varying degrees of separation.

The models are fitted to each these layouts. @fig-five-gau-projs a2, b2, c2 show the fitted models in a projection of the \fD{} space, taken from a tour. These clusters are fully \fD{} in nature, so we would expect the model to be a *crumpled sheet* that stretches in all four dimensions. This is what is mostly observed for tSNE and UMAP. The curious detail is that the model for PaCMAP is closer to a *pancake* in shape in every cluster! This single projection only shows this in three of the five clusters but if we examine a different projection the other clusters exhibit the pancake also. While we don't know what exactly causes this, it is likely due to some ordering of points in the \gD{} PaCMAP layout that induces the flat model. One could imagine that if the method used principal components on all the data, that it might induce some ordering that would produce the flat model. If this were the reason, the pancaking would be the same in all clusters, but it is not: The pancake is visible in some clusters in some projections but in other clusters it is visible in different projections. It might be due to some ordering by nearest neighbors in a cluster. The PaCMAP documentation doesn't provide any helpful clues. That this happens, though, makes the PaCMAP layout inadequate for representing the high-dimensional data. 

<!--Projections-->
```{r}
#| label: five-gau-proj-tsne-model

tsne_gau_scaled <- read_rds("data/five_gau_clusters/tsne_gau_scaled.rds")
tr_from_to_df_gau1 <- read_rds("data/five_gau_clusters/tr_from_to_df_gau1.rds")

trimesh_removed_gau_tsne <- ggplot() + 
  geom_point(
    data = tsne_gau_scaled,
    aes(
      x = tSNE1,
      y = tSNE2
    ),
    colour = clr_choice,
    alpha = 0.1) + 
  geom_segment(
    data = tr_from_to_df_gau1,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    colour = "#000000",
    linewidth = 0.5) +
  interior_annotation("a1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )

```

```{r}
#| label: five-gau-tsne-projs
## First projection
proj_obj1 <- read_rds("data/five_gau_clusters/tsne_gau_proj_obj1.rds")

five_gau_proj_tsne_model1 <- plot_proj(
  proj_obj = proj_obj1,
  point_param = c(1.5, 0.05, clr_choice),
  line_param = c(0.8, 0.4, "#000000"),
  plot_limits = c(-0.89, 0.89),
  axis_text_size = 5,
  is_category = FALSE
) +
  interior_annotation("a2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection

proj_obj2 <- read_rds("data/five_gau_clusters/tsne_gau_proj_obj2.rds")

five_gau_proj_tsne_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "a3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none") 
```

```{r}
#| label: five-gau-proj-umap-model

umap_gau_scaled <- read_rds("data/five_gau_clusters/umap_gau_scaled.rds")
tr_from_to_df_gau1_umap <- read_rds("data/five_gau_clusters/tr_from_to_df_gau1_umap.rds")

trimesh_removed_gau_umap <- ggplot() + 
  geom_point(
    data = umap_gau_scaled,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    colour = clr_choice,
    alpha = 0.05) + 
  geom_segment(
    data = tr_from_to_df_gau1_umap,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    colour = "#000000",
    #alpha = 0.4,
    linewidth = 0.5) +
  interior_annotation("b1", 
                      position = c(0.95, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )
```

```{r}
#| label: five-gau-umap-projs

## First projection
proj_obj1 <- read_rds("data/five_gau_clusters/umap_gau_proj_obj1.rds")

five_gau_proj_umap_model1 <- plot_proj(
  proj_obj = proj_obj1,
  point_param = c(1.5, 0.05, clr_choice),
  line_param = c(0.8, 0.4, "#000000"),
  plot_limits = c(-0.89, 0.89),
  axis_text_size = 5,
  is_category = FALSE
) +
  interior_annotation("b2", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
proj_obj2 <- read_rds("data/five_gau_clusters/umap_gau_proj_obj2.rds")

five_gau_proj_umap_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "b3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none")
```

```{r}
#| label: five-gau-proj-pacmap-model

pacmap_gau_scaled <- read_rds("data/five_gau_clusters/pacmap_gau_scaled.rds")
tr_from_to_df_gau1_pacmap <- read_rds("data/five_gau_clusters/tr_from_to_df_gau1_pacmap.rds")
  
trimesh_removed_gau_pacmap <- ggplot() + 
  geom_point(
    data = pacmap_gau_scaled,
    aes(
      x = PaCMAP1,
      y = PaCMAP2),
      colour = clr_choice,
    alpha = 0.1) +
  geom_segment(
    data = tr_from_to_df_gau1_pacmap,
    aes(
      x = x_from,
      y = y_from,
      xend = x_to,
      yend = y_to),
    colour = "#000000",
    #alpha = 0.4,
    linewidth = 0.5) +
  interior_annotation("c1", 
                      position = c(0.08, 0.95),
                      cex = 2) +
  theme(
    aspect.ratio = 1
  )
```

```{r}
#| label: five-gau-pacmap-projs
## First projection
proj_obj1 <- read_rds("data/five_gau_clusters/pacmap_gau_proj_obj1.rds")

five_gau_proj_pacmap_model1 <- plot_proj(
  proj_obj = proj_obj1,
  point_param = c(1.5, 0.05, clr_choice),
  line_param = c(0.8, 0.4, "#000000"),
  plot_limits = c(-0.89, 0.89),
  axis_text_size = 5,
  is_category = FALSE
) +
  interior_annotation("c2",
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
proj_obj2 <- read_rds("data/five_gau_clusters/pacmap_gau_proj_obj2.rds")

five_gau_proj_pacmap_model2 <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.05, clr_choice), # size, alpha, color
  line_param = c(0.8, 0.4, "#000000"), # linewidth, alpha
  plot_limits = c(-1.3, 1.3), 
  axis_text_size = 4, 
  is_category = FALSE) +
  interior_annotation(label = "c3", position = c(0.08, 0.9), cex = 2) +
  theme(aspect.ratio = 1,
        legend.position = "none")

```

```{r}
#| label: fig-five-gau-projs
#| fig-height: 10
#| fig-width: 15
#| fig-pos: H
#| fig-cap: "NLDR's organise points in the \\gD{} layout in different ways, possibly misleadingly, illustrated using three layouts: (a) tSNE, (b) UMAP, (c) PaCMAP. The data has five Gaussian cluster in \\fD{}. The bottom row of plots shows a \\gD{} projection from a tour on \\fD{} revealing the differences generated by the layouts on the model fits.  We would expect the model fit to be like that in (a2) where it is distinctly is separate for each cluster but like a hairball in each. This would indicate the distinct clusters, each being fully \\fD{}. With (c2), the curiousity is that the model is a \\gD{} pancake shape in \\fD{}, indicating that there is some ordering of points done by PaCMAP, posisbly along some principal component axes."

trimesh_removed_gau_tsne + trimesh_removed_gau_umap + trimesh_removed_gau_pacmap +
five_gau_proj_tsne_model1 + five_gau_proj_umap_model1 + five_gau_proj_pacmap_model1 + 
  plot_layout(guides = "collect", nrow = 2) &
  theme(legend.position='none')
```

### Low density results in a squishing of the \gD{} representation in all methods {#sec-effect-dens}

Differences in density can arise by sampling at different rates in different subspaces of \pD{}. For example, the data shown in @fig-one-dens_clust-error all lies on a \gD{} curved sheet in \fD{}, but one end of the sheet is sampled densely and the other very sparsely. It was simulated to illustrate the effect of the density difference on layout generated by an NLDR, illustrated using the tSNE results. 

@fig-one-dens_clust-error (a2, b2) shows a \gD{} layout for tSNE created using the default hyper-parameters. One would expect to see a rectangular shape if the curved sheet is flattened, but the layout is triangular. The other two displays show the residuals as a dot density plot (a1, b1), and a \gD{} projection of the data and the model from \fD{} (a3, b3). Using linked brushing between the plots, we can highlight points in the tSNE layout, and examine where they fall in the original \fD{}. The darker (maroon) points indicate points that have been highlighted by linking. In row a, the points at the top of the triangle are highlighted, and we can see these correspond to higher residuals, and also to all points at the low density end of the curved sheet. In row b, points at the lower left side of the triangle are highlighted which corresponds to smaller residuals and one corner of the sheet at the high density end of the curved sheet. 

The tSNE behaviour is to squeeze the low density area of the data together into the layout. This is common in other NLDR methods also, which means analysts need to be aware that if their data is not sampled relatively uniformly, apparent closeness in the \gD{} may correspond to sparseness in \pD{}.

```{r}
#| label: error-model-one-dens-clust

error_df_one_curvy_abs_selected11 <- read_rds("data/one_c_shaped_dens_structure/error_df_one_curvy_abs_selected11.rds")

error_df_one_curvy_abs_deselected11 <- read_rds("data/one_c_shaped_dens_structure/error_df_one_curvy_abs_deselected11.rds")

error_plot_one_curvy_hist_selected <- ggplot(error_df_one_curvy_abs_deselected11, 
      aes(x = sqrt_row_wise_total_error, 
          y = density)) +
  geom_point(alpha=0.7, colour = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected11, 
      aes(x = sqrt_row_wise_total_error, 
          y = density),
      alpha=0.7, colour = "#800026", size = 3) +
  xlab(expression(e[hj])) +
  ylab("") +
  interior_annotation("b1",
                      cex = 2) +
  theme_bw() +
  theme(aspect.ratio = 1,
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))

error_df_one_curvy_abs_selected12 <- read_rds("data/one_c_shaped_dens_structure/error_df_one_curvy_abs_selected12.rds")

error_df_one_curvy_abs_deselected12 <- read_rds("data/one_c_shaped_dens_structure/error_df_one_curvy_abs_deselected12.rds")

error_plot_one_curvy_hist <- ggplot(error_df_one_curvy_abs_deselected12, 
      aes(x = sqrt_row_wise_total_error, 
          y = density)) +
  geom_point(alpha=0.7, colour = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected12, 
      aes(x = sqrt_row_wise_total_error, 
          y = density),
      alpha=0.7, colour = "#800026", size = 3) +
  xlab(expression(e[hj])) +
  ylab("") +
  interior_annotation("a1",
                      cex = 2) +
  theme_bw() +
  theme(aspect.ratio = 1,
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 15))
```

```{r}
#| label: link-model-one-dens-clust

plot_tsne_dens <- error_df_one_curvy_abs_deselected12 |>
  ggplot(aes(x = emb1,
             y = emb2)) +
  geom_point(alpha=0.7, colour = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected12,
             aes(x = emb1, y = emb2),
             alpha=0.7, colour = "#800026", size = 3) +
  theme(aspect.ratio = 1) +
  interior_annotation("a2",
                      position = c(0.08, 0.9),
                      cex = 2)

plot_tsne_dens_selected <- error_df_one_curvy_abs_deselected11 |>
  ggplot(aes(x = emb1,
             y = emb2)) +
  geom_point(alpha=0.7, colour = "#d9d9d9") +
  geom_point(data = error_df_one_curvy_abs_selected11,
             aes(x = emb1, y = emb2),
             alpha=0.7, colour = "#800026", size = 3) +
  theme(aspect.ratio = 1) +
  interior_annotation("b2",
                      position = c(0.08, 0.9),
                      cex = 2)
```

```{r}
#| label: proj-one-dens-clust

## First projection

projected_df <- read_rds("data/one_c_shaped_dens_structure/projected_df1.rds")
model_df <- read_rds("data/one_c_shaped_dens_structure/model_df1.rds")
axes <- read_rds("data/one_c_shaped_dens_structure/axes1.rds")
circle <- read_rds("data/one_c_shaped_dens_structure/circle1.rds")

five_c_shaped_proj_tsne_model1 <- ggplot() +
      geom_point(
        data = projected_df,
        aes(
          x = proj1,
          y = proj2,
          colour = factor(cluster, levels = c("deselected", "selected")),
          size = factor(cluster, levels = c("deselected", "selected"))),
        alpha = 0.5) +
      geom_segment(
        data = model_df,
        aes(
          x = proj1_from,
          y = proj2_from,
          xend = proj1_to,
          yend = proj2_to),
        colour = "#000000",
        linewidth = 0.8,
        alpha = 0.4) +
      geom_segment(
          data=axes,
          aes(x=x1, y=y1, xend=x2, yend=y2),
          colour="grey70") +
        geom_text(
          data=axes,
          aes(x=x2, y=y2),
          label=rownames(axes),
          colour="grey50",
          size = 4.5) +
        geom_path(
          data=circle,
          aes(x=c1, y=c2), colour="grey70") +
        xlim(c(-0.35, 0.35)) +
        ylim(c(-0.35, 0.35)) +
        interior_annotation(label = "a3", position = c(0.08, 0.9), cex = 2) +
        scale_color_manual(values = c('#d9d9d9', '#800026')) +
        scale_size_manual(values = c('deselected' = 1.5, 'selected' = 3)) +
        theme(aspect.ratio = 1,
              legend.position = "none")

projected_df <- read_rds("data/one_c_shaped_dens_structure/projected_df2.rds")
model_df <- read_rds("data/one_c_shaped_dens_structure/model_df2.rds")
axes <- read_rds("data/one_c_shaped_dens_structure/axes2.rds")
circle <- read_rds("data/one_c_shaped_dens_structure/circle2.rds")

five_c_shaped_proj_tsne_model1_selected <- ggplot() +
      geom_point(
        data = projected_df,
        aes(
          x = proj1,
          y = proj2,
          colour = factor(cluster, levels = c("deselected", "selected")),
          size = factor(cluster, levels = c("deselected", "selected"))),
        alpha = 0.5) +
      geom_segment(
        data = model_df,
        aes(
          x = proj1_from,
          y = proj2_from,
          xend = proj1_to,
          yend = proj2_to),
        colour = "#000000",
        linewidth = 0.8,
        alpha = 0.4) +
      geom_segment(
          data=axes,
          aes(x=x1, y=y1, xend=x2, yend=y2),
          colour="grey70") +
        geom_text(
          data=axes,
          aes(x=x2, y=y2),
          label=rownames(axes),
          colour="grey50",
          size = 4.5) +
        geom_path(
          data=circle,
          aes(x=c1, y=c2), colour="grey70") +
        xlim(c(-0.35, 0.35)) +
        ylim(c(-0.35, 0.35)) +
  interior_annotation(label = "b3", position = c(0.08, 0.9), cex = 2) +
  scale_color_manual(values = c('#d9d9d9', '#800026')) +
  scale_size_manual(values = c('deselected' = 1.5, 'selected' = 3)) +
  theme(aspect.ratio = 1,
        legend.position = "none")
```

```{r}
#| label: fig-one-dens_clust-error
#| fig-width: 15
#| fig-height: 10
#| fig-pos: H
#| out-width: 100%
#| fig-cap: "Exploring the effect of density on the NLDR layout using a C-shaped structure with different density in each end. The sparse end of the structure shows a high model error, while the dense end shows a low model error. The top part of the triangular shaped with linked brushing. The tSNE layout, with dense points (a1-a3) are colored according to their \\fD{} model error. Darker colors represent a high error, while lighter colors indicate a low error. a2, a3 and b2, b3 present the results from linked plots that brushed the high \\fD{} error points in \\gD{} and \\fD{}. It helps in identifying the high \\fD{} model errors that occur due to the sparse end."

error_plot_one_curvy_hist + plot_tsne_dens + 
  five_c_shaped_proj_tsne_model1 +
  error_plot_one_curvy_hist_selected + plot_tsne_dens_selected +
  five_c_shaped_proj_tsne_model1_selected +
    plot_layout(guides = "collect", nrow = 2) &
    theme(legend.position='none')
```

## Applications {#sec-applications}

To illustrate the approach we use two examples: PBMC3k data (single cell gene expression) where an NLDR layout is used to represent cluster structure present in the \pD{} data, and MNIST hand-written digits where NLDR is used to represent essentially a low-dimensional nonlinear manifold in \pD{}.

### PBMC3k {#sec-pbmc}

This is a benchmark single-cell data set collected on Human Peripheral Blood Mononuclear Cells (PBMC3k) as used in @pbmc2019. Single-cell data measures the gene expression of individual cells in a sample of tissue (see  for example, @haque2017). This type of data is used to obtain an understanding of cellular level behavior and heterogeneity in their activity. Clustering of single-cell data is used to identify groups of cells with similar expression profiles. NLDR often used to summarize the cluster structure. Usually, NLDR does not use the cluster labels to compute the layout, but uses color to represent the cluster labels when it is plotted. 

In this data there are $2622$ single cells and $1000$ gene expressions (variables). Following their pre-processing, different NLDR techniques were performed on the first nine principal components. @fig-NLDR-variety shows this data using a variety of methods, and different hyper-parameters. You can see that the result is wildly different depending on the choices. Layout a is a reproduction of the layout published in @chen2024. This layout suggests that the data has three very well separated clusters, each with an odd shape. The question is whether this accurately represents the cluster structure in the data, or whether they should have chosen b or c or d or e or f or g. This is what our new method can help with -- to decide which is the more accurate \gD{} representation of the cluster structure in the \pD{} data. 

@fig-pbmc-rmse shows RMSE across a range of binwidths ($a_1$) for each of the layouts in @fig-NLDR-variety. The layouts were made using tSNE, UMAP, PHATE, PaCMAP, and TriMAP with various hyper-parameter settings. Lines are color coded to match the color of the layouts shown on the right. Lower RMSE indicates the better fit. Using a range of binwidths shows how the model changes, with possibly the best model being one that is universally low RMSE across all binwidths. It can be seen that layout f is sub-optimal with universally higher RMSE. Layout a, the published one, is better but it is not as good as layouts b, d, or e. With some imagination layout d perhaps shows three barely distinguishable clusters. Layout e shows three, possibly four, clusters that are more separated. The choice reduces from eight to these two. Layout d has slightly better RMSE when the $a_1$ is small, but layout e beats it at larger values. Thus we could argue that layout e is the most accurate representation of the cluster structure, of these eight.

To further assess the choices, we need to look at the model in the data space, by using a tour to show the wireframe model overlaid on the data in the $9\text{-}D$ space (@fig-model-pbmc-author-proj). Here we compare the published layout (a) versus what we argue is the best layout (e). The top row (a1, a2, a3) correspond to the published layout and the bottom row (e1, e2, e3) correspond to the optimal choice according to our procedure. The middle and right plots show two projections. The primary difference between the two models is that, the model of layout e does not fill out to the extent of the data but concentrates in the center of each point cloud. Both suggest that three clusters is a reasonable interpretation of the structure, but layout e more accurately reflects the separation between them, which is small.

<!--Fit the best model for author suggestion and compute error-->
```{r}
#| label: hexbin-pbmc
#| 
umap_pbmc_scaled_with_cluster <- read_rds("data/pbmc3k/umap_pbmc_scaled_with_cluster.rds")

umap_tr_from_to_df_pbmc <- read_rds("data/pbmc3k/umap_tr_from_to_df_pbmc.rds")
  
trimesh_removed_pbmc <- ggplot() + 
  geom_point(
    data = umap_pbmc_scaled_with_cluster,
    aes(
      x = UMAP1,
      y = UMAP2,
      colour = cluster
    ),
    alpha = 0.2
  )  +
  geom_segment(data = umap_tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada")) +
  interior_annotation("a1", c(0.08, 0.9), cex = 2) +
  theme(
    aspect.ratio = 1
  )
```


```{r}
#| label: pbmc-umap-model-proj

## First projection
proj_obj1 <- read_rds("data/pbmc3k/pbmc_umap_proj_obj1.rds")

pbmc_proj_umap_model1 <- plot_proj(
  proj_obj = proj_obj1, 
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-0.65, 0.65), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "a2", cex = 2) + 
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

## Second projection
proj_obj2 <- read_rds("data/pbmc3k/pbmc_umap_proj_obj2.rds")

pbmc_proj_umap_model2 <- plot_proj(
    proj_obj = proj_obj2, 
    point_param = c(1.5, 0.2), # size, alpha, color
    plot_limits = c(-1, 0.75), 
    axis_text_size = 4,
    is_category = TRUE) +
  interior_annotation(label = "a3", cex = 2) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

```

```{r}
#| label: combine-data-pbmc

error_pbmc_umap <- read_rds("data/pbmc3k/error_pbmc_umap_30_min_dist_0.3.rds")
error_pbmc_umap2 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.01.rds")
error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.8.rds")
error_pbmc_tsne <- read_rds("data/pbmc3k/error_pbmc_tsne_5.rds")
error_pbmc_tsne2 <- read_rds("data/pbmc3k/error_pbmc_tsne_30.rds")
error_pbmc_phate <- read_rds("data/pbmc3k/error_pbmc_phate_5.rds")
error_pbmc_trimap <- read_rds("data/pbmc3k/error_pbmc_trimap_12_4_3.rds")
error_pbmc_pacmap <- read_rds("data/pbmc3k/error_pbmc_pacmap_30_random_0.9_5.rds")

error_pbmc <- bind_rows(error_pbmc_umap, 
                        error_pbmc_umap2,
                        error_pbmc_umap3,
                        error_pbmc_tsne,
                        error_pbmc_tsne2,
                        error_pbmc_phate,
                        error_pbmc_trimap,
                        error_pbmc_pacmap)

error_pbmc <- error_pbmc |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(RMSE == min(RMSE)) |>
  ungroup()

error_pbmc <- error_pbmc |>
  mutate(method = factor(method,
                         levels = c("UMAP_5_min_dist_0.8", "UMAP_30_min_dist_0.3", "UMAP_5_min_dist_0.01","tsne_5", "tsne_30", "phate_5", "trimap_12_4_3", "pacmap")))
```

```{r}
#| label: error-comp-pbmc

error_plot_pbmc <- plot_mse(error_pbmc) +
  scale_x_continuous(breaks = sort(unique(error_pbmc$a1))[c(1, 5, 9, 13, 17, 21, 26)]) +
  scale_color_manual(values=c('#999999','#a65628','#e41a1c','#377eb8','#4daf4a','#ff7f00','#984ea3','#f781bf')) 

```

```{r}
#| fig-cap: "Assessing which of the 8 NLDR layouts on the PBMC3k data  (shown in @fig-NLDR-variety) is the better representation using RMSE for varying binwidth ($a_1$). Color used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-h). Layout f is universally poor. Layouts a, c, g, h that show large separations between clusters are universally suboptimal. Layout d with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. The choice of best is between layouts b and e, that have small separations between oddly shaped clusters. Layout e is the best choice."
#| label: fig-pbmc-rmse
#| fig-pos: H
#| out-width: 100%

design <- "
AABB
AABB
AABB
"
error_plot_pbmc + 
  wrap_plots(nldr1c, nldr2c, nldr3c, nldr4c,
             nldr5c, nldr6c, nldr7c, nldr8c, ncol = 2) +
  plot_layout(design = design)
```

<!--best choice-->
<!--Fit the best model and compute error-->
```{r}
#| label: tsne-num-bins-pbmc
tsne_pbmc_scaled_best_with_cluster <- read_rds("data/pbmc3k/tsne_pbmc_scaled_best_with_cluster.rds")

tsne_tr_from_to_df_pbmc <- read_rds("data/pbmc3k/tsne_tr_from_to_df_pbmc.rds")

trimesh_removed_pbmc_best <- ggplot() + 
  geom_point(
    data = tsne_pbmc_scaled_best_with_cluster,
    aes(
      x = tSNE1,
      y = tSNE2,
      colour = cluster
    ),
    alpha = 0.3
  ) +
  geom_segment(data = tsne_tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#000000",
               linewidth = 1) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada")) +
  interior_annotation("e1", cex = 2, c(0.08, 0.9)) +
  theme(
    aspect.ratio = 1
  )
```

```{r}
#| label: pbmc-tsne-model-proj-best

## First projection
proj_obj1 <- read_rds("data/pbmc3k/pbmc_tsne_proj_obj1.rds")

pbmc_proj_tsne_model1_best <- plot_proj(
  proj_obj = proj_obj1,
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-0.65, 0.65), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "e2", cex = 2) +
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))

## Second projection
proj_obj2 <- read_rds("data/pbmc3k/pbmc_tsne_proj_obj2.rds")

pbmc_proj_tsne_model2_best <- plot_proj(
  proj_obj = proj_obj2,
  point_param = c(1.5, 0.2), # size, alpha, color
  plot_limits = c(-1, 0.75), 
  axis_text_size = 4,
  is_category = TRUE) +
  interior_annotation(label = "e3", cex = 2) + 
  scale_color_manual(values = c("#8dd3c7", "#fdb462", "#bebada"))
```

```{r}
#| echo: false
#| fig-cap: "Compare the published \\gD{} layout (a) made with UMAP and the \\gD{} layout selected by RMSE plot (e) made by tSNE. The two plots on the right show  projections from a tour, with the models overlaid. The published layout suggested three very separated clusters, but this is not present in the data. While there may be three clusters they are not well-separated. The difference in model fit also indicates this: the published layout a does not spread out fully into the point cloud like the model generated from layout e. This supports the choice that layout e is the better representation of the data, because it does not exaggerate separation between clusters."
#| label: fig-model-pbmc-author-proj
#| fig-pos: H
#| fig-width: 10
#| fig-height: 15
#| out-width: 90%

free(trimesh_removed_pbmc) + free(trimesh_removed_pbmc_best) +
  pbmc_proj_umap_model1 + pbmc_proj_tsne_model1_best +
  pbmc_proj_umap_model2 + pbmc_proj_tsne_model2_best +
  plot_layout(nrow=2, byrow=FALSE) &
  theme(legend.position='none')
```

### MNIST hand-written digits {#sec-mnist}

The digit "1" of the MNIST dataset [@lecun1998] consists of $7877$ grayscale images of handwritten "1"s. Each image is $28 \times 28$ pixels which corresponds to $784$ variables. The first $10$ principal components, explaining $83\%$ of the total variation, are used. This data is essentially lies on a nonlinear manifold in the high dimensions, defined by the shapes that "1"s make in when sketched, meaning that some pixels are almost always dark and some almost always light. We expect that the best layout captures this type of structure and does not exhibit distinct clusters. 

```{r}
#| label: read-mnist-nldr
# Read a variety of different NLDR representations of mnist
# and plot them on same aspect ratio
tsne_mnist <- read_rds("data/mnist/mnist_tsne30.rds")

nldr_mnist1 <- tsne_mnist |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("a", c(0.08, 0.9)) +
  theme(aspect.ratio = 1)

tsne_mnist2 <- read_rds("data/mnist/mnist_tsne89.rds")

nldr_mnist6 <- tsne_mnist2 |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("f", c(0.08, 0.9)) +
  theme(aspect.ratio = 1)

umap_mnist <- read_rds("data/mnist/mnist_umap.rds")

nldr_mnist2 <- umap_mnist |>
  ggplot(aes(x = emb1,
             y = emb2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("b") +
  theme(aspect.ratio = 1)

phate_mnist <- read_rds("data/mnist/mnist_phate.rds")

nldr_mnist3 <- phate_mnist |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("c", c(0.94, 0.92)) +
  theme(aspect.ratio = 1)

trimap_mnist <- read_rds("data/mnist/mnist_trimap.rds")

nldr_mnist4 <- trimap_mnist |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("d", position = c(0.86, 0.92)) +
  theme(aspect.ratio = 1)

pacmap_mnist <- read_rds("data/mnist/mnist_pacmap.rds")

nldr_mnist5 <- pacmap_mnist |>
  ggplot(aes(x = emb1,
             y = emb2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e", c(0.08, 0.9)) +
  theme(aspect.ratio = 1)
```

```{r}
#| label: combine-data-mnist

error_mnist_umap <- read_rds("data/mnist/error_mnist_umap.rds")
error_mnist_tsne <- read_rds("data/mnist/error_mnist_tsne.rds")
error_mnist_tsne2 <- read_rds("data/mnist/error_mnist_tsne2.rds")
error_mnist_phate <- read_rds("data/mnist/error_mnist_phate.rds")
error_mnist_trimap <- read_rds("data/mnist/error_mnist_trimap.rds")
error_mnist_pacmap <- read_rds("data/mnist/error_mnist_pacmap.rds")

error_mnist <- bind_rows(error_mnist_umap, 
                        error_mnist_tsne,
                        error_mnist_tsne2,
                        error_mnist_phate,
                        error_mnist_trimap,
                        error_mnist_pacmap)

error_mnist <- error_mnist |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(RMSE == min(RMSE)) |>
  ungroup() |>
  mutate(method = factor(method,
                         levels = c("UMAP", "tSNE", "tSNE2", "PHATE", "TriMAP", "PaCMAP")))
```

```{r}
#| label: error-comp-mnist

error_plot_mnist <- plot_mse(error_mnist) +
  scale_x_continuous(breaks = sort(
    unique(error_mnist$a1))[append(seq(1, length(unique(error_mnist$a1)), 
                                       by = 5), 24)]) +
  scale_color_manual(
    values=c('#a65628', '#984ea3', '#999999', 
             '#377eb8', '#ff7f00', '#e41a1c')) +
  theme(aspect.ratio=1.5)

```


```{r}
#| fig-cap: "Assessing which of the 6 NLDR layouts of the MNIST digit 1 data is the better representation using RMSE for varying binwidth ($a_1$). Colour is used for the lines and points in the left plot to match the scatterplots of the NLDR layouts (a-f). Layout c is universally poor. Layouts a, f that show a big cluster and a small circular cluster are universally optimal. Layout a performs well at tiny binwidth (where most points are in their own bin) and not as well as f with larger binwidth, thus layout f is the best choice."
#| label: fig-mnist-rmse
#| fig-pos: H
#| out-width: 92%

error_plot_mnist + wrap_plots(nldr_mnist1, nldr_mnist2, nldr_mnist3, 
                                    nldr_mnist4, nldr_mnist5, nldr_mnist6, ncol = 2)
```

@fig-mnist-rmse compares the fit of six layouts computed using UMAP (b), PHATE (c), TriMAP (d), PaCMAP (e) with default hyper-parameter setting and two tSNE runs, one with default hyper-parameter setting (a) and the other changing perplexity to $89$ (f). The layouts are reasonably similar in that they all have the observations in a single blob. Some (b, c) have a more curved shape than others. Layout e is the most different having a linear shape, and a single very large outlier. Both a and f have a small clump of points perhaps slightly disconnected from the other points, in the lower to middle right. 

The layout plots are colored to match the lines in the RMSE vs binwidth ($a_1$) plot. Layouts a, b and f fit the data better than c, d, e, and layout f appears to be the bets fit. @fig-clust-mnist shows this model in the data space in two projections from a tour. The data is curved in the $10\text{-}D$ space, and the fitted model captures this curve. The small clump of points in the \gD{} layout is highlighted in both displays. These are almost all inside the curve of the bulk of points and are sparsely located. The fact that they are packed together in the \gD{} layout is likely due to the handling of density differences by the NLDR. 

The next step is to investigate the \gD{} layout to understand what information is learned from this representation. @fig-model-error-mnist summarizes this investigation. Plot a shows the layout with points colored by their residual value - darker color indicates larger residual and poor fit. The plots b, c, d, e show samples of hand-written digits taken from inside the colored boxes. Going from top to bottom around the curve shape we can see that the "1"s are drawn with from right slant to a left slant. The "1"s in d (black box) tend to have the extra up stroke but are quite varied in appearance. The "1"s shown in the plots labelled e correspond to points with big residuals. They can be seen to be more strangely drawn than the others. Overall, this \gD{} layout shows a useful way to summarize the variation in way "1"s are drawn.

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->

<!-- Fit the model and compute error-->
```{r}
#| label: hexbin-mnist

sc_ltr_pos_mnist <- c(0.96, 0.96)

tsne_minst_scaled <- read_rds("data/mnist/mnist_tsne_minst_scaled.rds")

tsne_plot_mnist_clust <- ggplot() + 
  geom_point(data = tsne_minst_scaled,
             aes(
               x = emb1,
               y = emb2,
               colour = cluster
             ),
             alpha=0.2)+
  scale_color_manual(values=c('#d9d9d9', '#ff7f00')) +
  interior_annotation("a", c(0.08, 0.93), cex = 2.5) +
  theme(legend.position = "none") 

## Compute error
error_df <- read_rds("data/mnist/mnist_error_df.rds")
  
highd_error_2d_mnist <- error_df |>
  ggplot(aes(x = emb1,
             y = emb2,
             colour = sqrt_row_wise_tot_error)) +
  geom_point(alpha=0.2) +
  scale_colour_continuous_sequential(palette = "YlOrRd", n_interp = 20) +
  geom_rect(aes(xmin = 0.82, xmax = 0.92, ymin = 0.38, ymax = 0.58),
               fill = "transparent", colour = "#000000", linewidth = 1) +
  geom_rect(aes(xmin = 0.9, xmax = 1, ymin = 1.25, ymax = 1.45),
               fill = "transparent", colour = "#375cb1", linewidth = 1)  +
  geom_rect(aes(xmin = 0.35, xmax = 0.45, ymin = 0.55, ymax = 0.7),
               fill = "transparent", colour = "#008000", linewidth = 1)  +
  geom_rect(aes(xmin = 0.7, xmax = 0.8, ymin = 0.25, ymax = 0.4),
               fill = "transparent", colour = "#4c389a", linewidth = 1) +
  interior_annotation(label = "a", c(0.08, 0.93), cex = 2) +
  theme(legend.position = "none",
        aspect.ratio = 1,
        plot.margin = margin(0, 0, 0, 0),
        panel.spacing = unit(0, "pt"),
        axis.ticks.length = unit(0, "pt"),
        strip.text = element_blank(),   # Optional, if you're using facets
        axis.title = element_blank()   # Optional
        )
```

```{r}
#| label: mnist-model-proj

proj_obj1 <- read_rds("data/mnist/mnist_proj_obj1.rds")

mnist_proj_tsne_model1_link <- plot_proj(
    proj_obj = proj_obj1,
    point_param = c(1.2, 0.5), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-1.1, 0.9), 
    axis_text_size = 5, 
    is_category = TRUE) +
  interior_annotation(label = "b", cex = 2.5) +
  scale_color_manual(values = c("#d9d9d9",'#ff7f00')) +
  theme(
      legend.position = "none"
  )

## Second projection
proj_obj2 <- read_rds("data/mnist/mnist_proj_obj2.rds")

mnist_proj_tsne_model2_link <- plot_proj(
    proj_obj = proj_obj2, 
    point_param = c(1.2, 0.5), # size, alpha, color
    line_param = c(0.5, 0.5, "#000000"), # linewidth, alpha
    plot_limits = c(-1, 1), 
    axis_text_size = 5, 
    is_category = TRUE) +
  interior_annotation(label = "c", cex = 2.5) +
  scale_color_manual(values = c("#d9d9d9",'#ff7f00')) +
  theme(
      legend.position = "none"
  )

```

```{r}
#| label: fig-clust-mnist
#| fig-pos: H
#| fig-height: 5
#| fig-width: 15
#| out-width: 100%
#| fig-cap: "The tSNE layout of the MNIST digit 1 data shows a big nonlinear cluster (grey) and a small cluster (orange) located very close to the one corner of the big cluster in \\gD{} (a). The MNIST digit 1 data has a nonlinear structure in $10\\text{-}D$. Two \\gD{} projections from a tour on $10\\text{-}D$ reveal that the closeness of the clusters in $10\\text{-}D$ and the twisted pattern of the model fit with tSNE. The brushing feature in the linked plots helps in visualizing the closeness of the small cluster to the big cluster."

tsne_plot_mnist_clust + mnist_proj_tsne_model1_link + mnist_proj_tsne_model2_link +
  plot_layout(ncol = 3)
```

```{r}
#| label: img-diff-pos

## Data with pixel values

img_right_top <- read_rds("data/mnist/mnist_img_right_top.rds")

right_top_img <- plot_digit_img(
  img_right_top, palette = "Blues 2", title_text = "b")

img_middle <- read_rds("data/mnist/mnist_img_middle.rds")

middle_img <- plot_digit_img(
  img_middle, palette = "Greens 3", title_text = "c")

img_right_bottom <- read_rds("data/mnist/mnist_img_right_bottom.rds")

right_bottom_img <- plot_digit_img(
  img_right_bottom, palette = "Purples 2", title_text = "d")

img_error_outside <- read_rds("data/mnist/mnist_img_error_outside.rds")

imge_error_sample_outside <- plot_digit_img(
  img_error_outside, palette = "Grays", title_text = "e")

##### inside
img_error_inside <- read_rds("data/mnist/mnist_img_error_inside.rds")

imge_error_sample_inside <- plot_digit_img(
  img_error_inside, palette = "Oranges", title_text = "f")

```

```{r}
#| label: fig-model-error-mnist
#| fig-cap: "The $10\\text{-}D$ model error in \\gD{} layout of the MNIST digit 1 data shows a pattern. Most low model errors are distributed along the big nonlinear cluster, while most large model errors are distributed along the small cluster. The images associated with large model errors shows different patterns of digit 1, some inside (f) the nonlinear structure and others outside (e). Along the nonlinear cluster, the angle of digit 1 changes (b-d)."
#| fig-pos: H
#| fig-width: 10
#| fig-height: 10
#| out-width: 100%

# Define your current layout
top_layout <- right_top_img + middle_img + right_bottom_img + imge_error_sample_outside + imge_error_sample_inside + plot_layout(nrow = 5)

final_layout <- wrap_plots(highd_error_2d_mnist, top_layout)

final_layout
```

## Discussion {#sec-discussion}

We have developed an approach to help assess and compare NLDR layouts, generated by different methods and hyper-parameter choice(s). It depends on conceptualizing the \gD{} layout as a model, allowing for the creation of a wireframe representation of the model that can be lifted into \pD{}. The fit is assessed by viewing the model in the data space, computing residuals and RMSE. Different layouts can be compared using the RMSE, and provides quantitative and objective methods for deciding on the most suitable NLDR layout to represent the \pD{} data. It also provides a way to predict the values of new \pD{} observations in the \gD{}, which could be useful for implementing uncertainty checks such as using training and testing samples.

Two examples illustrating usage are provided: the PBMC3k data where the NLDR is summarizing clustering in \pD{} and hand-written digits illustrating how NLDR represents an intrinsically lower dimensional nonlinear manifold. We demonstrated that the published layout of the PBMC3k is inaccurate, because it grossly exaggerates separation between clusters, and even suggests separation when there is none. This is common when layouts are chosen subjectively -- often a preference for the "prettiest". Our approach provides a way to objectively choose the layout and hopefully avoids the use of misleading layouts in the future. In the hand-written digits we illustrate how our model fit statistics show that a flat disc layout is superior to the curved shaped layouts, and how to identify oddly written "1"s using the residuals of the fitted model.

Additional exploration of metrics to summarize the fit could be a new direction for the work. The difficulty is capturing nonlinear fits, for which Euclidean distance can be sub-optimal. We have used a very simple approach based on clustering methods, Euclidean distances to nearest centroid, which can approximate nonlinear patterns. Other cluster metrics would be natural choices to explore.

This new method also reveals some interesting curiosities about NLDR procedures. The fitted model appears as a "pancake" in some data where clusters are regularly shaped and high-dimensional, for some methods but not others, which is odd. One can imagine that if algorithms are initiated using principal components then some ordering of points along the major axes might generate this pattern. Alternatively, if local distances dominate the algorithm then is might be possible to see this pattern with well-separated regular clusters. We also demonstrated that there is a tendency for NLDR algorithms to be confused by different density in the data space, and some patterns in the layout are due to density differences rather than nonlinear associations between variables. 

Most NLDR methods only provide a \gD{} but if a \kD{} ($k>2$) layout is provided the approach developed here could be extended. Binning into cubes could be done in $3\text{-}D$ or higher, relatively easily, and used as a the basis for a wireframe of the fitted model. @barber1996 and the software @stephane2023 have algorithms for convex hulls, which \pD{} which serve as an inspiration. A simpler approach using $k$-means clustering to provide centroids could also be possible, but the complication would be to determine how to connect the centroids into an appropriate wireframe.

The new methodology is accompanied by an R package called quollr, so that it is readily usable and broadly accessible. The package has methods to fit the model, compute diagnostics and also visualize the results, with interactivity. We have primarily used the langevitour software [@harisson2024] to view the model in the data space, but other tour software such as tourr [@wickham2011] and detourr [@hart2022] could be also used.

## Supplementary Materials {#sec-supplementary}

All the materials to reproduce the paper can be found at [https://github.com/JayaniLakshika/paper-nldr-vis-algorithm](https://github.com/JayaniLakshika/paper-nldr-vis-algorithm). The Appendix includes more details about the hexagonal binning algorithm and a comparison to the results of the newly reported scDEED [@xia2023] statistic.

The R package `quollr`, available on CRAN and at [https://jayanilakshika.github.io/quollr/](https://jayanilakshika.github.io/quollr/), provides software accompaying this paper to fit the wireframe model representation, compute diagnostics, visualize the model in the data with langevitour and link multiple plots interactively. 

Direct links to videos for viewing online are available in @tbl-links.

```{r}
#| label: tbl-links
#| tbl-pos: H
#| tbl-cap: "Videos of the langevitour animations and the linked plots."
# Video links in the paper

links_df <- read_csv("misc/video_links.csv")

# Create the table
kable(links_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("1.5cm", "4.5cm"))
```

## Acknowledgments

These `R` packages were used for the work: `tidyverse` [@hadley2019], `Rtsne` [@jesse2015], `umap` [@tomasz2023], `patchwork` [@thomas2024], `colorspace` [@achim2020], `langevitour` [@harisson2024], `conflicted` [@hadley2023], `reticulate` [@kevin2024], `kableExtra` [@hao2024]. These `python` packages were used for the work: `trimap` [@amid2019] and `pacmap` [@yingfan2021]. The article was created with `R` packages `quarto` [@jjallaire2024]. 


## References {.unnumbered}
  
::: {#refs}
:::
      
