---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Australia
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-linear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional ($p\text{-}D$) data using non-linear transformation. The methods and parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of $p\text{-}D$ distributions. To help evaluate the NLDR we have developed an algorithm to show the $2\text{-}D$ NLDR model in the $p\text{-}D$ space, viewed with a tour. One can see if the model fits everywhere or better in some subspaces, or completely mismatches the data. It is used to evaluate which $2\text{-}D$ layout is the best representation of the $p\text{-}D$ distribution and see how different methods may have similar summaries or quirks.
  
keywords: [high-dimensional data, dimension reduction, hexagon binning, low-dimensional representation, tour, data vizualization, model-in-the-data-space]
keywords-formatted: [high-dimensional data, dimension reduction, hexagon binning, low-dimensional representation, tour, data vizualization, model-in-the-data-space]

bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
# remotes::install_github("JayaniLakshika/quollr")
library(quollr)
library(tibble)
library(dplyr)
# remotes::install_github("jlmelville/snedata")
# library(snedata)
# library(ggflowchart)
#library(purrr) ## map function
#library(gridExtra) ## for grid.arrange
# library(rsample)
# library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)
library(tidyr)

library(Rtsne)
library(uwot)
library(phateR)
library(patchwork)
library(png)
library(langevitour)
library(colorspace)
library(kableExtra)
library(grid)
library(conflicted)
conflicts_prefer(dplyr::filter)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label, position = c(0.92, 0.92), cex = 1) {
  annotation_custom(grid::textGrob(label = label,
      x = unit(position[1], "npc"), y = unit(position[2], "npc"),
      gp = grid::gpar(cex = cex, col="grey70")))
}
```

```{r}
#| label: scale-highd-data
# Center the data by subtracting the mean of each column
center_data <- function(data) {
  apply(data, 2, function(col) col - mean(col))
}

# Function to scale data manually
scale_data_manual <- function(data, type_col) {
  # Step 1: Center the data (mean 0)
  data_centered <- center_data(data |> select(-all_of(type_col)))
  
  # Step 2: Calculate the standard deviation of each dimension
  sds <- apply(data_centered, 2, sd)
  
  # Step 3: Scale each dimension to have the range [0, 1]
  data_scaled <- apply(data_centered, 2, function(col) col / max(abs(col)))
  
  # Step 4: Scale dimensions according to their variation
  # The dimension with the highest standard deviation is scaled to [-1, 1]
  # Other dimensions are scaled to smaller ranges based on their standard deviations
  max_sd <- max(sds)
  
  # Normalize the standard deviations to get scaling factors
  scaling_factors <- sds / max_sd
  
  for (i in seq_along(scaling_factors)) {
    data_scaled[, i] <- data_scaled[, i] * scaling_factors[i]
  }
  
  # Combine the scaled data with the 'type' column and return as a tibble
  data_scaled <- as_tibble(data_scaled) %>% 
    mutate(!!type_col := data[[type_col]])
  
  return(data_scaled)
}
```

```{r}
#| label: gen-axes

gen_axes <- function(data, proj_data, limits = 1, axis_pos_x = NULL, axis_pos_y = NULL) {
  
  rng <- range(proj_data)
  proj_data <- proj_data/max(abs(rng))
  colnames(proj_data) <- c("P1", "P2")
  proj_data <- data.frame(proj_data)
  obs_labels <- as.character(1:nrow(data))
  
  axis_scale <- limits/6
  
  if (is.null(axis_pos_x)) {
    
    axis_pos_x <- -2/3 * limits
    
  }
  
  if (is.null(axis_pos_y)) {
    
    axis_pos_y <- -2/3 * limits
    
  }
  
  adj <- function(x, axis_pos) axis_pos + x * axis_scale
  axes <- data.frame(x1 = adj(0, axis_pos_x), 
                     y1 = adj(0, axis_pos_y), 
                     x2 = adj(projection[, 1], axis_pos_x), 
                     y2 = adj(projection[, 2], axis_pos_y))
  
  axis_labels <- colnames(data)
  rownames(axes) <- axis_labels
  
  theta <- seq(0, 2 * pi, length = 50)
  circle <- data.frame(c1 = adj(cos(theta), axis_pos_x), 
                       c2 = adj(sin(theta), axis_pos_y))
  
  return(list(axes = axes, circle = circle))
  
}
```

```{r}
#| label: code-setup
set.seed(20240110)
#source("nldr_code.R", local = TRUE)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{1.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data. Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2022], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
#| label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a")

nldr1c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("a")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr2 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b")

nldr2c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#f781bf') +
  interior_annotation("b")


#umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_15_min_dist_0.99.rds")
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c")

nldr3c <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour='#999999') +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d")

nldr4c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#984ea3') +
  interior_annotation("d")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e")

nldr5c <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("e")

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f")

nldr6c <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("f")

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g")

nldr7c <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("g")

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h")

nldr8c <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("h")
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Eight different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 30), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The paper is organised as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD{}   representations of \pD{}   data have been computed using multidimensional scaling (MDS) [@borg2005], which includes principal components analysis (PCA) [@jolliffe2011] as a special case.  The \kD{}   representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{}   layout is constructed by minimizing a stress function that differences distances between points in \pD{}   with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}   minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is \gD{}, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in \gD{}    represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@cfa69] and multidimensional scaling (MDS) [@borg2005]. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)
<!--### Notation -->

<!-- @tbl-notation summarises the notation used to explain the new methodology. The observed data is denoted as $\mathbfit{x}_{n \times p}$ where $x_{ij}$ would indicate the $i^{th}$ observation on the $j^{th}$ variable sampled from a population $\mathbfit{X}$. To refer to variable $j$, we would use $X_j$.--> 

<!--
$X_{n \times p} = \begin{bmatrix} \textbf{x} _{1} & \textbf{x}_ {2} & \cdots & \textbf{x}_{n} \\  \end{bmatrix}^\top$

$Y_{n \times d} = \begin{bmatrix} \textbf{y} _{1} & \textbf{y}_ {2} & \cdots & \textbf{y}_{n} \\  \end{bmatrix}^\top$

$C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$

$C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$ $p$-D mappings of 2D hexagon bin centroids of the $k^{th}$ hexagon

-->

```{r}
#| label: tbl-notation
#| tbl-cap: "Summary of notation for describing new methodology."
# Notation used in the paper

notation_df <- read_csv("misc/notation.csv")

# Create the table
kable(notation_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("3cm", "12cm"))
```

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with (hyper-)parameters $\mathbfit{\theta}$. The parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. <!--Any input requirements for the data (such as normalization, or preprocessing through the use of PCA or similar) is considered part of the function $g$.-->

With our goal being to make a representation of this \gD{} layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbour information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. We recommend using a hexagon grid because it better reflects the data distribution and has less artifacts than a rectangular grid. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-scurve, and documented below.

```{r}
#| label: scurve-training
training_data_scurve <- read_rds("data/s_curve/s_curve_training.rds")
```

<!--UMAP applied for Scurve data-->
```{r}
#| label: umap-scurve
umap_scurve <- read_rds(file = "data/s_curve/s_curve_umap.rds") 

scurve_scaled_obj <- gen_scaled_data(
  data = umap_scurve)

umap_scurve_scaled <- scurve_scaled_obj$scaled_nldr
lim1 <- scurve_scaled_obj$lim1
lim2 <- scurve_scaled_obj$lim2
r2 <- diff(lim2)/diff(lim1)

sc_ltr_pos <- c(0.08, 0.93)
# sc_xlims <- c(-0.5, 1.47)
# sc_ylims <- c(-0.32, 2.1)
sc_xlims <- c(-0.25, 1.25)
sc_ylims <- c(-0.3, 2)

nldr_scurve <- umap_scurve_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, colour="#000000", size = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)
```

<!--Full hexagon grid with UMAP data-->

```{r}
#| label: hexbin-scurve
## Compute hexbin parameters
num_bins_x_scurve <- 7

## hexagon binning to have regular hexagons
hb_obj_scurve <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = num_bins_x_scurve, 
  r2 = r2,
  q = 0.1)

## Data set with all centroids
all_centroids_df <- hb_obj_scurve$centroids

## Generate all coordinates of hexagons
hex_grid <- hb_obj_scurve$hex_poly

## To obtain the standardise counts within hexbins
counts_df <- hb_obj_scurve$std_cts
df_bin_centroids <- extract_hexbin_centroids(
  centroids_df = all_centroids_df, 
  counts_df = counts_df) |>
  filter(drop_empty == FALSE)

hex_grid_with_counts <- 
  left_join(hex_grid,
            counts_df, 
            by = c("hex_poly_id" = "hb_id"))

hex_grid_scurve <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = umap_scurve_scaled, 
             aes(x = UMAP1, y = UMAP2), 
             alpha = 0.5, size = 0.5, color = "#000000") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos)
```

<!--Non-empty bins with bin centroids-->

```{r}
#| label: empty-bin-scurve
hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids$hexID)

hex_grid_nonempty_scurve <- ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids, 
             aes(x = c_x, y = c_y), 
             color = "#33a02c",
             size = 1) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos) 
```

<!--2D model-->

```{r}
#| label: triangulate-scurve
## Triangulate bin centroids
tr1_object_scurve <- tri_bin_centroids(
  df_bin_centroids, x = "c_x", y = "c_y")
tr_from_to_df_scurve <- gen_edges(
  tri_object = tr1_object_scurve)

## Compute 2D distances
distance_scurve <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_scurve, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_scurve <- find_lg_benchmark(
  distance_edges = distance_scurve, 
  distance_col = "distance")

trimesh_removed_scurve <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = benchmark_scurve, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos) 
```

To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "S-curve". It is constructed by simulating $n=750$ observations from $\theta \sim U(-3\pi/2, 3\pi/2)$, $X_1 = \sin(\theta)$, $X_2 \sim U(0, 2)$ (adding thickness to the S), $X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)$. The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance. We would consider $T=(X_1, X_2, X_3)$ to be the geometric structure (true model) that we hope to capture.

```{r}
#| label: s-curve-true-model-proj1

true_model_df <- read_rds("data/s_curve/scurve_true_model.rds")
wireframe_true_model <- read_rds("data/s_curve/scurve_true_model_wireframe.rds")

data_scurve <- training_data_scurve |> 
  select(-ID) |> 
  mutate(type = "data")

model_scurve <- true_model_df |> 
    select(-ID) |> 
  mutate(type = "model")

# Apply the scaling
df_model_data_scurve <- bind_rows(data_scurve, model_scurve)
scaled_scurve <- scale_data_manual(df_model_data_scurve, "type") |>
  as_tibble()

scaled_s_curve_noise <- scaled_scurve |>
  filter(type == "data") |>
  select(-type)

scaled_s_curve_true <- scaled_scurve |>
  filter(type == "model") |>
  select(-type)

projection <- cbind(
    c(0.45181,0.04747,0.02613,0.25735,0.20622,0.08755,-0.11488),
    c(0.03231,0.23556,0.35166,-0.11741,0.12730,0.09544,0.34262))

projected <- as.matrix(scaled_s_curve_noise) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_true <- as.matrix(scaled_s_curve_true) %*% projection

projected_true_df <- projected_true |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())


model_df <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_true_df)[-NCOL(projected_true_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_true_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_true_df)):NCOL(model_df)] <- paste0(names(projected_true_df)[-NCOL(projected_true_df)], "_to")

limits <- 1
rng <- range(projected)
projected <- projected/max(abs(rng))
colnames(projected) <- c("P1", "P2")
projected <- data.frame(projected)
obs_labels <- as.character(1:nrow(scaled_s_curve_noise))

axis_scale <- limits/6
axis_pos <- -2/3 * limits

# axis_scale <- limits/4
# axis_pos <- -0.5 * limits

adj <- function(x) axis_pos + x * axis_scale
axes <- data.frame(x1 = adj(0), 
                   y1 = adj(0), 
                   x2 = adj(projection[, 1]), 
                   y2 = adj(projection[, 2]))

axis_labels <- colnames(scaled_s_curve_noise)
rownames(axes) <- axis_labels

# axis_scale <- 0.15 * limits
# axis_pos <- -0.6 * limits
# adj <- function(x) axis_pos + x * axis_scale
theta <- seq(0, 2 * pi, length = 50)
circle <- data.frame(c1 = adj(cos(theta)), 
                     c2 = adj(sin(theta)))

scurve_proj1_true_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#3182bd") + 
  geom_point(
        size = 0.8,
        alpha = 0.5,
        color = "#000000") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed()

```

```{r}
#| label: s-curve-true-model-proj2

projection <- cbind(
    c(0.03627,0.15991,0.39768,-0.21032,-0.14886,-0.17589,0.23305),
    c(-0.34731,-0.08183,0.15819,0.31926,0.08105,0.10083,0.25627))

projected <- as.matrix(scaled_s_curve_noise) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_true <- as.matrix(scaled_s_curve_true) %*% projection

projected_true_df <- projected_true |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())


model_df <- dplyr::left_join(
  wireframe_true_model, 
  projected_true_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_true_df)[-NCOL(projected_true_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_true_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_true_df)):NCOL(model_df)] <- paste0(names(projected_true_df)[-NCOL(projected_true_df)], "_to")

limits <- 1
rng <- range(projected)
projected <- projected/max(abs(rng))
colnames(projected) <- c("P1", "P2")
projected <- data.frame(projected)
obs_labels <- as.character(1:nrow(scaled_s_curve_noise))

# axis_scale <- 2 * limits/3
# axis_pos <- -0.6 * limits
axis_scale <- limits/6
axis_pos <- -2/3 * limits

adj <- function(x) axis_pos + x * axis_scale
axes <- data.frame(x1 = adj(0), 
                   y1 = adj(0), 
                   x2 = adj(projection[, 1]), 
                   y2 = adj(projection[, 2]))

axis_labels <- colnames(scaled_s_curve_noise)
rownames(axes) <- axis_labels

# axis_scale <- 0.15 * limits
# axis_pos <- -0.6 * limits
# adj <- function(x) axis_pos + x * axis_scale
theta <- seq(0, 2 * pi, length = 50)
circle <- data.frame(c1 = adj(cos(theta)), c2 = adj(sin(theta)))

scurve_proj2_true_model <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#3182bd") +
  geom_point(
        size = 0.8,
        alpha = 0.5,
        color = "#000000") +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed()

```

```{r}
#| label: fig-scurve-true-sc
#| fig-cap: "Two views of the true model (blue lines) in $2\\text{-}D$ projections from $7\\text{-}D$, for the S-curve data (black points). The data is spread along the S shape, and does not vary much from this curve. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/I5GL23vLiw0>)." 
#| fig-pos: H
#| fig-width: 10
#| fig-height: 5

scurve_proj1_true_model + scurve_proj2_true_model + 
  plot_layout(ncol = 2)
```

<!--langevitour with true model for S-curve--> 
<!-- ::: {#fig-scurve-true-sc layout-ncol="3" fig-pos="H"} -->

<!-- ![](figures/scurve/sc_true_1.png){width="150" fig-align="center"} -->

<!-- ![](figures/scurve/sc_true_2.png){width="150" fig-align="center"} -->

<!-- ![](figures/scurve/sc_true_3.png){width="150" fig-align="center"} -->

<!-- Three views of the true model (grey points and lines) in $2\text{-}D$ projections from $7\text{-}D$, for the S-curve data (purple points). The data is spread along the S shape, and does not vary much from this curve. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/I5GL23vLiw0>).  -->
<!-- ::: -->

<!-- XXX The points on the "theoretical model" should be equidistant along the S. They are still to bunched in some spots and too far apart in others. -->


```{r}
#| label: fig-NLDR-scurve
#| echo: false
#| fig-cap: "Key steps for constructing the model on the UMAP layout ($k=2$): (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids. The S-curve data is shown."
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
 
nldr_scurve + hex_grid_scurve + 
  hex_grid_nonempty_scurve + 
  trimesh_removed_scurve +
  plot_layout(ncol = 4)
```

### Algorithm to represent the model in \gD{}  

#### Scale the data

Because we are working with distances between points, starting with data having a standard scale, e.g. [0, 1], is recommended. The default should take the aspect ratio produced by the NLDR $(r_1, r_2, ..., r_k)$ into account. When $k=2$, as in hexagon binning, the default range is $[0, y_{i,\text{max}}], i=1,2$, where $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = \frac{r_2}{r_1}$ (@fig-NLDR-scurve). If the NLDR aspect ratio is ignored then set $y_ {2,\text{max}} = 1$. <!-- \times \frac{2}{\sqrt{3}}$. (The $\frac{2}{\sqrt{3}}$ accounts for the different height ($a_1$) and width ($a_2$) of a regular hexagon.) The scaling of data should take the size of the hexagons into account, but choice of number of bins should. -->

#### Computing hexagon grid configuration

Although there are several implementations of hexagon binning [@carr1987], and a published paper [@dan2023], surprisingly, none has sufficient detail or components that produce everything needed for this project. So we described the process used here. @fig-hex-param illustrates the notation used. 

The \gD{} hexagon grid is defined by its bin centroids. Each hexagon, $H_h$ ($h = 1, \dots, b$) is uniquely described by centroid, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. The number of bins in each direction is denoted as $(b_1, b_2)$, with  $b = b_1 \times b_2$ being the total number of bins. We expect the user to provide just $b_1$ and we calculate $b_2$ using the NLDR ratio, to compute the grid. 

To ensure that the grid covers the range of data values a buffer parameter ($q$) is set as a proportion of the range. By default,  $q=0.1$. The buffer should be extending a full hexagon width ($a_1$) and height ($a_2$) beyond the data, in all directions. The lower left position where the grid starts is defined as $(s_1, s_2)$, and corresponds to the centroid of the lowest left hexagon, $C_{1}^{(2)} = (c_{11}, c_{12})$. This must be smaller than the minimum data value. Because it is one buffer unit, $q$ below the minimum data values, $s_1 = -q$ and $s_2 = -qr_2$. 

The value for $b_2$ is computed by fixing $b_1$. Considering the upper bound of the first NLDR component, $a_1 > \frac{1+2q}{b_1 -1}$. Similarly, for the second NLDR component, $a_2 > \frac{r_2 + q(1 + r_2)}{(b_2 - 1)}$. Since $a_2 = \frac{\sqrt(3)}{2}a_1$ for regular hexagons, $a_1 > \frac{2[r_2 + q(1 + r_2)]}{\sqrt{3}(b_2 - 1)}$. This is a linear optimization problem. Therefore, the optimal solution must occur on a vertex. Therefore, $b_2 = \Big\lceil1 +\frac{2[r_2 + q(1 + r_2)](b_1 - 1)}{\sqrt{3}(1 + 2q)}\Big\rceil$.

```{r}
#| label: code-illustration
# Code to draw illustration for notation
start_pt <- all_centroids_df |> 
  filter(hexID == 1)
d_rect <- tibble(x1min = 0, 
                 x1max = 1,
                 x2min = 0,
                 x2max = r2)

# To move the rectangle to ignore the overlap with the centroids
rect_adj <- tibble(x1 = 0.03, x2 = 0.03)

a1 <- tibble(x = all_centroids_df$c_x[4],
             xend = all_centroids_df$c_x[5],
             y = all_centroids_df$c_y[21],
             yend = all_centroids_df$c_y[21],
             label = expression(a[1]))
a2 <- tibble(x = all_centroids_df$c_x[25],
             xend = all_centroids_df$c_x[25],
             y = all_centroids_df$c_y[25],
             yend = all_centroids_df$c_y[33],
             label = expression(a[2]))
hex_param_vis <- ggplot() + 
    geom_polygon(data = hex_grid, 
                        aes(x = x, 
                            y = y, 
                            group = hex_poly_id),
                 fill = "white", 
                 color = "#bdbdbd") +
    geom_point(data = all_centroids_df, aes(
      x = c_x, 
      y = c_y), 
      color = "#31a354", size = 0.9) +
    geom_point(data = start_pt, aes(x = c_x, 
                                    y = c_y), 
               color = "black") + 
    geom_rect(data=d_rect, 
              aes(xmin = x1min - rect_adj$x1,# - rect_adj$s1, 
                  xmax = x1max - rect_adj$x1,# - rect_adj$s1, 
                  ymin = x2min - rect_adj$x2,# - rect_adj$s2, 
                  ymax = x2max - rect_adj$x2),# - rect_adj$s2), 
              fill = "white", 
              color = "black", 
              alpha = 0, 
              linewidth = 0.7) +
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1max - rect_adj$x1, 
                                y=x2min - rect_adj$x2)) + 
    geom_point(data=d_rect, aes(x=x1min - rect_adj$x1, 
                                y=x2max - rect_adj$x2)) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,0)", 
             hjust=-0.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1max - rect_adj$x1, 
                     y=d_rect$x2min - rect_adj$x2,
                     label = "(0,1)", 
             hjust=1.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1min - rect_adj$x1, 
                     y=d_rect$x2max - rect_adj$x2,
                     label = expression(group("(", 
                        list(0, y[2][max]),")")), 
            hjust=-0.1, vjust=1.2) + 
    geom_segment(data=d_rect, aes(
      x = x1min  - rect_adj$x1, # 0 - 0.03, 
      y = -0.35, 
      xend = x1max - rect_adj$x1, #1 - 0.03, 
      yend = -0.35), #-0.35),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
                 color = "black")+
    annotate("text", x=0.5, y=-0.45, 
             label = expression(r[1]), color = "black") +
    geom_segment(data=d_rect, aes(
      x = -0.25, 
      y = x2min - rect_adj$x2, #0 - 0.05, 
      xend = -0.25, 
      yend = x2max - rect_adj$x2), #r2 - 0.05),
      arrow = arrow(length = unit(0.03, "npc"),
                       ends = "both"), 
                 color = "black")+ 
    annotate("text", x=-0.35, y=1, 
             label = expression(r[2]), color = "black") +
    geom_segment(data = a1, aes(
      x = x, #-0.1 + 0.2087578, 
      y = y, #-0.15, 
      xend = xend, #-0.1 + 0.2087578*2, 
      yend = yend), #-0.15),
      arrow = arrow(length = unit(0.03, "npc"),
        ends = "both"), 
        color = "black")+ # a1 = 0.2087578
    annotate("text", 
             x=(a1$x+a1$xend)/2, 
             y=a1$y, 
             label = expression(a[1]), 
             color = "black",
             vjust = 1.2) +
    geom_segment(data = a2, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + # a2 = 0.1807896
    annotate("text", x=a2$x, y=(a2$y+a2$yend)/2, 
             label = expression(a[2]), 
             color = "black", hjust=-0.2) +
    annotate("text", x=-0.18, y=-0.25, 
      label = expression(group("(", list(s[1], s[2]), ")")),
      color = "black") +
  coord_equal()
```

```{r}
#| label: fig-hex-param
#| fig-cap: "The components of the hexagon grid illustrating notation."
#| out-height: 30%
#| fig-pos: H
 
hex_param_vis
```

<!-- Number of bins is set by fixing $b_1$, which determines the binwidth accounting the offset $q_1$ and breaks on $x_1$, is calculated as  -->

<!-- $$ -->
<!-- a_1 = \frac{r_1 + q_1}{b_1}. -->
<!-- $$ -->

<!-- The computed binwidth then determines the number of vertical bins, $b_2$, and vertical binwidth, which are computed based on $r_2$, the offset $q_2$, and the height/width ratio of a regular hexagon, $\frac{2}{\sqrt{3}}$ as -->

<!-- $$ -->
<!-- a_2 = \frac{r_2 + q_2}{0.75 \times b_2}. -->
<!-- $$ -->

<!-- To define a hexagon grid across the \kD{} space, with hexagons of fixed height ($a_1$), width ($a_2$),  it is necessary to determine how many hexagons should be represented along each axis in the \kD{} space.

When $k=2$, the number of bins along the $x$ and $y$ axes, $b_1$ and $b_2$, is computed by considering the scaling factors ${r_1, r_2}$, along with the offset along the axes ($q_1$, $q_2$), and the height ($a_1$) and width ($a_2$) of a regular hexagon (@fig-NLDR-scurve (b)). ($b_1 = \frac{r_1 + q_1}{a_2}$, and $b_2 = \frac{r_2 + q_2}{0.75 \times a_1}$) where $0.75 \times a_1$ accommodate to have hexagons fill the entire 2D space without leaving any gaps between them.)-->

<!-- XXX We don't need parameters for the regular hexagon, these are fixed constants. -->


#### Binning the data

<!-- Points are allocated to the bin they fall into based on the nearest centroid. In situations where a point is equidistant from multiple centroids, tie-breaking rules are applied. If multiple centroids are in the same row, the point is assigned to the leftmost centroid. If multiple centroids are in different rows, the point is assigned to the bottom centroid. -->

<!-- $\{ i \in H_h, h = 1, \dots, b, \text{ and } i = 1, \dots, n\}$ -->

Observations are grouped into bins based on their nearest centroid. This produces a reduction in size of the data from $n$ to $m$, where $m\leq b$ (total number of bins). This can be defined using the function $u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}$, where
$u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2}$, mapping observation $i$ into $H_h = \{i| u(i) = h\}$. 

By default, the bin centroid is used for describing a hexagon (as done in @fig-NLDR-scurve (c)), but any measure of center, such as a mean or weighted mean of the points within each hexagon, could be used. The bin centers, and the binned data, are the two important components needed to render the model representation in high dimensions.  

<!-- XXX How are you doing this? Do you check the bounds of the hexagon? Or do you use distance to centroid? -->

<!--
Define a hexagon grid across the \kD{} space, using hexagons with fixed height ($a_2$), width ($a_1$). Each of the \kD{} points will belong to a hexagon bin. That is, for each $y \in \mathbfit{Y}$, we can (uniquely) identify the hexagon that the point belongs to. This identification is done finding the nearest bin centroid for the \kD{} points by considering the \kD{} Euclidean distance.    

When $k=2$, the starting coordinates $(s_1, s_2)$ mark the lower left of the grid. This is the bottom left bin centroid. By starting from there, points are generated to fill the grid accounting $b_1$, $b_2$, $a_1$, and $a_2$. 
-->

<!--We deliberately separate out the creation of the hexagon grid from the mapping of points on the grid.-->

#### Indicating neighborhood

Delaunay triangulation [@lee1980;@alb2024] is used to connect points so that edges indicate neighbouring observations, in both the NLDR layout (@fig-NLDR-scurve (d)) and the \pD{} model representation. When the data has been binned the triangulation connectd centroids. The edges preserve the neighborhood information when the model is lifted into \pD{}. 

<!-- When $k = 2$ Delaunay triangulation on $C^{(2)}$ generates the model in \gD{} space, which is a triangular mesh (@fig-NLDR-scurve (d)). It generates convex hulls of $C^{(2)}$ such that the circumcircle of every triangle in the triangulation contains no other points from $C^{(2)}$. -->

When shapes are non-linear in the NLDR layout, some edges could be long. It can also happen that distant centroids can be connected, particularly if clustering is present, which can result in long line segments. In order to generate a smooth surface in \gD{}, these long line segments should be removed when tuning the model fit.

<!--need to add what is meant by a long edge-->

### Rendering the model in \pD{}

The last step is to lift the \kD{} model into \pD{} by computing \pD{} vectors that represent bin centroids. We use the \pD{} mean of the points in $H_h$ to map the centroid $C_{h}^{(2)} = (c_{h1}, c_{h2})$ to a point in \pD{}. Let the \pD{} mean be

$$C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.$$
Furthermore, line segments that exist in the \kD{} model generate line segments in \pD{} by connecting the \pD{} means of the corresponding \kD{} bin centroids. If additional long edges need to be removed, compute the edges in \pD{} and pruned any detected long edges to improve the accuracy. Once pruned, re-plot the \gD{} view to ensure it accurately captures the data.


<!-- langevitour with pD model for S-curve -->
::: {#fig-scurve-sc layout-ncol="4" fig-pos="H"}
![](figures/scurve/umap_trimesh_layout.png)

![](figures/scurve/sc_umap_best_1.png)

![](figures/scurve/sc_umap_best_2.png)

![](figures/scurve/sc_umap_best_3.png)

Model in \gD{}, on the UMAP layout, and three views of the fit in projections from $7\text{-}D$, for the S-curve data ($(s_1, \ s_2) = (-0.160, \ -0.263)$, $b = 405 \  (15, \ 27)$, $m = 70$, benchmark value to remove low density hexagons is $0.250$, and benchmark value to remove large edges is $0.159$). MSE is $0.0462$. The model closely fits the shape, but it doesn't fully fill out the width of the S, which means that it does not adequately capture the surface (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/i7F7xpN1Hz8>).
:::

<!-- XXX You need to have consistent colouring. In langevitour the points corresponding to observations are purple. They should be purple in the \gD{} view too.  -->

<!-- XXX Should there be a second long edge removal? You can compute the edges in \pD{} now, and if there are long edges maybe these should be pruned, and we re-plot the \gD{} view.  -->

### Measuring the fit {#sec-summary}
 <!-- Fitted values,  Error calculation-->

The model here is similar to a confirmatory factor analysis model [@brown2015], $\widehat{T}(X_1, X_2, X_3) + \Epsilon$. The difference between the fitted model and observed values would be considered to be residuals, and for this problem are $7\text{-}D$. 

<!--#### Fitted values-->

Observations are associated with their bin center, $C_{h}^{(p)}$, which are also considered to be the *fitted values*. These can also be denoted as $\widehat{X}$. <!--The fitted values of the points in $H_h$ refers to the \pD{} mapping $C_{h}^{(p)}$ of the corresponding \kD{} model point $C_{h}^{(2)}$.-->

<!--#### Error-->

The error is computed by taking the squared \pD{} Euclidean distance, corresponding to computing the mean squared error (MSE) as:

$$\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2$${#eq-equation1} 

where $n$ is the number of observations, $b$ is the number of bins, $n_h$ is the number of observations in $h^{th}$ bin, $p$ is the number of variables, $\mathbfit{x}_{hij}$ is the $j^{th}$ dimensional data of $i^{th}$ observation in $h^{th}$ hexagon.

```{r}
#| label: best-umap-model-scurve

# Initialize effective bins along x
effective_bin1_scurve <- 15

# Fit high-dimensional model
scurve_model <- fit_highd_model(
  training_data = training_data_scurve,
  emb_df = umap_scurve_scaled,
  bin1 = effective_bin1_scurve,
  r2 = r2,
  q = 0.16,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_scurve <- scurve_model$df_bin_centroids
df_bin_scurve <- scurve_model$df_bin

hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids_scurve$hexID)

```

```{r}
#| label: p-d-error-in-2d-scurve

## Compute error
error_df_scurve <- augment(
  df_bin_centroids = df_bin_centroids_scurve,
  df_bin = df_bin_scurve,
  training_data = training_data_scurve,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "x")

error_df_scurve <- error_df_scurve |>
  mutate(type = case_when(
    row_wise_abs_error == 0 ~ "no error",
    row_wise_abs_error <= 0.2 ~ "error 0-0.2",
    row_wise_abs_error <= 0.4 ~ "error 0.2-0.4",
    row_wise_abs_error <= 0.6 ~ "error 0.4-0.6",
    .default = "error greter than 0.6"
  )) |>
  mutate(type = factor(type, levels = c(
    "no error", "error 0-0.2", "error 0.2-0.4", "error 0.4-0.6", "error greter than 0.6")))


error_df_scurve <- error_df_scurve |>
  bind_cols(umap_scurve_scaled |>
              select(-ID))

error_plot_scurve <- error_df_scurve |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             color = type)) +
  geom_point(alpha=0.5) +
  scale_colour_brewer(
    type = "qual",
    palette = 2
  ) +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10)
  ) +
  coord_equal()
```

```{r}
#| label: fig-p-d-error-in-2d-scurve
#| fig-cap: "error"
#| out-height: 30%
#| fig-pos: H

error_plot_scurve
```

### Prediction into \gD{}

A new benefit of this fitted model is that it allows us to now predict a new observation's value in the NLDR, for any method. The steps are to determine the closest bin centroid in \pD{}, $C^{(p)}_{h}$ and predict it to be the centroid of this bin in \gD{}, $C^{(2)}_{h}$. This can be written as, let $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$, then the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. 

<!--The prediction approach involves finding the nearest \kD{} model point for a new \pD{} point. We define the function $z: \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{m\times p}$, where $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$ maps each \pD{} point to its nearest \pD{} mapping of the model. Therefore, the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$ be the predicted values.-->  

<!--prediction for true-model of S-curve-->
```{r}
#| label: model-prediction-umap-original

predict_umap_scurve <- read_rds(file = "data/s_curve/s_curve_umap_predict_true.rds")

predict_scurve_obj <- gen_scaled_data(
    data = predict_umap_scurve)

predict_umap_scurve_scaled <- predict_scurve_obj$scaled_nldr

plot_predict_umap <- ggplot(data = predict_umap_scurve_scaled,
                            aes(
                              x = UMAP1,
                              y = UMAP2
                            )) +
  geom_point(alpha = 0.5) +
  interior_annotation("b", c(0.92, 0.96))
```

```{r}
#| label: model-prediction-umap-quollr

true_pred_df <- predict_emb(
  test_data = true_model_df,
  df_bin_centroids = df_bin_centroids_scurve,
  df_bin = df_bin_scurve,
  type_NLDR = "UMAP"
)


# Compute radius r
a1 <- calc_bins_y(bin1 = 15, r2 = r2, q = 0.16)$a1
r <- a1 / 2

# Function to jitter points within a circumcircle of a hexagon
jitter_within_circumcircle <- function(center_x, center_y, radius, num_points) {
  theta <- runif(num_points, 0, 2 * pi)
  r <- radius * sqrt(runif(num_points))
  x <- center_x + r * cos(theta)
  y <- center_y + r * sin(theta)
  jittered_points <- data.frame(UMAP1 = x, UMAP2 = y, ID = points_in_hex$ID)
  return(jittered_points)
}

# Jittered points data frame
jittered_points_df <- data.frame()

# Iterate through each hexagon and jitter points within the circumcircle
for (hex_id in unique(true_pred_df$pred_hb_id)) {
  # hex_points <- hex_grid_nonempty %>% filter(hex_poly_id == hex_id)
  # center_x <- mean(hex_points$x)
  # center_y <- mean(hex_points$y)
  # points_in_hex <- true_pred_df %>% filter(pred_UMAP_1 >= min(hex_points$x) & pred_UMAP_1 <= max(hex_points$x) & pred_UMAP_2 >= min(hex_points$y) & pred_UMAP_2 <= max(hex_points$y))
  
  points_in_hex <- true_pred_df |>
    filter(pred_hb_id == hex_id)
  
  center_x <- points_in_hex |> 
    pull(pred_UMAP_1) |> 
    unique()
  
  center_y <- points_in_hex |> 
    pull(pred_UMAP_2) |> 
    unique()

  if (nrow(points_in_hex) > 0) {
    jittered_points <- jitter_within_circumcircle(center_x, center_y, r, nrow(points_in_hex))
    jittered_points_df <- rbind(jittered_points_df, jittered_points)
  }
}

plot_predict_umap_model <- ggplot(data = jittered_points_df,
                            aes(
                              x = UMAP1,
                              y = UMAP2
                            )) +
  geom_point(alpha = 0.5) +
  interior_annotation("c", c(0.92, 0.96))
```

```{r}
#| label: true-model-projection-langevitour

true_model_df_rm_id <- true_model_df |>
  select(-ID)

projection <- cbind(
    c(0.18390,-0.18443,-0.06860,0.00654,-0.02921,0.02313,-0.08963),
    c(-0.02902,0.03638,-0.16872,0.22418,0.01146,0.02777,0.01453))
projected_df <- as.matrix(true_model_df_rm_id) %*% projection |> 
  as_tibble(.name_repair = "unique") |> 
  rename(c("proj1" = "...1", 
           "proj2" = "...2"))

proj_plot_scurve_true <- ggplot(
  data = projected_df, 
  aes(
    x = proj1, 
    y = proj2)) +
    geom_point(alpha = 0.5) +
  interior_annotation("a", c(0.92, 0.96))  
```

```{r}
#| label: model-prediction-points-line

predict_umap_scurve_scaled <- predict_umap_scurve_scaled |>
  mutate(type = "predict_from_umap") 

jittered_points_df <- jittered_points_df |>
  mutate(type = "predict_from_model") 

predict_point_df <- bind_rows(
  predict_umap_scurve_scaled,
  jittered_points_df
)

plot_predict_umap_model_connect <- ggplot(data = predict_point_df,
                            aes(
                              x = UMAP1,
                              y = UMAP2, 
                              group = ID
                            )) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.3) +
  interior_annotation("d", c(0.92, 0.96))
```

```{r}
#| echo: false
#| label: fig-predict-scurve
#| fig-pos: H
#| fig-width: 9
#| fig-height: 5
#| fig-cap: "Comparison of prediction generated using the exiting `umap` prediction method and our method: (a) A view of the true model in projections from $7\\text{-}D$, (b) predicted data from the `umap` prediction method, (c) predicted data from our method, and (d) comparison of predictions from both methods. In plot (d), points representing the same data but predicted by different methods are connected by lines. Need to add concluded sentence after changing the data."

proj_plot_scurve_true + plot_predict_umap + 
  plot_predict_umap_model + plot_predict_umap_model_connect +
  plot_layout(ncol = 4)
```

<!-- ::: {#fig-scurve-pred-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/scurve/sc_true_only.png){width="150" fig-align="center"} -->

<!-- ![](figures/scurve/pred_true_view.png){width="150" fig-align="center"} -->


<!-- A view of the true model in projections from $7\text{-}D$, and predictions of the true model in \gD{}, for the S-curve data. The predictions fits the UMAP layout which means that it capture the geometry of S-curve with UMAP. -->
<!-- ::: -->

<!-- XXX Prediction should have a jitter option, to spread the predicted points out fully in the hexagons. -->

### Tuning
<!-- removal of low density bins, removing long edges, choice of bins-->

The model fitting can be adjusted using these parameters: 

- hexagon bin parameters
    - bottom left bin position $(s_1, \ s_2)$, 
    - the total number of bins ($b$), 
- bin density cutoff, to remove low-density hexagons, and 
- edge length maximum, remove long edges from \gD{} representation. 

Default values are provided for each of these, but it is expected that the user will examine the MSE for a range of choices. Choosing these parameters according to MSE can be automated but it is recommended that the user examine the resulting model representation by overlaying it on the data in \pD{}. The next few subsections describe the calculation of default values, and the effect that different choices have on the model fit.

#### Hexagon bin parameters

The values $(s_1, \ s_2)$ define the position of the centroid of the bottom left hexagon. By default, this is at $s_1 = -q, s_2 = -qr_2$, where $q$ is the buffer sound the data. The choice of these values can have some effect on the distribution of bin counts. @fig-param-scurve (a) illustrates this. The distribution of bin counts for $s_1$ varying between $-0.1-0.0$ is shown. Generally, a more uniform distribution among these possibilities would indicate that the bins are reliably capturing the underlying distribution of observations. 

<!--The starting position of the hexagonal grid is important because different starting points result in different distributions of data across bins, even with the same total number of bins. This variation affects the model due to the differing number of non-empty bins. Therefore, it is necessary to evaluate various starting points with different total numbers of bins to determine which configurations are more effective at capturing the structure and fitting the model.--> 

<!--add three choice of bins 7, 11, 14-->

```{r}
#| label: hexbin-regular-scurve1
## hexagon binning to have regular hexagons
hb_obj_scurve1 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 7, 
  r2 = r2)

a1_1 <- calc_bins_y(
  bin1 = 7, 
  r2 = r2
)$a1

## Data set with all centroids
all_centroids_df1 <- hb_obj_scurve1$centroids

## Generate all coordinates of hexagons
hex_grid1 <- hb_obj_scurve1$hex_poly

## To obtain the standardise counts within hexbins
counts_df1 <- hb_obj_scurve1$std_cts
df_bin_centroids1 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df1, 
  counts_df = counts_df1) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 7")

hex_grid_with_counts_s_curve1 <- full_join(
  hex_grid1, 
  df_bin_centroids1 |> select(hexID, std_counts), 
    by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve1 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve1, 
    aes(x = x, y = y,
      group = hex_poly_id, 
      fill = std_counts), color = "grey70", linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
             aes(x = UMAP1, y = UMAP2),
             alpha = 0.3,
             size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  interior_annotation("a", position = c(0.92, 0.93)) +
  xlim(c(-0.25, 1.35)) + ylim(c(-0.35, 2))

hex_grid_coloured_scurve1_dens <- ggplot(hex_grid_with_counts_s_curve1) +
  geom_histogram(aes(x=std_counts), 
                 breaks=seq(0, 1, 0.05),
                 fill="slategray4", colour="white")

#hex_grid_coloured_scurve1_dens <- 
#  hex_grid_with_counts_s_curve1 |>
#  filter(!is.na(std_counts)) |>
#  mutate(ord_std_counts = n() - rank(std_counts)) |>
#  ggplot() +
#  geom_line(aes(x=ord_std_counts, y=std_counts), 
#                 colour="slategray4")
```

```{r}
#| label: hexbin-regular-scurve2
## hexagon binning to have regular hexagons
hb_obj_scurve2 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 11, 
  r2 = r2)

a1_2 <- calc_bins_y(
  bin1 = 11, 
  r2 = r2
  )$a1

## Data set with all centroids
all_centroids_df2 <- hb_obj_scurve2$centroids

## Generate all coordinates of hexagons
hex_grid2 <- hb_obj_scurve2$hex_poly

## To obtain the standardise counts within hexbins
counts_df2 <- hb_obj_scurve2$std_cts
df_bin_centroids2 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df2, 
  counts_df = counts_df2) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 11")

hex_grid_with_counts_s_curve2 <- full_join(
  hex_grid2, 
  df_bin_centroids2 |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve2 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve2, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", 
        linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  xlim(c(-0.25, 1.35)) + ylim(c(-0.35, 2)) +
  interior_annotation("b")

hex_grid_coloured_scurve2_dens <- ggplot(hex_grid_with_counts_s_curve2) +
  geom_histogram(aes(x=std_counts), 
                 breaks=seq(0, 1, 0.05),
                 fill="slategray4", colour="white")

#hex_grid_coloured_scurve2_dens <- 
#  hex_grid_with_counts_s_curve2 |>
#  filter(!is.na(std_counts)) |>
#  mutate(ord_std_counts = n() - rank(std_counts)) |>
#  ggplot() +
#  geom_line(aes(x=ord_std_counts, y=std_counts), 
#                 colour="slategray4")

```

```{r}
#| label: hexbin-regular-scurve3
## hexagon binning to have regular hexagons
hb_obj_scurve3 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 13, 
  r2 = r2)

a1_3 <- calc_bins_y(
  bin1 = 13, 
  r2 = r2
)$a1

## Data set with all centroids
all_centroids_df3 <- hb_obj_scurve3$centroids

## Generate all coordinates of hexagons
hex_grid3 <- hb_obj_scurve3$hex_poly

## To obtain the standardise counts within hexbins
counts_df3 <- hb_obj_scurve3$std_cts
df_bin_centroids3 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df3, 
  counts_df = counts_df3) |>
  filter(drop_empty == FALSE) |>
  mutate(b1 = "b1 = 14")

hex_grid_with_counts_s_curve3 <- full_join(hex_grid3, 
                                          df_bin_centroids3 |> select(hexID, std_counts), 
                                          by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve3 <-  ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve3, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  xlim(c(-0.25, 1.35)) + ylim(c(-0.35, 2)) +
  interior_annotation("c")
 
hex_grid_coloured_scurve3_dens <- ggplot(hex_grid_with_counts_s_curve3) +
  geom_histogram(aes(x=std_counts), 
                 breaks=seq(0, 1, 0.05),
                 fill="slategray4", colour="white") 
#hex_grid_coloured_scurve3_dens <- 
#  hex_grid_with_counts_s_curve3 |>
#  filter(!is.na(std_counts)) |>
#  mutate(ord_std_counts = n() - rank(std_counts)) |>
#  ggplot() +
#  geom_line(aes(x=ord_std_counts, y=std_counts), 
#                 colour="slategray4")
```

```{r}
#| echo: false
#| label: fig-bins-scurve
#| fig-pos: H
#| fig-cap: "Hexbin density plots of UMAP layout of the S-curve data, using three different bin inputs: (a) $b = 91 \\text{ } (7, \\text{ }13)$, (b) $b = 220 \\text{ } (11, \\text{ }20)$, and (c) $b = 312 \\text{ } (13, \\text{ }24)$. Color indicates standardized counts, dark indicating high count and light indicates low count. At the smallest bin size the data segregates into two separate groups, suggesting this is too many bins. Using the MSE of the model fit in $p\\text{-}D$ helps decide on a useful choice of number of bins."
#| fig-width: 6
#| fig-height: 4

hex_grid_coloured_scurve1 + 
  hex_grid_coloured_scurve2 + 
  hex_grid_coloured_scurve3 +
hex_grid_coloured_scurve1_dens +
  hex_grid_coloured_scurve2_dens +
  hex_grid_coloured_scurve3_dens +
  plot_layout(guides='collect', ncol = 3,
              heights = c(2,1)) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
``` 

The default number of bins $b=b_1\times b_2$ is computed based on the sample size, by setting $b_1=n^{1/3}$, consistent with the Diaconis-Freedman rule [@freedman1981]. The value of $b_2$ is determined analytically by $b_1, q, r_2$. Values of $b_1$ between $2$ and $b_1 = \sqrt{\frac{n}{2}}$ are allowed. @fig-param-scurve (b) shows the effect of different choices of $b_1$ on the MSE of the fitted model. 

<!-- To determine the effective $b$, candidate values are selected based on the range between the minimum and approximate maximum $b_1$, because $b_2$ is computed from $b_1$. The minimum $b_1$ is set to $2$, while the maximum number is estimated by taking the square root of $\frac{n}{2}$. By evaluating MSE across varying $b$ within this range for different $q$, helps to determine an appropriate values for $b$ and $q$ (@fig-param-scurve (a)).--> 

<!--add MSE vs total number of error plot-->

<!--To generate errors for different total number of bins-->

```{r}
#| label: errors-scurve
## To initialize number of bins along the x-axis
bin1_vec_scurve <- 5:19 #sqrt(NROW(training_data_scurve)/2)
#buffer_vec <- seq(0.05, 0.1, by = 0.01)

error_scurve <- data.frame(matrix(nrow = 0, ncol = 0))

for (xbins in bin1_vec_scurve) {
  #for (buff in buffer_vec) {
  
  bin_obj <- calc_bins_y(bin1 = xbins, r2 = r2, q = 0.1)
    
  bin2 <- bin_obj$bin2
  a1 <- bin_obj$a1

  scurve_model <- fit_highd_model(
    training_data = training_data_scurve,
    emb_df = umap_scurve_scaled,
    bin1 = xbins,
    r2 = r2,
    q = 0.1, 
    is_bin_centroid = TRUE,
    is_rm_lwd_hex = FALSE,
    col_start_highd = "x"
  )

  df_bin_centroids_scurve <- scurve_model$df_bin_centroids
  df_bin_scurve <- scurve_model$df_bin

  ## Compute error
  error_df <- glance(
    df_bin_centroids = df_bin_centroids_scurve,
    df_bin = df_bin_scurve,
    training_data = training_data_scurve,
    newdata = NULL,
    type_NLDR = "UMAP",
    col_start = "x") |>
    mutate(bin1 = xbins,
           bin2 = bin2,
           b = bin1 * bin2,
           b_non_empty = NROW(df_bin_centroids_scurve),
           a1 = round(a1, 2))

  error_scurve <- bind_rows(error_scurve, error_df)
    
  #}

}

# mse_scurve_b <- ggplot(error_scurve, 
#                      aes(x = b, 
#                          y = log(MSE),
#                          color = buff,
#                          group = buff)) + 
#   geom_point(size = 1) +
#   geom_line(linewidth = 0.05) + 
#   geom_vline(xintercept = 220, linetype="solid", 
#              color = "black", linewidth=0.1) +
#   geom_vline(xintercept = 112, linetype=2, 
#              color = "black", linewidth=0.1) +
#   geom_vline(xintercept = 364, linetype=2, 
#              color = "black", linewidth=0.1) +
#   scale_colour_continuous_sequential(palette = "Magenta") +
#   labs(x = expression(b), y = "log(MSE)", color = expression(q)) +
#   ggtitle("(a)") +
#   theme(aspect.ratio = 0.75,
#         axis.text.x = element_text(size = 12),
#         axis.text.y = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12, 
#             angle=90),
#         plot.title = element_text(size = 12),
#         legend.title = element_text(size = 12),
#         legend.text = element_text(size = 10),
#         legend.key.height = unit(1, 'cm'),
#         legend.key.width = unit(1, 'cm'),
#         legend.key.size = unit(1, 'cm'))

# diaconis_bin_width <- calc_bins_y(
#     bin1 = ceiling(NROW(training_data_scurve)^(1/3)),
#     r2 = r2,
#     q = 0.1)$a1

## Find the minimum MSE when have duplicate a1
error_scurve <- error_scurve |>
  group_by(a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()

mse_scurve_b <- ggplot(error_scurve, 
                     aes(x = a1, 
                         y = MSE)) +
  geom_vline(xintercept = 0.10, linetype=2,
           color = "#bdbdbd", linewidth=1) +
  geom_vline(xintercept = 0.12,
             linetype="solid",
             color = "#bdbdbd",
             linewidth=1) +
  geom_vline(xintercept = 0.18, linetype=2,
             color = "#bdbdbd", linewidth=1) +
  geom_line(linewidth = 0.5) + 
  geom_point(size = 1) +
  scale_x_continuous(breaks = sort(unique(round(error_scurve$a1, 2)))[c(1, 3, 5, 6, 8, 10:12)]) +
  labs(x = expression(a[1]), y = "MSE") +
  ggtitle("(a)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())


a1_m_scurve <- ggplot(error_scurve,
                     aes(x = a1,
                         y = b_non_empty)) +
  geom_vline(xintercept = 0.12,
         linetype=2,
         color = "#bdbdbd",
         linewidth=1) +
  geom_hline(yintercept = 52,
           linetype=2,
           color = "#bdbdbd",
           linewidth=1) +
  geom_point(size = 1) +
  geom_line(linewidth = 0.05) +
  # geom_vline(xintercept = 11, linetype="solid",
  #            color = "#bdbdbd", linewidth=1) +
  # geom_vline(xintercept = 9, linetype=2,
  #            color = "#bdbdbd", linewidth=1) +
  # geom_vline(xintercept = 13, linetype=2,
  #            color = "#bdbdbd", linewidth=1) +
  scale_y_continuous(breaks = sort(unique(error_scurve$b_non_empty))[-3]) +
  scale_x_continuous(breaks = sort(unique(round(error_scurve$a1, 2)))[c(1, 3, 5, 6, 8, 10:12)]) +
  labs(x = expression(a[1]), y = expression(m)) +
  ggtitle("(b)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

# 
# b1_m_scurve2 <- ggplot(error_scurve, 
#                      aes(x = bin1, 
#                          y = b_non_empty)) + 
#   geom_point(size = 1) +
#   geom_line(linewidth = 0.05) + 
#   # geom_vline(xintercept = 11, linetype="solid",
#   #            color = "#bdbdbd", linewidth=1) +
#   # geom_vline(xintercept = 9, linetype=2,
#   #            color = "#bdbdbd", linewidth=1) +
#   # geom_vline(xintercept = 13, linetype=2,
#   #            color = "#bdbdbd", linewidth=1) +
#   scale_x_continuous(breaks = 5:19) +
#   labs(x = expression(b[1]), y = expression(m)) +
#   ggtitle("(c)") +
#   theme(aspect.ratio = 0.75,
#         axis.text.x = element_text(size = 12),
#         axis.text.y = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12, 
#             angle=90),
#         axis.ticks.x = element_line(linewidth = 0.5),
#         axis.ticks.y = element_line(linewidth = 0.5),
#         plot.title = element_text(size = 12),
#         legend.title = element_text(size = 12),
#         legend.text = element_text(size = 10),
#         legend.key.height = unit(1, 'cm'),
#         legend.key.width = unit(1, 'cm'),
#         legend.key.size = unit(1, 'cm'))
```

#### Removal of low density bins

By default, when assessing the choice of $b_1$, the total number of bins is measured by the number of **non-empty** bins. This more accurately reflects the hexagon grid relative the MSE than the full number of bins in the grid. It may also be beneficial to remove low count bins also, in the situation where data is clustered or stringy, where the observed data is sparse. In order to decide if this is necessary, you would examine the distribution of bin counts, or the density which puts the counts on a standard scale. If there is something of a gap at low values, this would suggest a potential value to use as a cutoff. Alternatively, one could choose to remove based on a percentile, the bins with density in the lowest 5% of all bins, for example. @fig-param-scurve (c) illustrates the effect on the model representation of removing bins below different percentages. Generally, we would urge caution in removing low count bins. 

<!-- Once setting up the hexagon grid with an appropriate number of bins, some hexagon bins may have few or no data points within them (@fig-bins-scurve (b)). To ensure comprehensive coverage of the NLDR data, it is necessary to select hexagon bins with a considerable number of data points. This involves calculating the number of points within each hexagon. Then, the standard count is computed by dividing the number of points within each hexagon by the maximum number of points in the grid. Next, bins with a standard count less than a benchmark value are removed (@fig-param-scurve (c)). There is no specific rule for selecting a benchmark value. However, the following steps can help determine a suitable value for removing low-density hexagons:

1. Plot the distribution of the standardized counts (@fig-param-scurve (b)).
2. Examine the distribution of counts.
3. Select the first quantile value if the distribution is skewed.
-->

<!--add distribution of density and add the first quantile as the benchmark-->

```{r}
#| label: distribution-scurve
## To initialize effective bins along x
effective_bin1_scurve <- 11

effective_bin2_scurve <- calc_bins_y(
  bin1 = effective_bin1_scurve, 
  r2 = r2)$bin2

scurve_model <- fit_highd_model(
  training_data = training_data_scurve,
  emb_df = umap_scurve_scaled,
  bin1 = effective_bin1_scurve,
  r2 = r2,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_scurve <- scurve_model$df_bin_centroids
df_bin_scurve <- scurve_model$df_bin

## To obtain the first quntile
benchmark1 <- round(quantile(df_bin_centroids_scurve$std_counts, names = FALSE)[2], 3)
```

```{r}
#| label: bin-counts-scurve
#| eval: false

df_bin_centroids_all <- bind_rows(
  df_bin_centroids1,
  df_bin_centroids2,
  df_bin_centroids3
)

# cell_count_scurve <- ggplot(df_bin_centroids_all, 
#                             aes(x = reorder(as.factor(hexID), 
#                                             -bin_counts), 
#                                 y = bin_counts,
#                                 group = as.factor(b1),
#                                 color = as.factor(b1))) + 
#   geom_line() + 
#   # geom_hline(yintercept = benchmark1, color = "black", linewidth=0.5, alpha = 0.5) +
#   scale_colour_discrete_qualitative(palette = "Dark 3") +
#   xlab("hexagon id (re-ordered)") + 
#   ylab("bin count") +
#   ggtitle("(a)") +
#   theme(aspect.ratio = 0.75,
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12, angle = 90),
#         axis.text.x = element_blank(),
#         axis.text.y = element_text(size = 12),
#         plot.title = element_text(size = 12),
#         legend.text = element_text(size = 10),
#         legend.key.size = unit(1, 'cm'),
#         legend.key.height = unit(1, 'cm'))

cell_count_scurve <- ggplot(df_bin_centroids_all, 
                            aes(x = as.factor(hexID), 
                                y = std_counts,
                                group = as.factor(b1),
                                color = as.factor(b1))) + 
  geom_line() + 
  # geom_hline(yintercept = benchmark1, color = "black", linewidth=0.5, alpha = 0.5) +
  scale_colour_discrete_qualitative(palette = "Dark 3") +
  xlab("hexagon id (re-ordered)") + 
  ylab("standardised bin count") +
  ggtitle("(a)") +
  theme(aspect.ratio = 0.75,
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12, angle = 90),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 12),
        plot.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.key.size = unit(1, 'cm'),
        legend.key.height = unit(1, 'cm'))
```

The benchmark value for removing low-density hexagons ranges between $0$ and $1$. When analyzing how these benchmark values influence model performance, it's essential to observe the change in MSE as the benchmark value increases (@fig-param-scurve). The MSE shows a gradual decrease as the benchmark value goes from $1$ to $0$. Evaluating this rate of increase is important. If the increment is not considerable, the decision might lean towards retaining low-density hexagons.

<!--add MSE vs density (0 to 1)-->

<!--To generate errors for different total number of bins-->
```{r}
#| label: bins-scurve

## To initialize benchmark values to remove low density hexagons
benchmark_rm_hex_vec <- append(seq(0, 0.99, by=0.1), c(benchmark1, 0.99))

error_rm_scurve <- data.frame(matrix(nrow = 0, ncol = 0))

for (benchmark_rm_lwd in benchmark_rm_hex_vec) {
  
  df_bin_centroids_scurve_high_dens <- df_bin_centroids_scurve |>
    filter(std_counts > benchmark_rm_lwd)
  
  df_bin_scurve_high_dens <- df_bin_scurve |>
    filter(hb_id %in% df_bin_centroids_scurve_high_dens$hexID)

  ## Compute error
  error_df <- glance(
    df_bin_centroids = df_bin_centroids_scurve_high_dens,
    df_bin = df_bin_scurve_high_dens,
    training_data = training_data_scurve,
    newdata = NULL,
    type_NLDR = "UMAP",
    col_start = "x") |>
    mutate(benchmark_rm_lwd = round(benchmark_rm_lwd, 2),
           bin1 = effective_bin1_scurve,
           bin2 = effective_bin2_scurve,
           b = bin1 * bin2,
           b_non_empty = NROW(df_bin_centroids_scurve_high_dens),
           mean_counts = sum(df_bin_centroids_scurve_high_dens$bin_counts)/NROW(df_bin_centroids_scurve_high_dens))

  error_rm_scurve <- bind_rows(error_rm_scurve, error_df)

}

benchmark_label_df <- tibble(x = 0.2, y = 0.1, 
                          label = paste0("benchmark is ", benchmark1))

error_rm_scurve <- error_rm_scurve |>
  mutate(mean_counts = round(mean_counts, 0)) |>
  filter(b_non_empty >= 5)

mse_scurve_lwd <- ggplot(error_rm_scurve, 
                     aes(x = benchmark_rm_lwd, 
                         y = MSE)) + 
  geom_point(
    size = 1
    ) +
  geom_line(
    linewidth = 0.3
    ) + 
  # geom_vline(xintercept = benchmark1, linetype="solid", 
  #            color = "#bdbdbd", linewidth=1, alpha = 0.5) +
  # scale_x_continuous("standardized bin count", 
  #        transform = "reverse") + 
  scale_x_continuous(breaks = unique(error_rm_scurve$benchmark_rm_lwd)) +
  xlab("threshold to remove low-density hexagons") +
  ylab("MSE") +
  ggtitle("(c)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

# mse_scurve_lwd2 <- error_rm_scurve |>
#   filter(b_non_empty >= 5) |>
#   ggplot(aes(x = b_non_empty,y = MSE)) + 
#   geom_point(
#     size = 1
#     ) +
#   geom_line(
#     linewidth = 0.3
#     ) + 
#   # geom_vline(xintercept = benchmark1, linetype="solid", 
#   #            color = "#bdbdbd", linewidth=1, alpha = 0.5) +
#   scale_x_continuous("m (b1 = 14)") + 
#   ylab("MSE") +
#   ggtitle("(e)") +
#   theme(aspect.ratio = 0.75,
#         axis.text.x = element_text(size = 12),
#         axis.text.y = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12, 
#             angle=90),
#         plot.title = element_text(size = 12))
```

<!-- Furthermore, selecting the benchmark value for removing low-density hexagons is important. Removing unnecessary bins may lead to the formation of long edges and an uneven \gD{} model. Hence, rather than solely relying on the benchmark value to identify hexagons for removal, it's essential to consider the standard number of points in the neighboring hexagons of the identified low-density bins (see @fig-lwd-scurve (b)). If neighboring bins also show low counts, only those bins will be removed. The remaining bins are used to construct the \gD{} model.    -->


<!--Observing neighboring bins as well-->
```{r}
#| label: bin-neighbours
## First define low density bins using first quantile
df_bin_centroids_low <- df_bin_centroids_scurve |> 
  filter(std_counts <= benchmark1)

## Check neighboring bins
remove_id <- find_low_dens_hex(df_bin_centroids_all = df_bin_centroids_scurve, 
                               bin1 = effective_bin1_scurve, 
                               df_bin_centroids_low = df_bin_centroids_low)

## Remove the identified bins
df_bin_centroids_scurve_removed <- df_bin_centroids_scurve |>
  filter(hexID %in%remove_id)

df_bin_centroids_scurve_keep  <- df_bin_centroids_scurve |>
  filter(!(hexID %in%remove_id))

```

#### Removing long edges

Edges define the neighbourhood structure, in order to provide a smooth \gD{} representation of the fitted model. @fig-scurve-true-sc shows a wire frame of the true model that was used to generate the S-curve example data. The ideal is that the representation of the fitted model, at least for this example where we know the true model, should look similar to this. 

The Delaunay triangulation will ensure that all centroids are connected into a triangular mesh. For some structures, like clustered data, or highly non-linear shapes, breaks in the mesh are meaningful. When separated clusters are present the mesh should be broken across the gaps. For non-linear structures like the S-curve, the mesh should run unbroken along the S, but there should be no edges connecting the top of the S directly to the bottom of the S. For these reasons it is necessary to remove edges from the mesh in some applications.

The decision on edge length removal is made based on the distribution of edge lengths. In particular, a gap between values, where there a concentration of small values and then a few larger values, likely suggests a cutoff for edge removal. Because the triangulation is typically done on the hexagon centroids, there are particular discrete edge lengths, based on bin widths. @fig-NLDR-scurve (d) illustrates edge length distributions. 

There is an additional step that is needed. When the model is lifted into \pD{}, if the fit is good all the edges should be relatively small in this space, too. If this is not the case, then there are several possible actions: (1) re-do the NLDR to get a more representative layout; (2) identify the edge and remove it from the model, in \gD{} and \pD{}; (3) consider different values for the model fit, number of bins, initial bin position or removing low density bins.


<!-- (@fig-NLDR-scurve (d)), it is necessary to remove edges that connect distant bin centroids in the triangular mesh. These edges only exist in the \gD{} model and do not extend into \pD{}, so their removal does not impact the model in \pD{}. Although there are no specific criteria for determining the benchmark value to remove long edges, the following steps provide an approach to identifying a suitable threshold:

1. Plot the distribution of the \gD{} Euclidean distances (@fig-param-scurve (d)).
2. Identify the first largest difference between consecutive distance values.
3. Take the distance value corresponding to this difference as the benchmark value.-->

<!--distribution of distance along with the default benchmark-->
```{r}
#| label: triangulate-scurve2

## Triangulate bin centroids
tr1_object_scurve <- tri_bin_centroids(
  df_bin_centroids_scurve_keep, x = "c_x", y = "c_y")
tr_from_to_df_scurve <- gen_edges(
  tri_object = tr1_object_scurve)

## Compute 2D distances
distance_scurve <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_scurve, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_scurve <- find_lg_benchmark(
  distance_edges = distance_scurve, 
  distance_col = "distance")

benchmark_label_df <- tibble(y = benchmark_scurve - 0.03,
                             x = 1.3,
                             label = paste0("benchmark is ", benchmark_scurve))

## To draw the distance distribution
distance_scurve$group <- "1"
distance_scurve_plot <- ggplot(distance_scurve, 
                        aes(x = group, 
                            y = distance)) + 
  geom_quasirandom(
    size = 2, 
    alpha = 0.3
  ) + 
  ylim(0, max(unlist(distance_scurve$distance))+ 0.5) + 
  coord_flip() + 
  geom_hline(yintercept = benchmark_scurve,
     linetype="solid", 
     color = "black", linewidth=0.5, alpha = 0.5) +
  ylab(expression(d^{(2)})) +
  ggtitle("(d)") +
  theme(aspect.ratio = 0.75,
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        plot.title = element_text(size = 12))

# Define bin width and starting point
bin_width <- hb_obj_scurve2$centroids$c_x[2]-hb_obj_scurve2$centroids$c_x[1]
start_point <- bin_width/2

# Create bins and calculate the mean value for each bin range
bins <- seq(start_point, max(distance_scurve$distance) + bin_width, 
            by = bin_width)
bin_labels <- bins[-length(bins)] + bin_width / 2  # mean of each bin range

distance_scurve <- distance_scurve |> 
  mutate(bin = cut(distance, breaks = bins, include.lowest = TRUE, labels = bin_labels))

distance_hist <- ggplot(
  distance_scurve, aes(x = distance)) +
    geom_histogram(
      aes(y = after_stat(count) / sum(after_stat(count))), 
      binwidth = bin_width) +
  scale_x_continuous(breaks = bin_labels, labels = round(bin_labels, 2)) +
  ylab("standardized number of edges") +
  xlab(expression(d^{(2)})) +
  ggtitle("(d)") +
  theme_minimal() +
  theme(aspect.ratio = 0.75,
        panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        panel.grid.major.x = element_blank(),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line())

distance_scurve_summary <- distance_scurve |>
  group_by(bin) |>
  summarise(count = n()) |>
  mutate(std_counts = count/sum(count))

```

```{r}
#| echo: false
#| fig-cap: "Various plots to help assess best hexagon bin parameters, thresholds to remove low density bins and large edges. Both (b) and (c) show MSE, against number of bins along the x-axis and standardised count. A good benchmark value for these parameters is when the MSE drops and then flattens out. Plot (a) shows the distribution of stadardised counts of hexagons. Plot (d) shows the distribution of $2\\text{-}D$ Euclidean distances between bin centroids, with a good benchmark value for removing large edges would being the distance that shows the first large decrease."
#| label: fig-param-scurve
#| out-width: 100%
#| fig-width: 12
#| fig-height: 10
#| fig-pos: H

mse_scurve_b + a1_m_scurve +
  mse_scurve_lwd + distance_hist + 
  plot_layout(ncol=2)
```

## Best fit

Deciding on the best fit relies on several elements: 

- the choice of NLDR method, and the parameters used to create it, and
- model fit parameters: bin size, low density bin removal, long edge removal.

Comparing the MSE to obtain the best fit is suitable if one starts from the same NLDR representation. In theory, because the MSE is computed on \pD{} measuring the fit between model and data it might still be useful to compare different NLDR representations. A good NLDR representation should produce a good fit, producing a low MSE if the model fits the data well. However, it technically might be quite variable.

<!-- @fig-scurve-sc-best shows one of the best fits for S-curve data. TriMAP with $15$ nearest neighbors for forming the nearest neighbor triplet (`n_inliers`), $3$ outliers for forming the nearest neighbor triplets (`n_outliers`), $3$ random triplets per point (`n_random`) is used as the NLDR method. The model is generated with $103$ bins, which have a width of $0.154$ and a height of $0.133$. Edges with lengths greater than $0.266$ are removed.  -->

<!-- langevitour with pD model for S-curve -->
::: {#fig-scurve-sc-best layout-ncol="4" fig-pos="H"}
![](figures/scurve/tsne_trimesh_layout.png)

![](figures/scurve/sc_tsne_best_1.png)

![](figures/scurve/sc_tsne_best_2.png)

![](figures/scurve/sc_tsne_best_3.png)

Model in \gD{}, on the tSNE layout, and three views of the fit in projections from $7\text{-}D$, for the S-curve data ($(s_1, \ s_2) = (-0.170, \ -0.137)$, $b = 256 \  (16, \ 16)$, $m = 70$, benchmark value to remove low density hexagons is $0.250$, and benchmark value to remove large edges is $0.180$). MSE is $0.0432$. The model closely fits the shape, but it has some twists (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/ZuLdp89qJ6g>).
:::


<!-- The appropriate model for the S-curve using UMAP (n_neighbors: $15$) was created with $420$ bins. It was determined to be the best model after removing edges with a length greater than $0.134$. The decision was made considering that the model accurately captures the geometry of the S-curve. But better in some places only (@fig-scurve_sc_best). -->

<!-- In contrast, a poor model for the S-curve with PHATE was created using only $190$ bins and removing edges with a length greater than $0.3$. This model is inadequate as it has a higher MSE compared to the appropriate model. Visually, the model squeezed to the middle of the geometry of the S-curve (@fig-scurve-sc-bad). -->

<!-- XXX BEST FIT FOR S-CURVE MIGHT BE A DIFFERENT NLDR.  -->
<!-- SHOW THE SEVERAL SUB-OPTIMAL FITS. -->

<!--langevitour with best pD model for S-curve--> 
<!-- ::: {#fig-scurve-sc-best layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/scurve/sc_best_1.png) -->

<!-- ![](figures/scurve/sc_best_2.png) -->

<!-- ![](figures/scurve/sc_best_3.png) -->

<!-- Screen shots of the **langevitour** of the S-curve, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/4RFNENX_fXk>). MSE with $0.035$. -->
<!-- ::: -->

<!--langevitour with bad pD model for S-curve--> 
<!-- ::: {#fig-scurve-sc-bad layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/scurve/sc_bad_1.png) -->

<!-- ![](figures/scurve/sc_bad_2.png) -->

<!-- ![](figures/scurve/sc_bad_3.png) -->

<!-- Screen shots of the **langevitour** of the S-curve, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/DHPvhkZ0lUM>). MSE with $0.090$. -->
<!-- ::: -->



<!------>
<!--add more details regarding to the how the change happening with different number of bins-->

<!--best model-->
<!--need to mention why it is the best-->

<!--bad model-->
<!--need to mention why it is the bad-->
<!--Not represented the whole data structure because there are some discontinuity in the S-curve-->
<!-- Why? bad 

<!-- add MSE values for the screenshot figure names-->

## Linked plots

Its important to access the \gD{} layout and the generated model overlaid on data in \pD{} together to understand whether it fits the points everywhere, fits better in some places, or simply mismatches the pattern. Interactivity also helps in understanding the quirks that occur with different NLDR techniques (XXXXRefer the video after finalising the S-curve).

## A curious difference between tSNE, UMAP and PaCMAP revealer

<!-- XXX THIS IS NOT INTERESTING IN ITS CURRENT FORM. THIS IS ONLY ANY INTERESTING EXAMPLE WHEN THE FILLED OUT VS FLAT SHAPES ARE DISCUSSED. -->

In this section, we will assess how t-SNE, UMAP, and PaCMAP algorithms preserve the clustering structure. We will use a simulated dataset consisting of five spherical Gaussian clusters where each cluster occupies a different corner of the $4\text{-}D$ space, with each cluster containing an equal number of points ($1000$) and the same within-cluster variation. The data simulated from $\Sigma = \begin{pmatrix}
0.0025 & 0 & 0 & 0 \\
0 & 0.0025 & 0 & 0 \\
0 & 0 & 0.0025 & 0 \\
0 & 0 & 0 & 0.0025 \\
\end{pmatrix}$.

In the t-SNE 2D layout, the distances between clusters appear smaller, leading to more tightly packed clusters, which is typical of t-SNE's focus on preserving local structures. This often results in less emphasis on the global distances between clusters. On the other hand, UMAP tends to maintain larger separations between clusters, better reflecting the global structure and preserving the relative distances between clusters more effectively. PaCMAP, meanwhile, balances these aspects by maintaining the distance between clusters and positioning them correctly, effectively capturing both global and local structures.

The t-SNE, UMAP, and PaCMAP layouts each reveal different aspects of the clustering structure within the dataset. When examining a specific cluster, which appears more like a filled-out pancake in higher dimensions, we observe that t-SNE, UMAP, and PaCMAP all effectively capture global structures. However, there is evidence suggesting that PaCMAP, with its default hyper-parameter settings, struggles to preserve the local structure as effectively as t-SNE and UMAP.

The differences in performance can be attributed to several factors. One key element is the choice of hyper-parameters, particularly the number of neighbors considered during the dimensionality reduction process. This parameter significantly impacts how well local structures are maintained. Additionally, the intrinsic dimensionality of the data, the balance between global and local structures, and issues related to optimization and initialization all contribute to the observed outcomes. 

It's important to note that these observations are made with specific hyper-parameter settings. Adjusting these parameters could lead to different results, potentially improving PaCMAP's ability to capture local structures or altering how t-SNE and UMAP handle global and local structures. Thus, while PaCMAP generally balances global and local preservation, careful tuning of its parameters is crucial for achieving the best results in any given dataset.



<!-- In this section, the effectiveness of the algorithm is described using a simulated dataset. The dataset consists of five spherical Gaussian clusters in $4\text{-}D$, with each cluster containing an equal number of points and the same within-cluster variation. -->

<!-- XXX Add tSNE, UMAP, and PaCMAP NLDR layouts and describe what is happening -->
<!-- XXX Discuss the models of one specific cluster (filled out, pancakes): tSNE, UMAP, PaCMAP are useful to capture global structures, but this is an evidence where PaCMAP fail to capture the local structure with default hyper-parameter settings than tSNE and UMAP -->
<!-- XXX Why? hyper-parameter choice specially number of neighbors, data characteristics (intrinsic dimensionality), imbalance between global and local structures, optimization and initialization issues -->
<!-- XXX Add this is with specific hyper-parameter choices -->

<!-- The \gD{} layouts generated by tSNE, UMAP, and PaCMAP show five well-separated clusters which evident these methods effectively preserve the global structure. In tSNE (@fig-gau-tsne-sc (a)), these clusters appear closely. UMAP arranges all clusters in a parallel manner, with three aligned in one line and the other two in a separate line (@fig-gau-umap-sc (a)). In contrast, PaCMAP shows one central cluster and the remaining four spread out in different directions (@fig-gau-pacmap-sc (a)). -->

<!-- The tSNE and UMAP shows *filled out* clusters which provide evidence that these methods preserve the local structure (@fig-gau-tsne-sc (c) and @fig-gau-umap-sc (c)). On the other hand, PaCMAP shows *flat* shapes clusters in the model and evident that PaCMAP fail to capture the within-cluster variation (@fig-gau-pacmap-sc (c)).  -->

<!--Projections-->
```{r}
#| label: five-gau-proj-tsne-model

training_data_gau <- read_rds("data/five_gau_clusters/data_five_gau.rds")

data_gau <- training_data_gau |> 
  select(-ID) |>
  mutate(type = "data")

tsne_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau_71.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = tsne_data_gau)
tsne_gau_scaled <- gau1_scaled_obj$scaled_nldr

tsne_gau <- tsne_gau_scaled |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.3, color = "#000000")

## Compute hexbin parameters
num_bins_x_gau1 <- 13
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = tsne_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x",
  q = 0.1
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

# tr_from_to_df_gau1 <- tr_from_to_df_gau1 |>
#   filter(row_number() != 76) |>
#   filter(row_number() != 34)

tr_from_to_df_gau1 <- tr_from_to_df_gau1 |>
  filter(row_number() != 149)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

benchmark_gau1 <- 0.1

trimesh_removed_gau1 <- vis_rmlg_mesh(
  distance_edges = distance_gau1,
  benchmark_value = benchmark_gau1,
  tr_coord_df = tr_from_to_df_gau1,
  distance_col = "distance")


## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = tsne_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  q = 0.1)

tsne_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                  tsne_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

distance_df_small_edges <- distance_df_small_edges |>
  filter(row_number() != 108)

## First projection
# projection <- cbind(
#     c(0.46477,-0.02024,0.56378,0.39736),
#     c(0.15607,-0.54876,-0.44090,0.41505))
projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558,-0.23854,-0.63659,0.35753))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.2,
  axis_pos_x = 0.15,
  axis_pos_y = -0.85)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_tsne_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.3) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(0.1, 0.38)) +
  ylim(c(-0.9, -0.5)) +
  interior_annotation("a1", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
# projection <- cbind(
#     c(-0.71895,0.29250,-0.29905,-0.01656),
#     c(-0.16698,-0.62707,-0.23756,0.46328))
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.3,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_tsne_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.35)) +
  ylim(c(-0.75, -0.4)) +
  interior_annotation("a2", 
                      position = c(0.08, 0.9),
                      cex = 2)

# five_gau_proj_tsne_model <- gen_proj_langevitour(
#   points_df = df_exe,
#   projection = projection,
#   edge_df = distance_df_small_edges |> select(-distance)
# ) +
#   xlim(c(-0.18, 0.1)) +
#   ylim(c(-0.78, -0.55))
```

```{r}
#| label: five-gau-proj-umap-model

umap_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = umap_data_gau)
umap_gau_scaled <- gau1_scaled_obj$scaled_nldr

umap_gau <- umap_gau_scaled |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.3, color = "#000000")

## Compute hexbin parameters
num_bins_x_gau1 <- 45
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = umap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = umap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1)

umap_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                umap_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## First projection
# projection <- cbind(
#   c(0.02743,0.21427,0.62893,-0.49816),
#   c(-0.70709,0.42156,-0.02593,0.10965))
projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558,-0.23854,-0.63659,0.35753))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.2,
  axis_pos_x = -0.55,
  axis_pos_y = -0.05)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.3) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.6, -0.3)) +
  ylim(c(0, -0.38)) +
  interior_annotation("b1", 
                      position = c(0.08, 0.9),
                      cex = 2)

## Second projection
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.3,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.35)) +
  ylim(c(-0.75, -0.4)) +
  interior_annotation("b2", 
                      position = c(0.08, 0.9),
                      cex = 2)

# five_gau_proj_umap_model <- gen_proj_langevitour(
#   points_df = df_exe,
#   projection = projection,
#   edge_df = distance_df_small_edges |> select(-distance)
# ) +
#   xlim(c(-0.18, 0.1)) +
#   ylim(c(-0.78, -0.55))
```

```{r}
#| label: five-gau-proj-pacmap-model

pacmap_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")
gau1_scaled_obj <- gen_scaled_data(
  data = pacmap_data_gau)
pacmap_gau_scaled <- gau1_scaled_obj$scaled_nldr

pacmap_gau <- pacmap_gau_scaled |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2)) +
  geom_point(alpha=0.3, color = "#000000")

## Compute hexbin parameters
num_bins_x_gau1 <- 21
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = pacmap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x",
  q = 0.1
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

trimesh_removed_gau1 <- vis_rmlg_mesh(
  distance_edges = distance_gau1,
  benchmark_value = benchmark_gau1,
  tr_coord_df = tr_from_to_df_gau1,
  distance_col = "distance")

## Hexagonal binning to have regular hexagons
hb_obj_gau1 <- hex_binning(
  data = pacmap_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  q = 0.1)

pacmap_data_with_hb_id <- hb_obj_gau1$data_hb_id

df_all_gau1 <- dplyr::bind_cols(training_data_gau |> dplyr::select(-ID),
                                pacmap_data_with_hb_id)

### Define type column
df <- df_all_gau1 |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin_gau1 |>
  dplyr::filter(hb_id %in% df_bin_centroids_gau1$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b <- df_b[match(df_bin_centroids_gau1$hexID, df_b$hb_id),] |>
  dplyr::select(-hb_id)

# Apply the scaling
df_model_data <- bind_rows(data_gau, df_b)
scaled_gau <- scale_data_manual(df_model_data, "type") |>
  as_tibble()

scaled_gau_data <- scaled_gau |>
  filter(type == "data") |>
  select(-type)

scaled_gau_data_model <- scaled_gau |>
  filter(type == "model") |>
  select(-type)

## Set the maximum difference as the criteria
distance_df_small_edges <- distance_gau1 |>
  dplyr::filter(distance < benchmark_gau1)

## First projection
projection <- cbind(
  c(-0.00215,-0.68905,-0.04778,-0.54223),
  c(0.42558,-0.23854,-0.63659,0.35753))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.2,
  axis_pos_x = -0.37,
  axis_pos_y = 0.22)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_pacmap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.3) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.41, -0.15)) +
  ylim(c(0.18, 0.52)) +
  interior_annotation("c1", 
                      position = c(0.08, 0.9),
                      cex = 2)

### Second projection
# projection <- cbind(
#     c(-0.60995,0.33361,0.32920,-0.31518),
#     c(0.19525,-0.30554,0.74417,0.07600))
projection <- cbind(
  c(0.36030,0.00630,0.66545,-0.34307),
  c(-0.01861,0.14409,-0.36798,-0.73066))

projected <- as.matrix(scaled_gau_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_gau_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_gau_data,
  proj_data = projected_df,
  limits = 0.3,
  axis_pos_x = -0.7,
  axis_pos_y = -0.7)

axes <- axes_obj$axes
circle <- axes_obj$circle

five_gau_proj_pacmap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.8,
    alpha = 0.15) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-0.75, -0.35)) +
  ylim(c(-0.75, -0.4))+
  interior_annotation("c2", 
                      position = c(0.08, 0.9),
                      cex = 2)


```

```{r}
#| label: fig-five-gau-nldr-layouts
#| fig-height: 5
#| fig-width: 15
#| fig-pos: H
#| fig-cap: "(a) tSNE, (b) UMAP, and (c) PaCMAP"

tsne_gau + umap_gau + pacmap_gau +
  plot_layout(ncol = 3)
```

```{r}
#| label: fig-five-gau-projs
#| fig-height: 10
#| fig-width: 15
#| fig-pos: H
#| fig-cap: "(a) tSNE, (b) UMAP, and (c) PaCMAP"

five_gau_proj_tsne_model1 + five_gau_proj_umap_model1 + five_gau_proj_pacmap_model1 + five_gau_proj_tsne_model2 + five_gau_proj_umap_model2 + five_gau_proj_pacmap_model2 +
  plot_layout(guides = "collect", nrow = 2) &
  theme(legend.position='none')
```

<!-- ::: {#fig-gau-tsne-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/tsne_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_tsne.png) -->

<!-- ![](figures/five_gau_clusters/sc_tsne_3.png) -->

<!-- The tSNE layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and tries to *filled out* the clusters. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/RASEE7N5MbM>).  -->
<!-- ::: -->

<!-- ::: {#fig-gau-umap-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/umap_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_umap.png) -->

<!-- ![](figures/five_gau_clusters/sc_umap_3.png) -->

<!-- The UMAP layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and tries to *filled out* the clusters, but not as much as tSNE. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/iG4bCPkJilw>).  -->
<!-- ::: -->

<!-- ::: {#fig-gau-pacmap-sc layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/five_gau_clusters/pacmap_layout.png) -->

<!-- ![](figures/five_gau_clusters/2d_model_pacmap.png) -->

<!-- ![](figures/five_gau_clusters/sc_pacmap_2.png) -->

<!-- The PaCMAP layout, model in \gD{}, and a view of the fit in projections from $4\text{-}D$, for the five Gaussian cluster data. The model fits the separation and shows *flat* shaped clusters. (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/z07cKXi8EJQ>).  -->
<!-- ::: -->

## Applications {#sec-applications}

### Single-cell gene expression
<!--
- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use our method to get a better representation
-->

In the field of single-cell studies, a common analytical task involves clustering to identify groups of cells with similar expression profiles. NLDR methods are commonly used to display clusters, and help to verify the results. For example, @chen2023 illustrates the use of UMAP to identify clusters in Human Peripheral Blood Mononuclear Cells (PBMC3k). There are $2622$ single cells. First 9 principal components are used to generate the UMAP. @fig-NLDR-variety (a) is the reproduction of the published plot. 

<!-- UMAP layout with author's suggested parameter choice-->
```{r}
#| label: published-pbmc
## Import data
training_data_pbmc <- read_rds("data/pbmc3k/pbmc_pca_50.rds")
names(training_data_pbmc) <- paste0("p", 1:50)

training_data_pbmc <- training_data_pbmc[, 1:9] |>
  mutate(ID = 1:NROW(training_data_pbmc))

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc)
umap_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

# umap_pbmc <- umap_pbmc_scaled |>
#   ggplot(aes(x = UMAP1,
#              y = UMAP2)) +
#   geom_point(alpha=0.3, color = "#000000") 
```

To determine whether the UMAP representation with the specific (hyper-)parameter choice suggested by @chen2023 preserves the data structure present in $9\text{-}D$ (@fig-model-pbmc-author-proj (b)), we visualize the model constructed with UMAP overlaid on the $9\text{-}D$ data. The @fig-NLDR-variety (a) shows three well-separated clusters with big separations. However, as shown in @fig-model-pbmc-author-proj, there is no big separation between three clusters in $9\text{-}D$. Therefore, the suggested UMAP representation (@fig-NLDR-variety (a)) does not accurately represent the structure of PBMC3k dataset in $9\text{-}D$. But, when visualizing the *model-in-the-data-space* some unobserved structures can be seen. Some clusters have non-linear continuity patterns and high-density patches (@fig-model-pbmc-author-proj).  

In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $a_1$ using UMAP, tSNE, PHATE, PaCMAP, and TriMAP with different (hyper-)parameter settings (@fig-pbmc-mse) were calculated. tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)).
The visualization of the *model-in-the-data-space* provides evidence for this (@fig-pbmc2-sc). Therefore, tSNE with perplexity $30$ accurately captures the data structure of the PBMC3k dataset, which is better than UMAP.


<!-- As shown in @fig-NLDR-variety (e), there are three well-separated clusters, although the separation between the clusters are small. Additionally, non-linear structures can also be observed within the clusters (@fig-NLDR-variety (e)). This demonstrates that tSNE accurately captures the data structure of the PBMC3k dataset, which UMAP did not achieve. -->


<!-- In order to find a reasonable NLDR representation for the PBMC3k dataset, the MSE for different $b_1$ using UMAP with different (hyper-)parameter settings and tSNE with default (hyper-)parameter setting (@fig-pbmc-mse) were calculated. After analyzing the results, it was found that tSNE with default (hyper-)parameter setting (perplexity: $30$) achieved the lowest error. Therefore, tSNE with a perplexity value set to $30$, the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset. -->


<!--Fit the best model for author suggestion and compute error-->
```{r}
#| label: hexbin-pbmc
## Compute hexbin parameters
num_bins_x_pbmc <- 26
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  q = 0.1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "p"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

tr_df <- distinct(tibble(
  x = c(tr_from_to_df_pbmc[["x_from"]], tr_from_to_df_pbmc[["x_to"]]), 
  y = c(tr_from_to_df_pbmc[["y_from"]], tr_from_to_df_pbmc[["y_to"]])))

distance_df_small_edges_pbmc <- distance_pbmc |>
  filter(distance < benchmark_pbmc)

tr_from_to_df_pbmc <- inner_join(
  tr_from_to_df_pbmc, distance_df_small_edges_pbmc, 
  by = c("from", "to"))

# trimesh_removed_pbmc <- vis_rmlg_mesh(
#   distance_edges = distance_pbmc,
#   benchmark_value = benchmark_pbmc,
#   tr_coord_df = tr_from_to_df_pbmc,
#   distance_col = "distance") +
#   geom_point(
#     data = umap_pbmc_scaled,
#     aes(
#       x = UMAP1,
#       y = UMAP2
#     ),
#     alpha = 0.3,
#     size = 0.1,
#     color = "#000000"
#   ) 

trimesh_removed_pbmc <- ggplot() + 
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#33a02c") +
  geom_point(
    data = umap_pbmc_scaled,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    alpha = 0.3,
    size = 0.1,
    color = "#000000"
  )  +
  coord_fixed() +
  interior_annotation("a1")

# ## Compute error
# error_df <- augment(
#   df_bin_centroids = df_bin_centroids_pbmc,
#   df_bin = df_bin_pbmc,
#   training_data = training_data_pbmc,
#   newdata = NULL,
#   type_NLDR = "UMAP",
#   col_start = "PC_")

# ## Categorize error
# 
# error_df <- error_df |>
#   mutate(type = case_when(
#     row_wise_abs_error <= 5 ~ "error 0-5",
#     row_wise_abs_error <= 10 ~ "error 5-10",
#     row_wise_abs_error <= 15 ~ "error 10-15",
#     row_wise_abs_error <= 20 ~ "error 15-20",
#     .default = "error greter than 20"
#   )) |>
#   mutate(type = factor(type, levels = c(
#     "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error greter than 20")))
# 
# ## To join embedding
# error_df <- error_df |>
#   bind_cols(umap_pbmc_scaled |>
#               select(-ID))
# 
# error_plot_pbmc <- error_df |>
#   ggplot(aes(x = UMAP1,
#              y = UMAP2,
#              color = type,
#              group = ID)) +
#   geom_point(alpha=0.5,
#              size = 0.1) +
#   interior_annotation("b", sc_ltr_pos)

```

```{r}
#| label: pbmc-umap-model-proj

data_pbmc <- training_data_pbmc |> 
  select(-ID) |>
  mutate(type = "data")

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hb_id %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)

# Combine with the true model for visualization
df <- dplyr::bind_rows(scaled_pbmc_data_model |> mutate(type = "model"),
                       scaled_pbmc_data |> mutate(type = "data"))

## First projection
projection <- cbind(
    c(0.24156,0.85058,0.11564,-0.03255,-0.28106,0.27463,-0.10539,0.08429,0.13107),
    c(-0.67212,0.02745,-0.26718,-0.33126,-0.34919,0.20839,-0.14498,-0.37708,0.15455))

projected <- as.matrix(scaled_pbmc_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_pbmc_data, 
  proj_data = projected_df,  
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("a2")

## Second projection
projection <- cbind(
    c(0.00067,0.53963,-0.16241,-0.09777,0.32766,0.44543,-0.51359,-0.29884,0.00951),
    c(0.62399,0.26016,0.06441,-0.34686,-0.40466,0.24643,0.23537,0.07972,0.34402))

projected <- as.matrix(scaled_pbmc_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_pbmc_data, 
  proj_data = projected_df,  
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("a3")

```

<!-- ::: {#fig-pbmc1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/umap_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_1.png) -->

<!-- ![](figures/pbmc3k/sc_2.png) -->

<!-- ![](figures/pbmc3k/sc_3.png) -->

<!-- Model in \gD{}, on the UMAP layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.041)$, $b = 870 \  (30, \ 29)$, $m = 135$, benchmark value to remove large edges is $0.099$) (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/VqqWuE0Jj6A>). -->
<!-- ::: -->
<!--compute absolute error for different parameter choices-->

```{r}
#| label: combine-data-pbmc

error_pbmc_umap <- read_rds("data/pbmc3k/error_pbmc_umap_30_min_dist_0.3.rds")
error_pbmc_umap2 <- read_rds("data/pbmc3k/error_pbmc_umap_5_min_dist_0.01.rds")
# error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_15_min_dist_0.99.rds")
error_pbmc_umap3 <- read_rds("data/pbmc3k/error_pbmc_umap_12_min_dist_0.99.rds")
error_pbmc_tsne <- read_rds("data/pbmc3k/error_pbmc_tsne_5.rds")
error_pbmc_tsne2 <- read_rds("data/pbmc3k/error_pbmc_tsne_30.rds")
error_pbmc_phate <- read_rds("data/pbmc3k/error_pbmc_phate_5.rds")
error_pbmc_trimap <- read_rds("data/pbmc3k/error_pbmc_trimap_12_4_3.rds")
error_pbmc_pacmap <- read_rds("data/pbmc3k/error_pbmc_pacmap_30_random_0.9_5.rds")

error_pbmc <- bind_rows(error_pbmc_umap, 
                        error_pbmc_umap2,
                        error_pbmc_umap3,
                        error_pbmc_tsne,
                        error_pbmc_tsne2,
                        error_pbmc_phate,
                        error_pbmc_trimap,
                        error_pbmc_pacmap)

error_pbmc <- error_pbmc |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-pbmc
# read the png file from device 

#pbmc_layouts <- readPNG("figures/pbmc3k/pbmc_nldr_layouts.png", native = TRUE) 

error_plot_pbmc <- ggplot(error_pbmc, 
                          aes(x = a1, 
                              y = MSE, 
                              colour = method)) + 
  geom_point(size = 0.8) +
  geom_line(linewidth = 0.3) + 
  # geom_vline(xintercept = 15, linetype="solid", 
  #            color = "black", linewidth=0.8, alpha = 0.5) +
  scale_x_continuous(breaks = sort(unique(error_pbmc$a1))[c(1, 5, 9, 13, 17, 21, 26)]) +
  scale_color_manual(values=c('#e41a1c','#377eb8','#4daf4a','#ff7f00',
                              '#984ea3','#999999','#a65628','#f781bf')) +
  scale_y_log10() +
  ylab("log(MSE)") +
  xlab(expression(paste("binwidth (", a[1], ")"))) +
  theme_minimal() +
  theme(panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none",
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title.x = element_text(size = 7),
        axis.title.y = element_text(size = 7)) 

```

```{r}
#| fig-cap: "Assessing which of the 8 NLDR layouts on the PBMC3k data  (shown in Figure 1) is the better representation using MSE for varying binwidth ($a_1$). Colour  used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-h). Layout f is universally poor. Layouts a, b, g, h that show large separations between clusters are universally suboptimal. Layout d with little separation performs well at tiny binwidth (where most points are in their own bin) and poorly as binwidth increases. The choice of best is between layouts c and e, that have small separations between oddly shaped clusters. Layout e is the best choice."
#| label: fig-pbmc-mse
#| fig-pos: H
#| out-height: 100%

free(error_plot_pbmc) + wrap_plots(nldr1c, nldr2c, nldr3c, nldr4c,
           nldr5c, nldr6c, nldr7c, nldr8c, ncol = 2)
```

<!--best choice-->

```{r}
#| label: umap-pbmc-read-best
umap_pbmc2 <- read_rds("data/pbmc3k/pbmc_umap_12_min_dist_0.99.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc2)
umap_pbmc_scaled_best <- pbmc_scaled_obj$scaled_nldr

# tsne_pbmc <- tsne_pbmc_scaled |>
#   ggplot(aes(x = tSNE1,
#              y = tSNE2)) +
#   geom_point(alpha=0.3, color = "#000000")
```

<!-- We then fit the model for tSNE, and visualize the resultant model in the $9\text{-}D$ data space. The model shows a quirk, as shown in @fig-pbmc2-sc. All three clusters are connected by an edge except the small and large clusters. Because the clusters are so close in \gD{}, they attempt to maintain the structure in \pD{} as well. This is evident that tSNE with perplexity $30$ provides a reasonable representation of PBMC3k data. -->

<!--Fit the best model and compute error-->
```{r}
#| label: num-bins-pbmc
## Compute hexbin parameters
num_bins_x_pbmc <- 13
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled_best,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  q = 0.1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "p"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

# tr_from_to_df_pbmc <- tr_from_to_df_pbmc |>
#   filter(!(row_number() %in% c(65, 112, 130)))

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

#benchmark_pbmc <- 0.1

## Remove an edge that exist as long edge in p-D
# tr_from_to_df_pbmc |> mutate(ID = row_number()) |> filter(from == 37) |> filter(to == 43) 

tr_df <- distinct(tibble::tibble(
  x = c(tr_from_to_df_pbmc[["x_from"]], tr_from_to_df_pbmc[["x_to"]]), 
  y = c(tr_from_to_df_pbmc[["y_from"]], tr_from_to_df_pbmc[["y_to"]])))

distance_df_small_edges_pbmc <- distance_pbmc |>
  filter(distance < benchmark_pbmc)

tr_from_to_df_pbmc <- inner_join(
  tr_from_to_df_pbmc, distance_df_small_edges_pbmc, 
  by = c("from", "to"))

# tr_from_to_df_pbmc <- tr_from_to_df_pbmc |>
#   filter(!(row_number() %in% c(155)))

trimesh_removed_pbmc_best <- ggplot() + 
  geom_segment(data = tr_from_to_df_pbmc, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#33a02c") +
  geom_point(
    data = umap_pbmc_scaled_best,
    aes(
      x = UMAP1,
      y = UMAP2
    ),
    alpha = 0.2,
    color = "#000000"
  )  +
  coord_fixed() +
  interior_annotation("b1")

# trimesh_removed_pbmc <- vis_rmlg_mesh(
#   distance_edges = distance_pbmc,
#   benchmark_value = benchmark_pbmc,
#   tr_coord_df = tr_from_to_df_pbmc,
#   distance_col = "distance") +
#   geom_point(
#     data = tsne_pbmc_scaled,
#     aes(
#       x = tSNE1,
#       y = tSNE2
#     ),
#     alpha = 0.2,
#     color = "#000000"
#   ) 
#   #xlim(sc_xlims) + ylim(sc_ylims) +
#   #interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "p")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    row_wise_abs_error <= 25 ~ "error 20-25",
    .default = "error greter than 25"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error 20-25", 
    "error greter than 25")))

## To join embedding
error_df <- error_df |>
  bind_cols(umap_pbmc_scaled_best |>
              select(-ID))

error_plot_pbmc_best <- error_df |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

```{r}
#| label: pbmc-umap-model-proj-best

df_b_pbmc <- df_bin_pbmc |>
  dplyr::filter(hb_id %in% df_bin_centroids_pbmc$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_pbmc <- df_b_pbmc[match(df_bin_centroids_pbmc$hexID, df_b_pbmc$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_pbmc <- bind_rows(data_pbmc, df_b_pbmc)
scaled_pbmc <- scale_data_manual(df_model_data_pbmc, "type") |>
  as_tibble()

scaled_pbmc_data <- scaled_pbmc |>
  filter(type == "data") |>
  select(-type)

scaled_pbmc_data_model <- scaled_pbmc |>
  filter(type == "model") |>
  select(-type)

# Combine with the true model for visualization
df <- dplyr::bind_rows(scaled_pbmc_data_model |> mutate(type = "model"),
                       scaled_pbmc_data |> mutate(type = "data"))

## Removed long edge in high-d
# distance_df_small_edges_pbmc <- distance_df_small_edges_pbmc |>
#   filter(!(row_number() %in% c(155)))

## First projection
projection <- cbind(
    c(0.24156,0.85058,0.11564,-0.03255,-0.28106,0.27463,-0.10539,0.08429,0.13107),
    c(-0.67212,0.02745,-0.26718,-0.33126,-0.34919,0.20839,-0.14498,-0.37708,0.15455))

projected <- as.matrix(scaled_pbmc_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_pbmc_data, 
  proj_data = projected_df,  
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model1_best <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("b2")

## Second projection
projection <- cbind(
    c(0.00067,0.53963,-0.16241,-0.09777,0.32766,0.44543,-0.51359,-0.29884,0.00951),
    c(0.62399,0.26016,0.06441,-0.34686,-0.40466,0.24643,0.23537,0.07972,0.34402))

projected <- as.matrix(scaled_pbmc_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_pbmc_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_pbmc |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_pbmc_data, 
  proj_data = projected_df,  
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

pbmc_proj_umap_model2_best <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 2) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("b3")

```

<!-- <!--bin1 = 15, bin2 = 20, b = 300, non_empty = 136--> 
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| fig-cap: "(a) Model generated in $2\\text{-}D$ with tSNE, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows three well-separated distant clusters. The $p\\text{-}D$ model errors are distributed along clusters, but most low $p\\text{-}D$ model errors present in the large cluster." -->
<!-- #| label: fig-model-pbmc -->
<!-- #| fig-pos: H -->
<!-- #| out-height: 30% -->

<!-- trimesh_removed_pbmc + error_plot_pbmc +  -->
<!--   plot_layout(guides='collect', ncol=2) & -->
<!--   theme(legend.position='bottom') -->
<!-- ``` -->

```{r}
#| echo: false
#| fig-cap: "Model in \\gD{}, on the tSNE layout (a), and three views of the fit in projections from $9\\text{-}D$ (b, c, d), for the PBMC3k data ($a_1 = 0.099$, $b = 221 \\  (13, \ 17)$, $m = 92$, benchmark value to remove large edges is $0.1$). The three well-separated clusters with small separations observed in the \\gD{} layout of tSNE (@fig-NLDR-variety (e)) is also visible when visualizing the model overlaid on the data space in $9\\text{-}D$. Furthermore, there are some non-linear clusters. The model shows densed points within clusters in $9\\text{-}D$ space, which is an additional patterns not visible in the \\gD{} representation (@fig-NLDR-variety (e)). The full video is available at <https://youtu.be/5Y1hE4i7N2k>."
#| label: fig-model-pbmc-author-proj
#| fig-pos: H
#| fig-width: 10
#| fig-height: 10

free(trimesh_removed_pbmc) + free(trimesh_removed_pbmc_best) +
  pbmc_proj_umap_model1 + pbmc_proj_umap_model1_best +
  pbmc_proj_umap_model2 + pbmc_proj_umap_model2_best +
  plot_layout(nrow=3) &
  theme(legend.position='none')
```

<!-- ::: {#fig-pbmc2-sc layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/pbmc3k/tsne_trimesh_plot.png) -->

<!-- ![](figures/pbmc3k/sc_4.png) -->

<!-- ![](figures/pbmc3k/sc_5.png) -->

<!-- ![](figures/pbmc3k/sc_6.png) -->

<!-- Model in \gD{}, on the tSNE layout, and three views of the fit in projections from $9\text{-}D$, for the PBMC3k data ($(s_1, \ s_2) = (-0.050, \ -0.058)$, $b = 300 \  (15, \ 20)$, $m = 136$, benchmark value to remove large edges is $0.133$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/5Y1hE4i7N2k>). -->
<!-- ::: -->


### Hand-written digits
<!--
- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 
-->

<!--add different NLDR layouts-->

The MNIST dataset consists of $70000$ grayscale images of handwritten digits [@lecun2010]. @yingfan2021 used this dataset to evaluate how PaCMAP preserves local structures in \gD{}. Because of the large number of images, the focus was specifically on the handwritten digit 1, which exhibits a non-linear structure in \gD{} (@fig-pacmap-author). There are $7877$ images of the digit 1 in the dataset. Additionally, PCA was applied as a preprocessing step and the first $10$ principal components were selected. The objective is to assess whether PaCMAP effectively preserves the non-linear structure in \gD{}. 

```{r}
#| label: read-mnist-nldr
# Read a variety of different NLDR representations of mnist
# and plot them on same aspect ratio
tsne_mnist <- read_rds("data/mnist/mnist_tsne30.rds")

nldr_mnist1 <- tsne_mnist |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour='#ff7f00') +
  interior_annotation("a", c(0.08, 0.93))

umap_mnist <- read_rds("data/mnist/mnist_umap.rds")

nldr_mnist2 <- umap_mnist |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour='#a65628') +
  interior_annotation("b")

phate_mnist <- read_rds("data/mnist/mnist_phate.rds")

nldr_mnist3 <- phate_mnist |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour='#377eb8') +
  interior_annotation("c")

trimap_mnist <- read_rds("data/mnist/mnist_trimap.rds")

nldr_mnist4 <- trimap_mnist |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour='#4daf4a') +
  interior_annotation("d")

pacmap_mnist <- read_rds("data/mnist/mnist_pacmap.rds")

nldr_mnist5 <- pacmap_mnist |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour='#e41a1c') +
  interior_annotation("e")
```

```{r}
#| label: combine-data-mnist

error_mnist_umap <- read_rds("data/mnist/error_mnist_umap.rds")
error_mnist_tsne <- read_rds("data/mnist/error_mnist_tsne.rds")
error_mnist_phate <- read_rds("data/mnist/error_mnist_phate.rds")
error_mnist_trimap <- read_rds("data/mnist/error_mnist_trimap.rds")
error_mnist_pacmap <- read_rds("data/mnist/error_mnist_pacmap.rds")

error_mnist <- bind_rows(error_mnist_umap, 
                        error_mnist_tsne,
                        error_mnist_phate,
                        error_mnist_trimap,
                        error_mnist_pacmap)

error_mnist <- error_mnist |>
  mutate(a1 = round(a1, 2)) |>
  filter(bin1 >= 5) |>
  group_by(method, a1) |>
  filter(MSE == min(MSE)) |>
  ungroup()
```

```{r}
#| label: error-comp-mnist

error_plot_mnist <- ggplot(error_mnist, 
                          aes(x = a1, 
                              y = MSE, 
                              colour = method)) + 
  geom_point(size = 0.8) +
  geom_line(linewidth = 0.3) + 
  # geom_vline(xintercept = 15, linetype="solid", 
  #            color = "black", linewidth=0.8, alpha = 0.5) +
  scale_x_continuous(breaks = sort(unique(error_mnist$a1))[append(seq(1, length(unique(error_mnist$a1)), by = 5), 24)]) +
  scale_color_manual(values=c('#e41a1c','#377eb8','#4daf4a', "#ff7f00",'#a65628')) +
  scale_y_log10() +
  ylab("log(MSE)") +
  xlab(expression(paste("binwidth (", a[1], ")"))) +
  theme_minimal() +
  theme(panel.border = element_rect(fill = 'transparent'),
        plot.title = element_text(size = 12, hjust = 0.5, vjust = -0.5),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        legend.position = "none",
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        axis.title.x = element_text(size = 7),
        axis.title.y = element_text(size = 7)) 

```


```{r}
#| fig-cap: "Different methods with default hyper-parameter settings"
#| label: fig-mnist-mse
#| fig-pos: H
#| out-height: 100%

free(error_plot_mnist) + wrap_plots(nldr_mnist1, nldr_mnist2, nldr_mnist3, 
                                    nldr_mnist4, nldr_mnist5, ncol = 2)
```

<!-- PaCMAP preserves non-linear structure in $784\text{-}D$. To evaluate whether PaCMAP provides a reasonable representation of the data, the \gD{} embedding of the handwritten digit 1 was selected. As shown in @fig-pacmap-author, the angle of the digit 1 images varies along the \gD{} structure. -->

<!-- PaCMAP layout with author's suggested parameter choice-->
```{r}
#| label: read-mnist
## Import data
training_data_mnist <- read_rds("data/mnist/mnist_10_pcs_of_digit_1.rds")
training_data_mnist <- training_data_mnist |>
  mutate(ID = 1:NROW(training_data_mnist))

data_mnist <- training_data_mnist |>
  select(-ID) |>
  mutate(type = "data")

mnist_scaled_obj <- gen_scaled_data(
  data = tsne_mnist)
tsne_minst_scaled <- mnist_scaled_obj$scaled_nldr 

tsne_plot_mnist <- tsne_minst_scaled |> 
  ggplot(aes(x = tSNE1, 
             y = tSNE2)) + 
  geom_point(alpha=0.1, color = "#000000") 

# # read the png file from device 
# left_top_img <- readPNG("figures/mnist/mnist_digit1_img_lt.png", native = TRUE)
# center_img <- readPNG("figures/mnist/mnist_digit1_img_c.png", native = TRUE)
# right_bottom_img <- readPNG("figures/mnist/mnist_digit1_img_rb.png", native = TRUE)
# 
# tsne_plot_mnist <- tsne_plot_mnist +
#     inset_element(p = left_top_img, 
#                   left = 0.8, 
#                   bottom = 0.85, 
#                   right = 0, 
#                   top = 0.95) +
#     inset_element(p = center_img, 
#                   left = 1.2, 
#                   bottom = 0.6, 
#                   right = 0, 
#                   top = 0.7) +
#     inset_element(p = right_bottom_img, 
#                   left = 1.7, 
#                   bottom = 0.48, 
#                   right = 0, 
#                   top = 0.58)
```

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->
```{r}
#| fig-cap: "$2\\text{-}D$ layout from PaCMAP applied for the digit 1 of the MNIST dataset. The (hyper-)parameter settings, beyond the defaults, are $10$ nearest neighbors, ratio of the number of mid-near pairs to the number of neighbors equal to $0.5$, and the ratio of the number of further pairs to the number of neighbors is $2$. We use our *model-in-the-data-space* to assess whether this is an accurate representation of non-linear structure present in $10\\text{-}D$, or if it is misleading. The angle of the digit 1 images varies along this structure. Images at the top-left of the $2\\text{-}D$ layout show the digit 1 angled more to the left, while those at the bottom-right show the digit 1 angled more to the right."
#| label: fig-pacmap-author
#| fig-pos: H
#| out-height: 22%

tsne_plot_mnist
```

<!-- Fit the model and compute error-->
```{r}
#| label: hexbin-mnist
## Compute hexbin parameters
num_bins_x_mnist <- 19
lim1 <- mnist_scaled_obj$lim1
lim2 <- mnist_scaled_obj$lim2
r2_mnist <- diff(lim2)/diff(lim1) 

mnist_model <- fit_highd_model(
  training_data = training_data_mnist,
  emb_df = tsne_minst_scaled,
  bin1 = num_bins_x_mnist,
  r2 = r2_mnist,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC"
)

df_bin_centroids_mnist <- mnist_model$df_bin_centroids
df_bin_mnist <- mnist_model$df_bin

## Triangulate bin centroids
tr1_object_mnist <- tri_bin_centroids(
  df_bin_centroids_mnist, x = "c_x", y = "c_y")
tr_from_to_df_mnist <- gen_edges(
  tri_object = tr1_object_mnist)

## Compute 2D distances
distance_mnist <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_mnist, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_mnist <- find_lg_benchmark(
  distance_edges = distance_mnist, 
  distance_col = "distance")

sc_ltr_pos_mnist <- c(0.96, 0.96)

tr_df <- distinct(tibble::tibble(
  x = c(tr_from_to_df_mnist[["x_from"]], tr_from_to_df_mnist[["x_to"]]), 
  y = c(tr_from_to_df_mnist[["y_from"]], tr_from_to_df_mnist[["y_to"]])))

distance_df_small_edges_mnist <- distance_mnist |>
  filter(distance < benchmark_mnist)

tr_from_to_df_mnist <- inner_join(
  tr_from_to_df_mnist, distance_df_small_edges_mnist, 
  by = c("from", "to"))

trimesh_removed_mnist <- ggplot() + 
  geom_segment(data = tr_from_to_df_mnist, 
               aes(
                 x = x_from, 
                 y = y_from, 
                 xend = x_to, 
                 yend = y_to),
               colour = "#33a02c") +
  geom_point(data = tsne_minst_scaled,
             aes(
               x = tSNE1,
               y = tSNE2
             ),
             alpha=0.5, 
             size = 0.1) +
  coord_fixed() +
  interior_annotation("a", sc_ltr_pos_mnist)

# trimesh_removed_mnist <- vis_rmlg_mesh(
#   distance_edges = distance_mnist, 
#   benchmark_value = benchmark_mnist, 
#   tr_coord_df = tr_from_to_df_mnist, 
#   distance_col = "distance") +
#   geom_point(data = pacmap_minst_scaled,
#              aes(
#                x = PaCMAP1,
#                y = PaCMAP2
#              ),
#              alpha=0.5, 
#              size = 0.1) +
#   interior_annotation("a", sc_ltr_pos_mnist)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_mnist, 
  df_bin = df_bin_mnist, 
  training_data = training_data_mnist, 
  newdata = NULL, 
  type_NLDR = "tSNE", 
  col_start = "PC") 

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 2 ~ "error 0-2",
    row_wise_abs_error <= 4 ~ "error 2-4",
    row_wise_abs_error <= 6 ~ "error 4-6",
    row_wise_abs_error <= 8 ~ "error 6-8",
    row_wise_abs_error <= 10 ~ "error 8-10",
    .default = "error greter than 10"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-2", "error 2-4", "error 4-6", "error 6-8", "error 8-10", 
    "error greter than 10")))

## To join embedding
error_df <- error_df |>
  bind_cols(tsne_minst_scaled |> 
              select(-ID))
  
error_plot_mnist <- error_df |>
  ggplot(aes(x = tSNE1,
             y = tSNE2, 
             color = type, 
             group = ID)) +
  geom_point(alpha=0.5, 
             size = 0.1) +
  scale_colour_brewer(
    type = "qual",
    palette = 2
  ) + 
  theme(
      legend.position = "bottom",
      legend.text = element_text(size = 10),          # Increase legend text size
      legend.key.size = unit(1, "lines")              # Increase size of legend keys
  ) +
  guides(
      color = guide_legend(
          override.aes = list(size = 2.5, alpha = 1)      # Increase point size and opacity in legend
      )
  ) 
```

As a result of visualizing the *model-in-the-data-space*, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data (@fig-mnist-tri-proj). It also shows some quirks like twisted patterns and long edges (@fig-mnist-tri-proj). Furthermore, the results help to identify anomalies (with large model errors) within and the outside the non-linear structure, displaying different patterns of the digit 1 (@fig-model-error-mnist). 

<!-- According to @fig-mnist-tri-proj (b), the non-linear structure observed in the \gD{} layout of PaCMAP (@fig-pacmap-author) is also visible when visualizing the model overlaid on the data space. This indicates that PaCMAP accurately captures the non-linear structure of the $10\text{-}D$ data. Additionally, the model shows a twisted pattern within the non-linear structure in $10\text{-}D$ space (@fig-mnist-tri-proj (c)), which is an additional pattern not visible in the \gD{} representation (@fig-pacmap-author). Furthermore, as shown in @fig-mnist-tri-proj (d), some long edges exist in the $10\text{-}D$ space that are not recognized as long edges in the \gD{} representation. However, PaCMAP provides a reasonable \gD{} representation of MNIST digit 1 data, as it preserves the non-linear structure present in the $10\text{-}D$ data.  -->

<!--bin1 = 22, bin2 = 17, b = 374, non_empty = 140-->
```{r}
#| label: mnist-model-proj
df_b_mnist <- df_bin_mnist |>
  dplyr::filter(hb_id %in% df_bin_centroids_mnist$hexID) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

## Reorder the rows of df_b according to the hexID order in df_b_with_center_data
df_b_mnist <- df_b_mnist[match(df_bin_centroids_mnist$hexID, df_b_mnist$hb_id),] |>
  dplyr::select(-hb_id) 

# Apply the scaling
df_model_data_mnist <- bind_rows(data_mnist, df_b_mnist)
scaled_mnist <- scale_data_manual(df_model_data_mnist, "type") |>
  as_tibble()

scaled_mnist_data <- scaled_mnist |>
  filter(type == "data") |>
  select(-type)

scaled_mnist_data_model <- scaled_mnist |>
  filter(type == "model") |>
  select(-type)

# Combine with the true model for visualization
df <- dplyr::bind_rows(scaled_mnist_data_model |> mutate(type = "model"),
                       scaled_mnist_data |> mutate(type = "data"))


## First projection
projection <- cbind(
    c(0.71503,0.00502,-0.04370,-0.01811,-0.01103,0.09877,0.06199,0.05396,0.07771,-0.01713),
    c(0.00587,0.71879,0.03485,0.08284,-0.07534,-0.05420,0.04315,-0.00547,-0.02995,0.01810))

projected <- as.matrix(scaled_mnist_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_mnist_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_mnist_data, 
  proj_data = projected_df, 
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_pacmap_model1 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("b")

## Second projection
projection <- cbind(
    c(0.13639,-0.18552,-0.38537,-0.03088,-0.04663,0.15013,-0.28351,-0.28865,-0.03379,-0.38009),
    c(-0.46592,-0.40953,-0.02752,0.12815,0.08356,-0.12991,-0.28307,0.11145,0.08031,0.10800))

projected <- as.matrix(scaled_mnist_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_mnist_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_mnist_data, 
  proj_data = projected_df, 
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_pacmap_model2 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.1) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("c")

## Third projection
projection <- cbind(
    c(0.47163,0.21622,0.04027,0.17328,0.03603,0.06488,-0.22226,-0.07186,0.39674,0.13485),
    c(-0.41187,0.36322,0.11264,-0.14175,0.19810,0.07062,0.00680,0.08795,0.37068,-0.11280))

projected <- as.matrix(scaled_mnist_data) %*% projection

projected_df <- projected |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  #dplyr::mutate(type = df_exe$type) |>
  dplyr::mutate(ID = dplyr::row_number())

projected_model <- as.matrix(scaled_mnist_data_model) %*% projection

projected_model_df <- projected_model |>
  tibble::as_tibble(.name_repair = "unique") |>
  dplyr::rename(c("proj1" = "...1",
                  "proj2" = "...2")) |>
  dplyr::mutate(ID = dplyr::row_number())

model_df <- dplyr::left_join(
  distance_df_small_edges_mnist |> select(-distance), 
  projected_model_df, 
  by = c("from" = "ID"))

names(model_df)[3:NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_from")

model_df <- dplyr::left_join(model_df, projected_model_df, by = c("to" = "ID"))
names(model_df)[(2 + NCOL(projected_model_df)):NCOL(model_df)] <- paste0(names(projected_model_df)[-NCOL(projected_model_df)], "_to")

axes_obj <- gen_axes(
  data = scaled_mnist_data, 
  proj_data = projected_df, 
  limits = 1.5,
  axis_pos_x = -0.75,
  axis_pos_y = -0.75)

axes <- axes_obj$axes
circle <- axes_obj$circle

mnist_proj_pacmap_model3 <- projected_df |>
  ggplot(
    aes(
      x = proj1, 
      y = proj2)) +
  geom_segment(
    data = model_df, 
    aes(
      x = proj1_from, 
      y = proj2_from, 
      xend = proj1_to, 
      yend = proj2_to), 
    color = "#33a02c") +
  geom_point(
    size = 0.5,
    alpha = 0.2) +
  scale_color_manual(values = c("#000000", "#33a02c")) +
  geom_segment(
    data=axes, 
    aes(x=x1, y=y1, xend=x2, yend=y2), 
    colour="grey70") +
  geom_text(
    data=axes, 
    aes(x=x2, y=y2, label=rownames(axes)), 
    colour="grey50",
    size = 3) +
  geom_path(
    data=circle, 
    aes(x=c1, y=c2), colour="grey70") +
  coord_fixed() +
  xlim(c(-1, 1)) +
  ylim(c(-1, 1)) +
  interior_annotation("d")

```

<!--add langevitour screenshots and youtube animation link-->
```{r}
#| label: fig-mnist-tri-proj
#| fig-pos: H
#| fig-height: 10
#| fig-width: 10
#| fig-cap: "Model in \\gD{}, on the PaCMAP layout (a), and three views (b, c, d) of the fit in projections from $10\\text{-}D$, for the digit 1 of MNIST data ($a_1 = 0.062$, $b = 285 \\  (19, \ 15)$, $m = 109$, and benchmark value to remove large edges is $0.108$). (b) The non-linear structure observed in the \\gD{} layout of PaCMAP (@fig-pacmap-author) is also visible when visualizing the model overlaid on the data space. This indicates that PaCMAP accurately captures the non-linear structure of the $10\\text{-}D$ data. (c) the model shows a twisted pattern within the non-linear structure in $10\\text{-}D$ space, which is an additional pattern not visible in the \\gD{} representation (@fig-pacmap-author). (d) some long edges exist in the $10\\text{-}D$ space that are not recognized as long edges in the \\gD{} representation. The full video is available at <https://youtu.be/zcg_GXBmqjA>."

free(trimesh_removed_mnist) + mnist_proj_pacmap_model1 +
  mnist_proj_pacmap_model2 + mnist_proj_pacmap_model3 +
  plot_layout(ncol = 2)
```

<!-- ::: {#fig-mnist1-sc layout-ncol="4" fig-pos="H"} -->
<!-- ![](figures/mnist/mnist_model_2d.png){#fig-mnist1-model} -->

<!-- ![](figures/mnist/sc_1.png){#fig-mnist1-sc1} -->

<!-- ![](figures/mnist/sc_2.png){#fig-mnist1-sc2} -->

<!-- ![](figures/mnist/sc_3.png){#fig-mnist1-sc3} -->

<!-- Model in \gD{}, on the PaCMAP layout, and three views of the fit in projections from $10\text{-}D$, for the digit 1 of MNIST data ($(s_1, \ s_2) = (-0.100, \ -0.059)$, $b = 374 \  (22, \ 17)$, $m = 140$, benchmark value to remove large edges is $0.094$). (The **langevitour** software is used to view the data with a tour, and the full video is available at <https://youtu.be/zcg_GXBmqjA>). -->


<!-- ::: -->
<!-- need to update the youtube recording-->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

```{r}
#| label: img-with-error

## Data with pixel values
mnist_data <- read_rds("data/mnist/mnist_digit_1.rds")

img_error_within <- c(6169, 5987, 773, 6671, 4785, 4699, 5173, 5475) 

pixels_gathered_within <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_error_within)

imge_error_sample_within <- pixels_gathered_within |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none") 

img_error_outside <- c(1489, 5095, 855, 3143, 453, 3619, 1015, 409) #, 5101

pixels_gathered_outside <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_error_outside)

imge_error_sample_outside <- pixels_gathered_outside |>
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance, ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none") 

```

```{r}
#| label: fig-model-error-mnist
#| fig-cap: "$10\\text{-}D$ model error in PaCMAP layout of MNIST digit 1 dataset in $2\\text{-}D$. Most low $10\\text{-}D$ model errors are distributed along the lower edge of the $2\\text{-}D$ structure, while most large $10\\text{-}D$ model errors are concentrated along the upper edge (a). There are certain data points that exhibit large error rates due to their deviation from the usual $10\\text{-}D$ data structure, which makes them anomalies. These anomalies can be classified into two types: those that are anomalies within the non-linear structure and those that lie outside of it. The images associated with high model error points within the non-linear structure display different patterns of the digit 1 (b). However, when comparing these images to the ones found outside of the non-linear structure, it becomes evident that the latter display different patterns of the digit 1 (c). "
#| fig-pos: H
#| fig-width: 4
#| fig-height: 8
#| out-height: 80%

error_plot_mnist / (imge_error_sample_within + imge_error_sample_outside) +
  plot_annotation(tag_levels = 'a') &
  theme(plot.tag.position  = c(0.92, 0.92),
        plot.tag = element_text(color = "grey70"))
```

<!-- There are certain data points that exhibit high error rates due to their deviation from the usual $10\text{-}D$ data structure, which makes them anomalies (@fig-model-mnist). These anomalies can be classified into two types: those that are anomalies within the non-linear structure and those that lie outside of it. The images associated with high model error points within the non-linear structure display different patterns of the digit 1, as shown in @fig-mnist1-within. However, when comparing these images to the ones found outside of the non-linear structure, it becomes evident that the latter display different patterns of the digit 1 (@fig-mnist1-out).  -->

<!--images that occur large error-->
<!-- within the nonlinear structure-->
<!--outside the nonlinear structure-->

<!-- ::: {#fig-mnist-anomalies layout-ncol="2" fig-pos="H"} -->
<!-- ![](figures/mnist/img_error_sample_within.png){#fig-mnist1-within} -->

<!-- ![](figures/mnist/img_error_sample_out.png){#fig-mnist1-out} -->

<!-- Some images of handwritten digit 1 which occur high model error (a) within the non-linear structure, and (b) outside the non-linear structure. The images shows different patterns of digit 1. -->
<!-- ::: -->

## Discussion {#sec-discussion}

<!-- - Summarise contributions -->
<!-- - Explain where it is expected or not expected to work, eg higher dimensional relationships -->
<!-- - Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views (need to add) -->
<!-- - Predicting new observations in $k$-D -->
<!-- - Extending layouts beyond $k$-D, when 2D is clearly inadequate. -->
<!-- - Diagnostic app to explore differences in distances (need to add) -->
<!-- - What might be useful enhancements -->


This study makes several important contributions to the field of NLDR. We have developed an algorithm to evaluate the most useful NLDR method and (hyper-)parameter choices for creating a reasonable \gD{} layout of high-dimensional data. Our objective is to fit a model for the \gD{} layout that preserves the relationships between neighboring points and turns it into a high-dimensional wireframe, which can be overlaid on the data and visualized using a tour. This approach is defined as *model-in-data-space*. Viewing a model in the data space is an ideal way to examine the fit.

The effectiveness of this approach is illustrated through various examples. For instance, the S-curve example demonstrates how the model accurately fits the points, capturing both local and global structures in high-dimensional space. Our simulation case study further, five Gaussian cluster example shows that while all observed NLDR methods preserve the global structure, only tSNE effectively maintains the local structure, highlighting the specific strengths and quirks of different methods.

Human behavior often shows a desire for more certainty and a tendency to prefer well-separated views. This emphasizes the importance of clear and distinct clusters. For example, in the UMAP layout of the **pbmc** dataset suggested by @chen2023, three distant, well-separated clusters are shown. However, our model reveals that these clusters are actually close to each other in \pD{}. Additionally, the model discovers non-uniform data distribution and non-linear structures within the clusters that are not visible in the UMAP layout, demonstrating the ability of our model in uncovering hidden data characteristics.

Evaluating the error or unexplained variance is important for assessing how well the model fits the data. By examining the error for different numbers of bins, we found that tSNE with a perplexity of $30$ provides a reasonable representation for the **pbmc** dataset. Connecting the closest clusters with line segments in the fitted model further supports the preservation of neighborhood relationships.

The **digit: 1** example further illustrates the model's ability to accurately capture non-linear structures and provide additional information. Key findings include a twisted pattern that compresses the structure in some projections and long line segments that detect anomalies.

Predicting new observations in \kD{} is particularly valuable due to the limitations of some NLDR methods, like tSNE, which don't provide a straightforward method for prediction. As a result, our approach offers a solution that capable of generating predicted \kD{} embedding regardless of the NLDR method employed, effectively addressing this functional gap.

In conclusion, while our method effectively captures and represents high-dimensional data structures, further enhancements could involve introducing approaches to bind the data, indicate line segments beyond \gD{}, and diagnose the fitted model. These improvements would help in creating a more accurate representation of the data when \gD{} layout is inadequate.

## Supplementary Materials

Code, and data for reproducing this paper are available at [https://github.com/JayaniLakshika/paper-nldr-vis-algorithm](https://github.com/JayaniLakshika/paper-nldr-vis-algorithm).

  
## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
