---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional (high-D) data using non-linear transformation. The methods and parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of high-D distributions. To help evaluate the NLDR we have developed an algorithm to show the 2D NLDR model in the high-D space, viewed with a tour. One can see if the model fits everywhere or better in some subspaces, or completely mismatches the data. It is used to evaluate which 2D layout is the best representation of the high-D distribution and see how different methods may have similar summaries or quirks.
  
keywords: [high-dimensional data, dimension reduction, hexagon binning, low-dimensional manifold, tour, data vizualization, model in the data space]
keywords-formatted: [high-dimensional data, dimension reduction, hexagon binning, low-dimensional manifold, tour, data vizualization, model in the data space]

bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(quollr)
library(tibble)
library(dplyr)
# remotes::install_github("jlmelville/snedata")
library(snedata)
library(ggflowchart)
library(purrr) ## map function
library(gridExtra) ## for grid.arrange
library(rsample)
library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)
library(tidyr)

library(Rtsne)
library(uwot)
library(phateR)
library(patchwork)
library(langevitour)
library(colorspace)
library(kableExtra)
library(grid)
library(conflicted)
conflicts_prefer(dplyr::filter)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label, position = c(0.92, 0.92)) {
  annotation_custom(grid::textGrob(label = label,
      x = unit(position[1], "npc"), y = unit(position[2], "npc"),
      gp = grid::gpar(cex = 1, col="grey70")))
}
```

```{r}
#| label: code-setup
set.seed(20240110)
#source("nldr_code.R", local = TRUE)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{1.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data. Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2022], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
# label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr2 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_15_min_dist_0.99.rds")
nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_51.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e")

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f")

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g")

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h")
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Six different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 51), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The paper is organised as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD{}   representations of \pD{}   data have been computed using multidimensional scaling (MDS) [@borg2005], which includes principal components analysis (PCA) [@jolliffe2011] as a special case.  The \kD{}   representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{}   layout is constructed by minimizing a stress function that differences distances between points in \pD{}   with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}   minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is \gD{}, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in \gD{}    represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@cfa69] and multidimensional scaling (MDS) [@borg2005]. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)
<!--### Notation -->

<!-- @tbl-notation summarises the notation used to explain the new methodology. The observed data is denoted as $\mathbfit{x}_{n \times p}$ where $x_{ij}$ would indicate the $i^{th}$ observation on the $j^{th}$ variable sampled from a population $\mathbfit{X}$. To refer to variable $j$, we would use $X_j$.--> 

<!--
$X_{n \times p} = \begin{bmatrix} \textbf{x} _{1} & \textbf{x}_ {2} & \cdots & \textbf{x}_{n} \\  \end{bmatrix}^\top$

$Y_{n \times d} = \begin{bmatrix} \textbf{y} _{1} & \textbf{y}_ {2} & \cdots & \textbf{y}_{n} \\  \end{bmatrix}^\top$

$C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$

$C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$ $p$-D mappings of 2D hexagon bin centroids of the $k^{th}$ hexagon

-->

```{r}
#| label: tbl-notation
#| tbl-cap: "Summary of notation for describing new methodology."
# Notation used in the paper

notation_df <- read_csv("misc/notation.csv")

# Create the table
kable(notation_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("3cm", "12cm"))
```

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with (hyper-)parameters $\mathbfit{\theta}$. The parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. <!--Any input requirements for the data (such as normalization, or preprocessing through the use of PCA or similar) is considered part of the function $g$.-->

With our goal being to make a representation of this \gD{}    layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbour information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first hexagonly bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-scurve, and documented below.


<!--UMAP applied for Scurve data-->
```{r}
umap_scurve <- read_rds(file = "data/s_curve/s_curve_umap.rds") 

scurve_scaled_obj <- gen_scaled_data(
  data = umap_scurve)

umap_scurve_scaled <- scurve_scaled_obj$scaled_nldr
lim1 <- scurve_scaled_obj$lim1
lim2 <- scurve_scaled_obj$lim2
r2 <- diff(lim2)/diff(lim1)

sc_ltr_pos <- c(0.08, 0.96)
sc_xlims <- c(-0.25, 1.25)
sc_ylims <- c(-0.2, 1.9)
# sc_xlims <- c(-0.25, 1.25)
# sc_ylims <- c(-0.25, 2)

nldr_scurve <- umap_scurve_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, colour="#000000", size = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)
```

<!--Full hexagon grid with UMAP data-->

```{r}
## Compute hexbin parameters
num_bins_x_scurve <- 6

## hexagon binning to have regular hexagons
hb_obj_scurve <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = num_bins_x_scurve, 
  r2 = r2)

## Data set with all centroids
all_centroids_df <- hb_obj_scurve$centroids

## Generate all coordinates of hexagons
hex_grid <- hb_obj_scurve$hex_poly

## To obtain the standardise counts within hexbins
counts_df <- hb_obj_scurve$std_cts
df_bin_centroids <- extract_hexbin_centroids(
  centroids_df = all_centroids_df, 
  counts_df = counts_df) |>
  filter(drop_empty == FALSE)

hex_grid_with_counts <- 
  left_join(hex_grid,
            counts_df, 
            by = c("hex_poly_id" = "hb_id"))

hex_grid_scurve <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = umap_scurve_scaled, 
             aes(x = UMAP1, y = UMAP2), 
             alpha = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos)
```

<!--Non-empty bins with bin centroids-->

```{r}
hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids$hexID)

hex_grid_nonempty_scurve <-  ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids, 
             aes(x = c_x, y = c_y), 
             color = "#33a02c") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos) 
```

<!--2D model-->

```{r}
## Triangulate bin centroids
tr1_object_scurve <- tri_bin_centroids(
  df_bin_centroids, x = "c_x", y = "c_y")
tr_from_to_df_scurve <- gen_edges(
  tri_object = tr1_object_scurve)

## Compute 2D distances
distance_scurve <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_scurve, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_scurve <- find_lg_benchmark(
  distance_edges = distance_scurve, 
  distance_col = "distance")

trimesh_removed_scurve <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = benchmark_scurve, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos) 
```

To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "S-curve". It is constructed by simulating $n=750$ observations from $\theta \sim U(-3\pi/2, 3\pi/2)$, $X_1 = \sin(\theta)$, $X_2 \sim U(0, 2)$ (adding thickness to the S), $X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)$. The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance. We would consider $T=(X_1, X_2, X_3)$ to be the geometric structure (true model) that we hope to capture.

XXX SHOW THE S-CURVE DATA WITH THE TRUE MODEL OVERLAID HERE, BEFORE THE NLDR VIEWS.


```{r}
#| label: fig-NLDR-scurve
#| echo: false
#| fig-cap: "Key steps for constructing the model on the UMAP layout ($k=2$): (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids. The S-curve data is shown."
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
 
nldr_scurve + hex_grid_scurve + 
  hex_grid_nonempty_scurve + 
  trimesh_removed_scurve +
  plot_layout(ncol = 4)
```

#### Scaling the data

Because we are working with distances between points, starting with data having a standard scale, e.g. [0, 1], is recommended. When the aspect ratio of the NLDR $(r_1, r_2, ..., r_k)$ is meaningful, the data can be scaled to take this into account. When $k=2$, as in hexagon binning, the default range is $[0, y_{i,\text{max}}], i=1,2$, where $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = \frac{r_2}{r_1}$ (@fig-NLDR-scurve). <!-- \times \frac{2}{\sqrt{3}}$. (The $\frac{2}{\sqrt{3}}$ accounts for the different height ($a_1$) and width ($a_2$) of a regular hexagon.) The scaling of data should take the size of the hexagons into account, but choice of number of bins should. -->

#### Computing hexagon grid configuration

The \gD{} hexagon grid is defined by the number of bins in each direction $(b_1, b_2)$, giving total number of bins as $b = b_1 \times b_2$, and a unique hexagon id, $h = 1, \dots, b$. Each hexagon, $H_h$ is uniquely described by centroid, $C_{h}^{(2)} = (c_{h1}, c_{h2})$.  (Note that, we detail the algorithm for constructing the grid and binning the data because we found that there was no readily available implementation that could produce what was needed for this project.)

To make the grid, a buffer parameter ($q$) is set as a proportion of the plot space. By default this is 0.1. A value should be chosen to provide at buffer of a full hexagon width ($a_1$) and height ($a_2$) for the grid to extend beyond the data. The lower left position where the grid starts is defined as $(s_1, s_2)$, corresponding to the centroid of the lowest left hexagon, which should be below the minimum data value. This is computed as one buffer unit, $q$ below the minimum data value, by default. The user provides the preferred number of bins in the horizontal direction, $b_1$, and the algorithm computes the appropriate number of bins in the vertical direction, $b_2$. 

The value for $b_2$ is computed by fixing $b_1$. Considering the lower bound of the NLDR, $a_1 > -2q$, and $a_1 > \frac{1+q}{b_1 -1}$. Similarly, according to the upper bound of the NLDR, $a_1 > \frac{2r_2(1 + q)}{\sqrt{3}(b_2 - 1)}$, because $a_2 = \frac{\sqrt(3)}{2}a_1$ for regular hexagons. Therefore, $b_2 = \Big\lceil1 +\frac{2r_2(b_1 - 1)}{\sqrt{3}}\Big\rceil$. 

```{r}
# Code to draw illustration for notation
start_pt <- all_centroids_df |> 
  filter(hexID == 1)
d_rect <- tibble(x1min = 0, 
                 x1max = 1,
                 x2min = 0,
                 x2max = r2)
c_start <- tibble(s1 = 0.03, s2 = 0.05)
a1 <- tibble(x = all_centroids_df$c_x[21],
             xend = all_centroids_df$c_x[22],
             y = all_centroids_df$c_y[21],
             yend = all_centroids_df$c_y[21],
             label = expression(a[1]))
a2 <- tibble(x = all_centroids_df$c_x[27],
             xend = all_centroids_df$c_x[27],
             y = all_centroids_df$c_y[27],
             yend = all_centroids_df$c_y[33],
             label = expression(a[2]))
hex_param_vis <- ggplot() + 
    geom_polygon(data = hex_grid, 
                        aes(x = x, 
                            y = y, 
                            group = hex_poly_id),
                 fill = "white", 
                 color = "#bdbdbd") +
    geom_point(data = all_centroids_df, aes(
      x = c_x, 
      y = c_y), 
      color = "#31a354", size = 0.9) +
    geom_point(data = start_pt, aes(x = c_x, 
                                    y = c_y), 
               color = "black") + 
    geom_rect(data=d_rect, 
              aes(xmin = x1min - c_start$s1,# - c_start$s1, 
                  xmax = x1max - c_start$s1,# - c_start$s1, 
                  ymin = x2min - c_start$s2,# - c_start$s2, 
                  ymax = x2max - c_start$s2),# - c_start$s2), 
              fill = "white", 
              color = "black", 
              alpha = 0, 
              linewidth = 0.7) +
    geom_point(data=d_rect, aes(x=x1min - c_start$s1, 
                                y=x2min - c_start$s2)) + 
    geom_point(data=d_rect, aes(x=x1max - c_start$s1, 
                                y=x2min - c_start$s2)) + 
    geom_point(data=d_rect, aes(x=x1min - c_start$s1, 
                                y=x2max - c_start$s2)) + 
    annotate("text", x=d_rect$x1min - c_start$s1, 
                     y=d_rect$x2min - c_start$s2,
                     label = "(0,0)", 
             hjust=-0.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1max - c_start$s1, 
                     y=d_rect$x2min - c_start$s2,
                     label = "(0,1)", 
             hjust=1.1, vjust=-0.3) + 
    annotate("text", x=d_rect$x1min - c_start$s1, 
                     y=d_rect$x2max - c_start$s2,
                     label = expression(group("(", 
                        list(0, y[2][max]),")")), 
            hjust=-0.1, vjust=1.2) + 
    geom_segment(data=d_rect, aes(
      x = x1min  - c_start$s1, # 0 - 0.03, 
      y = -0.35, 
      xend = x1max - c_start$s1, #1 - 0.03, 
      yend = -0.35), #-0.35),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
                 color = "black")+
    annotate("text", x=0.5, y=-0.45, 
             label = expression(r[1]), color = "black") +
    geom_segment(data=d_rect, aes(
      x = -0.25, 
      y = x2min - c_start$s2, #0 - 0.05, 
      xend = -0.25, 
      yend = x2max - c_start$s2), #r2 - 0.05),
      arrow = arrow(length = unit(0.03, "npc"),
                       ends = "both"), 
                 color = "black")+ 
    annotate("text", x=-0.35, y=1, 
             label = expression(r[2]), color = "black") +
    geom_segment(data = a1, aes(
      x = x, #-0.1 + 0.2087578, 
      y = y, #-0.15, 
      xend = xend, #-0.1 + 0.2087578*2, 
      yend = yend), #-0.15),
      arrow = arrow(length = unit(0.03, "npc"),
        ends = "both"), 
        color = "black")+ # a1 = 0.2087578
    annotate("text", 
             x=(a1$x+a1$xend)/2, 
             y=a1$y, 
             label = expression(a[1]), 
             color = "black",
             vjust = 1.2) +
    geom_segment(data = a2, aes(
      x = x, #-0.15, 
      y = y, #-0.1*r2 + 0.1807896*2, 
      xend = xend, #-0.15, 
      yend = yend), #-0.1*r2 + 0.1807896*3),
      arrow = arrow(length = unit(0.03, "npc"),
                               ends = "both"), 
      color = "black") + # a2 = 0.1807896
    annotate("text", x=a2$x, y=(a2$y+a2$yend)/2, 
             label = expression(a[2]), 
             color = "black", hjust=-0.2) +
    annotate("text", x=-0.18, y=-0.25, 
      label = expression(group("(", list(s[1], s[2]), ")")),
      color = "black") 
```

```{r}
#| label: fig-hex-param
#| fig-cap: "The components of the hexagon grid illustrating notation."
#| out-height: 30%
#| fig-width: 2.5
#| fig-height: 3.75
#| fig-pos: H
 
hex_param_vis
```

<!-- Number of bins is set by fixing $b_1$, which determines the binwidth accounting the offset $q_1$ and breaks on $x_1$, is calculated as  -->

<!-- $$ -->
<!-- a_1 = \frac{r_1 + q_1}{b_1}. -->
<!-- $$ -->

<!-- The computed binwidth then determines the number of vertical bins, $b_2$, and vertical binwidth, which are computed based on $r_2$, the offset $q_2$, and the height/width ratio of a regular hexagon, $\frac{2}{\sqrt{3}}$ as -->

<!-- $$ -->
<!-- a_2 = \frac{r_2 + q_2}{0.75 \times b_2}. -->
<!-- $$ -->

<!-- To define a hexagon grid across the \kD{} space, with hexagons of fixed height ($a_1$), width ($a_2$),  it is necessary to determine how many hexagons should be represented along each axis in the \kD{} space.

When $k=2$, the number of bins along the $x$ and $y$ axes, $b_1$ and $b_2$, is computed by considering the scaling factors ${r_1, r_2}$, along with the offset along the axes ($q_1$, $q_2$), and the height ($a_1$) and width ($a_2$) of a regular hexagon (@fig-NLDR-scurve (b)). ($b_1 = \frac{r_1 + q_1}{a_2}$, and $b_2 = \frac{r_2 + q_2}{0.75 \times a_1}$) where $0.75 \times a_1$ accommodate to have hexagons fill the entire 2D space without leaving any gaps between them.)-->

<!-- XXX We don't need parameters for the regular hexagon, these are fixed constants. -->


#### Binning the data

<!-- Points are allocated to the bin they fall into based on the nearest centroid. In situations where a point is equidistant from multiple centroids, tie-breaking rules are applied. If multiple centroids are in the same row, the point is assigned to the leftmost centroid. If multiple centroids are in different rows, the point is assigned to the bottom centroid. -->

<!-- $\{ i \in H_h, h = 1, \dots, b, \text{ and } i = 1, \dots, n\}$ -->

Observations are grouped into bins based on their nearest centroid. This produces a reduction in size of the data from $n$ to $m$, where $m\leq b$ (total number of bins). This can be defined using the function $u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}$, where
$u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2}$, mapping observation $i$ into $H_h = \{i| u(i) = h\}$. 

By default, the bin centroid is used for describing a hexagon (as done in @fig-NLDR-scurve (c)), but any measure of center, such as a mean or weighted mean of the points within each hexagon, could be used. The bin centers, and the binned data, are the two important components needed to render the model representation in high dimensions.  

<!-- XXX How are you doing this? Do you check the bounds of the hexagon? Or do you use distance to centroid? -->

<!--
Define a hexagon grid across the \kD{} space, using hexagons with fixed height ($a_2$), width ($a_1$). Each of the \kD{} points will belong to a hexagon bin. That is, for each $y \in \mathbfit{Y}$, we can (uniquely) identify the hexagon that the point belongs to. This identification is done finding the nearest bin centroid for the \kD{} points by considering the \kD{} Euclidean distance.    

When $k=2$, the starting coordinates $(s_1, s_2)$ mark the lower left of the grid. This is the bottom left bin centroid. By starting from there, points are generated to fill the grid accounting $b_1$, $b_2$, $a_1$, and $a_2$. 
-->

<!--We deliberately separate out the creation of the hexagon grid from the mapping of points on the grid.-->

#### Indicating neighborhood

Delaunay triangulation [@lee1980] is used to connect neighboring centroids, which is needed to preserve neighborhood information when the model is lifted into \pD{}. 

<!-- When $k = 2$ Delaunay triangulation on $C^{(2)}$ generates the model in \gD{} space, which is a triangular mesh (@fig-NLDR-scurve (d)). It generates convex hulls of $C^{(2)}$ such that the circumcircle of every triangle in the triangulation contains no other points from $C^{(2)}$. -->

It can also happen that distant centroids can be connected, which can result in long line segments. In order to generate a smooth surface in \gD{}, these long line segments must be removed (@fig-NLDR-scurve (d)).

<!--need to add what is meant by a long edge-->

### Rendering the model in \pD{}

The last step is to lift the \kD{} model into \pD{} by computing \pD{} vectors that represent bin centroids. We use the \pD{} mean of the points in $H_h$ to map the centroid $C_{h}^{(2)} = (c_{h1}, c_{h2})$ to a point in \pD{}. Let the \pD{} mean be

$$C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.$$
Furthermore, line segments that exist in the \kD{} model generate line segments in \pD{} by connecting the \pD{} means of the corresponding \kD{} bin centroids.

<!--langevitour with pD model for S-curve-->
::: {#fig-scurve-sc layout-ncol="4" fig-pos="H"}
![](figures/scurve/umap_view_copy.png)

![](figures/scurve/sc_1.png)

![](figures/scurve/sc_2.png)

![](figures/scurve/sc_3.png)

XXX Model in \gD{}, on the UMAP layout, and three views of the fit in projections from $7\text{-}D$, for the S-curve data. The model closely fits the shape, but it has breaks which means that it does not adequately capture the surface. (The **langevitour** software is used to view the data with a tour, and the full video is available at (<https://youtu.be/G1m4q9k--v4>). NEED TO CHANGE FIRST PLOT TO BE 2D WIREFRAME.
:::


### Measuring the fit {#sec-summary}
 <!-- Fitted values,  Error calculation-->

The model here is similar to a confirmatory factor analysis model (REF), $\widehat{T}(X_1, X_2, X_3) + \Epsilon$. The difference between the fitted model and observed values would be considered to be residuals, and for this problem are $7\text{-}D$. 

<!--#### Fitted values-->

Observations are associated with their bin center, $C_{h}^{(p)}$, which are also considered to be the *fitted values*. These can also be denoted as $\widehat{X}$. <!--The fitted values of the points in $H_h$ refers to the \pD{} mapping $C_{h}^{(p)}$ of the corresponding \kD{} model point $C_{h}^{(2)}$.-->

<!--#### Error-->

The error is computed by taking the squared \pD{} Euclidean distance, corresponding to computing the mean squared error (MSE) as:

$$\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2$${#eq-equation1} 

where $n$ is the number of observations, $b$ is the number of bins, $n_h$ is the number of observations in $h^{th}$ bin, $p$ is the number of variables, $\mathbfit{x}_{hij}$ is the $j^{th}$ dimensional data of $i^{th}$ observation in $h^{th}$ hexagon. 

### Prediction into \gD{}

A new benefit of this fitted model is that it allows us to now predict a new observation's value in the NLDR, for any method. The steps are to determine the closest bin center in \pD{}, $C^{(p)}_{h}$ and predict it to be the centroid of this bin in \gD{}, $C^{(2)}_{h}$. This can be written as, let $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$, then the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$. 

<!--The prediction approach involves finding the nearest \kD{} model point for a new \pD{} point. We define the function $z: \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{m\times p}$, where $z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}$ maps each \pD{} point to its nearest \pD{} mapping of the model. Therefore, the new observation $i$ falls in the hexagon, $H_h = \{i| z(i) = h\}$ and the corresponding \kD{} bin centroids, $C_{h}^{(2)} = (c_{h1}, c_{h2})$ be the predicted values.-->  

### Tuning
<!-- removal of low density bins, removing long edges, choice of bins-->

The performance and robustness of our model depend on three key parameters: (i) the total number of bins ($b$), (ii) a benchmark value used to remove low-density hexagons, and (iii) a benchmark value used to remove long edges. However, there is no analytical formula to calculate an appropriate value for these parameters. The selection of these parameter values depends on the model performance computed by MSE (see @sec-summary). 

#### Choice of bins

The number of hexagon bins in the hexagon grid has a considerable impact on the construction of the \gD{} model, serving as the initial step. The chosen total number of bins must effectively capture the structure of the NLDR data. If the number of bins is too low, the model may not be able to capture the structure of the NLDR data effectively (see @fig-bins-scurve (a)), while if there are too many bins, it may result in over-fitting the individual points of the NLDR data (see @fig-bins-scurve (c)). Therefore, it is important to determine an effective number of bins to construct a reasonable model (see @fig-bins-scurve (b)).

<!--add three choice of bins 7, 10, 14-->

```{r}
## hexagon binning to have regular hexagons
hb_obj_scurve1 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 7, 
  r2 = r2,
  q = 0.05)

## Data set with all centroids
all_centroids_df1 <- hb_obj_scurve1$centroids

## Generate all coordinates of hexagons
hex_grid1 <- hb_obj_scurve1$hex_poly

## To obtain the standardise counts within hexbins
counts_df1 <- hb_obj_scurve1$std_cts
df_bin_centroids1 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df1, 
  counts_df = counts_df1) |>
  filter(drop_empty == FALSE)

hex_grid_with_counts_s_curve1 <- full_join(
  hex_grid1, 
  df_bin_centroids1 |> select(hexID, std_counts), 
    by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve1 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve1, 
    aes(x = x, y = y,
      group = hex_poly_id, 
      fill = std_counts), color = "grey70", linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
             aes(x = UMAP1, y = UMAP2),
             alpha = 0.3,
             size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  interior_annotation("a") +
  xlim(sc_xlims) + ylim(sc_ylims)
```

```{r}
## hexagon binning to have regular hexagons
hb_obj_scurve2 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 12, 
  r2 = r2,
  q = 0.07)

## Data set with all centroids
all_centroids_df2 <- hb_obj_scurve2$centroids

## Generate all coordinates of hexagons
hex_grid2 <- hb_obj_scurve2$hex_poly

## To obtain the standardise counts within hexbins
counts_df2 <- hb_obj_scurve2$std_cts
df_bin_centroids2 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df2, 
  counts_df = counts_df2) |>
  filter(drop_empty == FALSE)

hex_grid_with_counts_s_curve2 <- full_join(
  hex_grid2, 
  df_bin_centroids2 |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve2 <- ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve2, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", 
        linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, 
    na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b")

```

```{r}
## hexagon binning to have regular hexagons
hb_obj_scurve3 <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = 14, 
  r2 = r2,
  q = 0.05)

## Data set with all centroids
all_centroids_df3 <- hb_obj_scurve3$centroids

## Generate all coordinates of hexagons
hex_grid3 <- hb_obj_scurve3$hex_poly

## To obtain the standardise counts within hexbins
counts_df3 <- hb_obj_scurve3$std_cts
df_bin_centroids3 <- extract_hexbin_centroids(
  centroids_df = all_centroids_df3, 
  counts_df = counts_df3) |>
  filter(drop_empty == FALSE)

hex_grid_with_counts_s_curve3 <- full_join(hex_grid3, 
                                          df_bin_centroids3 |> select(hexID, std_counts), 
                                          by = c("hex_poly_id" = "hexID")) 

hex_grid_coloured_scurve3 <-  ggplot() + 
  geom_polygon(
    data = hex_grid_with_counts_s_curve3, 
    aes(x = x, y = y, 
        group = hex_poly_id, 
        fill = std_counts), 
        color = "grey70", linewidth=0.2) +
  geom_point(data = umap_scurve_scaled,
           aes(x = UMAP1, y = UMAP2),
           alpha = 0.3,
           size = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c")
  
```

```{r}
#| echo: false
#| label: fig-bins-scurve
#| fig-pos: H
#| fig-cap: "Hexbin density plots of UMAP layout of the S-curve data, using three different bin inputs: (a) $b = 91$ (7, 13), (b) $b = 190$ (10, 19), and (c) $b = 364$ (14, 26). Color indicates standardized counts, dark indicating high count and light indicates low count. At the smallest bin size the data segregates into two separate groups, suggesting this is too many bins. Using the MSE of the model fit in $p-D$ helps decide on a useful choice of number of bins."
#| fig-width: 6
#| fig-height: 3

hex_grid_coloured_scurve1 + hex_grid_coloured_scurve2 + hex_grid_coloured_scurve3 +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
``` 

To determine the effective $b$, candidate values are selected based on the range between the minimum and approximate maximum $b_1$, because $b_2$ is computed from $b_1$. The minimum $b_1$ is set to $2$, while the maximum number is estimated by taking the square root of $\frac{n}{2}$. The analysis evaluates the MSE across varying $b$ within this range, covering the minimum to maximum values along both axes (see @fig-mse-scurve-b).

<!--add MSE vs total number of error plot-->

```{r}
training_data_scurve <- read_rds("data/s_curve/s_curve_training.rds")
```

<!--To generate errors for different total number of bins-->
```{r}
## To initialize number of bins along the x-axis
bin1_vec_scurve <- 4:19 #sqrt(NROW(training_data_scurve)/2)

error_scurve <- data.frame(matrix(nrow = 0, ncol = 0))

for (xbins in bin1_vec_scurve) {

  bin2 <- calc_bins_y(bin1 = xbins, r2 = r2, q = 0.07)$bin2

  scurve_model <- fit_highd_model(
    training_data = training_data_scurve,
    emb_df = umap_scurve_scaled,
    bin1 = xbins,
    r2 = r2,
    q = 0.07, 
    is_bin_centroid = TRUE,
    is_rm_lwd_hex = FALSE,
    col_start_highd = "x"
  )

  df_bin_centroids_scurve <- scurve_model$df_bin_centroids
  df_bin_scurve <- scurve_model$df_bin

  ## Compute error
  error_df <- glance(
    df_bin_centroids = df_bin_centroids_scurve,
    df_bin = df_bin_scurve,
    training_data = training_data_scurve,
    newdata = NULL,
    type_NLDR = "UMAP",
    col_start = "x") |>
    mutate(bin1 = xbins,
           bin2 = bin2,
           b = bin1 * bin2,
           b_non_empty = NROW(df_bin_centroids_scurve))

  error_scurve <- bind_rows(error_scurve, error_df)

}

mse_scurve_b <- ggplot(error_scurve, 
                     aes(x = b, 
                         y = log(MSE))) + 
  geom_point(size = 1) +
  geom_line() + 
  geom_vline(xintercept = 190, linetype="solid", 
             color = "black", linewidth=0.8, alpha = 0.5) +
  geom_vline(xintercept = 91, linetype=2, 
             color = "black", linewidth=0.8, alpha = 0.5) +
  geom_vline(xintercept = 364, linetype=2, 
             color = "black", linewidth=0.8, alpha = 0.5) +
  ylab("log(MSE)") +
  xlab("b") +
  ggtitle("(a)") +
  theme(aspect.ratio = 0.75,
        axis.text.x = element_text(size = 5),
        axis.text.y = element_text(size = 5),
        axis.title.x = element_text(size = 5),
        axis.title.y = element_text(size = 5, 
            angle=90),
        title = element_text(size = 72))
```

```{r}
#| fig-cap: "MSE from UMAP applied to S-curve dataset with diffferent $b$ choices. What is the effective $b$ to create the model? The residual plot have a steep slope at the beginning, indicating that a smaller $b$ causes a larger amount of MSE. Then, the slope gradually declines or level off, indicating that a higher $b$ generates a smaller MSE. Using the elbow method, it was observed that when the $b$ is set to $190$, the lowest MSE occurred."
#| label: fig-mse-scurve-b
#| fig-pos: H
#| out-height: 50%
#| eval: false

mse_scurve_b
```

#### Removal of low density bins

Once setting up the hexagon grid with an appropriate number of bins, some hexagon bins may have few or no data points within them (see @fig-bins-scurve (b)). To ensure comprehensive coverage of the NLDR data, it is necessary to select hexagon bins with a considerable number of data points. This involves calculating the number of points within each hexagon. Then, the standard count is computed by dividing the number of points within each hexagon by the maximum number of points in the grid. Next, bins with a standard count less than a benchmark value are removed (see @fig-lwd-scurve (a)). There is no specific rule for selecting a benchmark value. However, the following steps can help determine a suitable value for removing low-density hexagons:

1. Plot the distribution of the standardized counts (see @fig-stdcts-scurve).
2. Examine the distribution of counts.
3. Select the first quantile value if the distribution is skewed.

<!--add distribution of density and add the first quantile as the benchmark-->

```{r}
## To initialize effective bins along x
effective_bin1_scurve <- 12

effective_bin2_scurve <- calc_bins_y(
  bin1 = effective_bin1_scurve, 
  r2 = r2, 
  q = 0.07)$bin2

scurve_model <- fit_highd_model(
  training_data = training_data_scurve,
  emb_df = umap_scurve_scaled,
  bin1 = effective_bin1_scurve,
  r2 = r2,
  q = 0.07,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_scurve <- scurve_model$df_bin_centroids
df_bin_scurve <- scurve_model$df_bin

## To obtain the first quntile
benchmark1 <- round(quantile(df_bin_centroids_scurve$std_counts, names = FALSE)[2], 3)
```

```{r}
cell_count_scurve <- ggplot(df_bin_centroids_scurve, 
                            aes(x = reorder(as.factor(hexID), 
                                            -std_counts), 
                                y = std_counts)) + 
  geom_quasirandom(
    size = 0.3,
    alpha = 0.5
  ) + 
  geom_hline(yintercept = benchmark1, color = "black", linewidth=0.5, alpha = 0.5) +
  xlab("hexagon id (re-ordered)") + 
  ylab("standardized bin count") +
  ggtitle("(b)") +
  theme(aspect.ratio = 0.75,
        axis.title.x = element_text(size = 5),
        axis.title.y = element_text(size = 5, angle = 90),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 5),
        title = element_text(size = 36))
```

```{r}
#| echo: false
#| fig-cap: "Distribution of standardize counts by hexagons."
#| label: fig-stdcts-scurve
#| out-height: 50%
#| fig-pos: H
#| eval: false

cell_count_scurve
```

The benchmark value for removing low-density hexagons ranges between $0$ and $1$. When analyzing how these benchmark values influence model performance, it's essential to observe the change in MSE as the benchmark value increases (see @fig-mse-scurve-lwd). The MSE shows a gradual increase as the benchmark value progresses from $0$ to $1$. Evaluating this rate of increase is important. If the increment is not considerable, the decision might lean towards retaining low-density hexagons.

<!--add MSE vs density (0 to 1)-->

<!--To generate errors for different total number of bins-->
```{r}
## To initialize benchmark values to remove low density hexagons
benchmark_rm_hex_vec <- append(seq(0, 0.99, by=0.1), c(benchmark1, 0.99))

error_rm_scurve <- data.frame(matrix(nrow = 0, ncol = 0))

for (benchmark_rm_lwd in benchmark_rm_hex_vec) {
  
  df_bin_centroids_scurve_high_dens <- df_bin_centroids_scurve |>
    filter(std_counts > benchmark_rm_lwd)
  
  df_bin_scurve_high_dens <- df_bin_scurve |>
    filter(hb_id %in% df_bin_centroids_scurve_high_dens$hexID)

  ## Compute error
  error_df <- glance(
    df_bin_centroids = df_bin_centroids_scurve_high_dens,
    df_bin = df_bin_scurve_high_dens,
    training_data = training_data_scurve,
    newdata = NULL,
    type_NLDR = "UMAP",
    col_start = "x") |>
    mutate(benchmark_rm_lwd = benchmark_rm_lwd,
           bin1 = effective_bin1_scurve,
           bin2 = effective_bin2_scurve,
           b = bin1 * bin2,
           b_non_empty = NROW(df_bin_centroids_scurve_high_dens))

  error_rm_scurve <- bind_rows(error_rm_scurve, error_df)

}

benchmark_label_df <- tibble(x = 0.2, y = 0.1, 
                          label = paste0("benchmark is ", benchmark1))

mse_scurve_lwd <- ggplot(error_rm_scurve, 
                     aes(x = benchmark_rm_lwd, 
                         y = log(MSE))) + 
  geom_point(
    size = 0.6
    ) +
  geom_line(
    linewidth = 0.3
    ) + 
  geom_vline(xintercept = benchmark1, linetype="solid", 
             color = "black", linewidth=0.5, alpha = 0.5) +
  scale_x_continuous("standardized bin count", 
         transform = "reverse") + 
  ylab("log(MSE)") +
  ggtitle("(c)") +
  theme(aspect.ratio = 0.75,
        axis.text.x = element_text(size = 5),
        axis.text.y = element_text(size = 5),
        axis.title.x = element_text(size = 5),
        axis.title.y = element_text(size = 5, 
            angle=90),
        title = element_text(size = 72))
```

```{r}
#| fig-cap: "MSE from UMAP applied to S-curve dataset with diffferent bechmark choices. What is the effective bechmark value to remove low density hexagons? The residual plot have a steep slope at the end, indicating that a smaller bechmark value causes a small amount of MSE. Then, the slope gradually increases or level up, indicating that a higher bechmark value generates a higher MSE. Using the elbow method, it was observed that when the bechmark value is set to $0.242$, the lowest MSE occurred."
#| label: fig-mse-scurve-lwd
#| fig-pos: H
#| out-height: 50%
#| eval: false

mse_scurve_lwd
```

Furthermore, selecting the benchmark value for removing low-density hexagons is important. Removing unnecessary bins may lead to the formation of long edges and an uneven \gD{} model. Hence, rather than solely relying on the benchmark value to identify hexagons for removal, it's essential to consider the standard number of points in the neighboring hexagons of the identified low-density bins (see @fig-lwd-scurve (b)). If neighboring bins also show low counts, only those bins will be removed. The remaining bins are used to construct the \gD{} model.   

<!--Using first quantile as the benchmark value-->
```{r}
hex_grid_with_counts_s_curve2 <- full_join(
  hex_grid2, 
  df_bin_centroids_scurve |> select(hexID, std_counts), 
  by = c("hex_poly_id" = "hexID"))

hex_grid_with_low_counts <- full_join(hex_grid2, 
    df_bin_centroids_scurve, 
    by = c("hex_poly_id" = "hexID")) |>
  filter(!is.na(std_counts)) |>
  mutate(type = if_else(std_counts <= benchmark1, "low", "high")) |>
  filter(type == "low")

hex_grid_rm_lwd_scurve1 <-  ggplot(
  data = hex_grid_with_counts_s_curve2, 
  aes(x = x, y = y)) + 
  geom_polygon(aes(group = hex_poly_id, fill = std_counts), 
            color = "grey70", linewidth=0.2) +
  geom_text(data = hex_grid_with_low_counts, 
            color = "black", 
            aes(x = c_x, y = c_y, group = hex_poly_id, label = "X", size = 1)) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a")
```

<!--Observing neighboring bins as well-->
```{r}
## First define low density bins using first quantile
df_bin_centroids_low <- df_bin_centroids_scurve |> 
  filter(std_counts <= benchmark1)

## Check neighboring bins
remove_id <- find_low_dens_hex(df_bin_centroids_all = df_bin_centroids_scurve, 
                               bin1 = effective_bin1_scurve, 
                               df_bin_centroids_low = df_bin_centroids_low)

## Remove the identified bins
df_bin_centroids_scurve_removed <- df_bin_centroids_scurve |>
  filter(hexID %in%remove_id)

df_bin_centroids_scurve_keep  <- df_bin_centroids_scurve |>
  filter(!(hexID %in%remove_id))

hex_grid_with_low_counts_neighbor <- right_join(hex_grid, 
                                             df_bin_centroids_scurve_removed, 
                                             by = c("hex_poly_id" = "hexID"))

hex_grid_rm_lwd_scurve2 <-  ggplot(
  data = hex_grid_with_counts_s_curve2, 
  aes(x = x, y = y)) + 
  geom_polygon(aes(group = hex_poly_id, fill = std_counts), 
            color = "grey70", linewidth=0.2) +
  geom_text(data = hex_grid_with_low_counts_neighbor, 
            color = "black", 
            aes(x = c_x, y = c_y, group = hex_poly_id, label = "X", size = 1)) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b")
```

<!--
```{r}
#| echo: false
#| label: fig-lwd-scurve
#| fig-pos: H
#| fig-cap: "Two approaches to identify low-density hexagons. Hexagon grid (a) marks hexagons with a standardized count less than $0.25$. Hexagon grid (b) marks hexagons by considering the densities of neighboring bins. Removing hexagons by also investigating the densities of neighboring bins is more reliable for preserving the structure."
#| fig-width: 9
#| fig-height: 6

hex_grid_rm_lwd_scurve1 + hex_grid_rm_lwd_scurve2 +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
```
-->

#### Removing long edges

To create a smooth \gD{} representation (see @fig-NLDR-scurve (d)), it is necessary to remove edges that connect distant bin centroids in the triangular mesh. These edges only exist in the \gD{} model and do not extend into \pD{}, so their removal does not impact the model in \pD{}. Although there are no specific criteria for determining the benchmark value to remove long edges, the following steps provide an approach to identifying a suitable threshold:

1. Plot the distribution of the 2D Euclidean distances (see @fig-dist-scurve).
2. Identify the first largest difference between consecutive distance values.
3. Take the distance value corresponding to this difference as the benchmark value.

<!--distribution of distance along with the default benchmark-->
```{r}
## Triangulate bin centroids
tr1_object_scurve <- tri_bin_centroids(
  df_bin_centroids_scurve_keep, x = "c_x", y = "c_y")
tr_from_to_df_scurve <- gen_edges(
  tri_object = tr1_object_scurve)

## Compute 2D distances
distance_scurve <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_scurve, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_scurve <- find_lg_benchmark(
  distance_edges = distance_scurve, 
  distance_col = "distance")

benchmark_label_df <- tibble(y = benchmark_scurve - 0.03,
                             x = 1.3,
                             label = paste0("benchmark is ", benchmark_scurve))

## To draw the distance distribution
distance_scurve$group <- "1"
distance_scurve_plot <- ggplot(distance_scurve, 
                        aes(x = group, 
                            y = distance)) + 
  geom_quasirandom(
    size = 0.5, 
    alpha = 0.3
  ) + 
  ylim(0, max(unlist(distance_scurve$distance))+ 0.5) + 
  coord_flip() + 
  geom_hline(yintercept = benchmark_scurve,
     linetype="solid", 
     color = "black", linewidth=0.5, alpha = 0.5) +
  ylab(expression(d^{(2)})) +
  ggtitle("(d)") +
  theme(aspect.ratio = 0.75,
        axis.text.x = element_text(size = 5),
        axis.title.x = element_text(size = 5),
        title = element_text(size = 36))
```

```{r}
#| echo: false
#| fig-cap: "Various plots to help assess best number of bins, low density bin and large edge removal. Both (a) and (c) show MSE, against number of bins and standardised count. A good benchmark value for these parameters is when the MSE drops and then flattens out. Plot (b) shows the distribution of stadardised counts of hexagons. Plot (c) shows the distribution of $2\\text{-}D$ Euclidean distances between bin centroids, with a good benchmark value for removing large edges would being the distance that shows the first large increase."

#| label: fig-dist-scurve
#| out-width: 100%
#| fig-width: 12
#| fig-height: 10
#| fig-pos: H

mse_scurve_b + cell_count_scurve + 
mse_scurve_lwd + 
distance_scurve_plot + plot_layout(ncol=2)
```

<!--trimesh with different benchmark values-->

```{r}
## With calculated default: 0.201
trimesh_lg_scurve1 <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = benchmark_scurve, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos) 

trimesh_lg_scurve2 <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = 0.35, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos) 

trimesh_lg_scurve3 <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = 1.1, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos) 
```

```{r}
#| echo: false
#| label: fig-lg-scurve
#| fig-pos: H
#| fig-cap: "$2\\text{-}D$ model generated for different benchmark values to remove long edges: (a) benchmark = $0.201$ (default), (b) benchmark = $0.35$, and (c) benchmark = $1.1$. What is the effective benchmark value to remove long edges?"
#| fig-width: 6
#| fig-height: 3

trimesh_lg_scurve1 + trimesh_lg_scurve2 + trimesh_lg_scurve3 +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
```

## Best fit

Deciding on the best fit relies on several elements: 

- the choice of NLDR method, and the parameters used to create it, and
- model fit parameters: bin size, low density bin removal, long edge removal.

Comparing the MSE to obtain the best fit is suitable if one starts from the same NLDR representation. In theory, because the MSE is computed on \pD{} measuring the fit between model and data it might still be useful to compare different NLDR representations. A good NLDR representation should produce a good fit, producing a low MSE if the model fits the data well. However, it technically might be quite variable.  

The appropriate model for the S-curve using UMAP (n_neighbors: $15$) was created with $420$ bins. It was determined to be the best model after removing edges with a length greater than $0.134$. The decision was made considering that the model accurately captures the geometry of the S-curve. But better in some places only (@fig-scurve_sc_best).

In contrast, a poor model for the S-curve with PHATE was created using only $190$ bins and removing edges with a length greater than $0.3$. This model is inadequate as it has a higher MSE compared to the appropriate model. Visually, the model squeezed to the middle of the geometry of the S-curve (@fig-scurve-sc-bad).

XXX BEST FIT FOR S-CURVE MIGHT BE A DIFFERENT NLDR. 
SHOW THE SEVERAL SUB-OPTIMAL FITS.

<!--langevitour with best pD model for S-curve--> 
<!-- ::: {#fig-scurve-sc-best layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/scurve/sc_best_1.png) -->

<!-- ![](figures/scurve/sc_best_2.png) -->

<!-- ![](figures/scurve/sc_best_3.png) -->

<!-- Screen shots of the **langevitour** of the S-curve, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/4RFNENX_fXk>). MSE with $0.035$. -->
<!-- ::: -->

<!--langevitour with bad pD model for S-curve--> 
<!-- ::: {#fig-scurve-sc-bad layout-ncol="3" fig-pos="H"} -->
<!-- ![](figures/scurve/sc_bad_1.png) -->

<!-- ![](figures/scurve/sc_bad_2.png) -->

<!-- ![](figures/scurve/sc_bad_3.png) -->

<!-- Screen shots of the **langevitour** of the S-curve, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/DHPvhkZ0lUM>). MSE with $0.090$. -->
<!-- ::: -->



<!------>
<!--add more details regarding to the how the change happening with different number of bins-->

<!--best model-->
<!--need to mention why it is the best-->

<!--bad model-->
<!--need to mention why it is the bad-->
<!--Not represented the whole data structure because there are some discontinuity in the S-curve-->
<!-- Why? bad 

<!-- add MSE values for the screenshot figure names-->

## A curious difference between t-SNE and UMAP revealer

XXX THIS IS NOT INTERESTING IN ITS CURRENT FORM. THIS IS ONLY ANY INTERESTING EXAMPLE WHEN THE FILLED OUT VS FLAT SHAPES ARE DISCUSSED.

In this section, the effectiveness of the algorithm is described using a simulated dataset. The dataset consists of five spherical Gaussian clusters in $4\text{-}D$, with each cluster containing an equal number of points and the same within-cluster variation.

In the \gD{} layouts generated by various NLDR techniques, as shown in @fig-NLDR-variety-gau, five well-separated clusters are shown. In tSNE (see @fig-NLDR-variety-gau (a)), these clusters appear closely. UMAP arranges all clusters in a parallel manner, with three aligned in one line and the other two in a separate line (see @fig-NLDR-variety-gau (b)). In contrast, PHATE shows two closely positioned clusters and three more distant ones (see @fig-NLDR-variety-gau (c)). In TriMAP, two clusters are close, though not as tightly as PHATE, while the other three are well-separated (see @fig-NLDR-variety-gau (d)). Finally, PaCMAP shows one central cluster and the remaining four spread out in different directions (see @fig-NLDR-variety-gau (e)). 

<!-- five Gaussian clusters with different NLDR techniques-->
```{r}
#| warning: false
#| echo: false

## Import data
training_data_gau <- read_rds("data/five_gau_clusters/data_five_gau_training.rds")

tsne_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau_61.rds")
umap_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")
phate_data_gau <- read_rds("data/five_gau_clusters/phate_data_five_gau.rds")
trimap_data_gau <- read_rds("data/five_gau_clusters/trimap_data_five_gau.rds")
pacmap_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")

## Visualize embedding

nldr_gau1 <- tsne_data_gau |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.1, size=1) +
  interior_annotation("a")

nldr_gau2 <- umap_data_gau |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1) +
  interior_annotation("b")

nldr_gau3 <- phate_data_gau |>
  ggplot(aes(x = PHATE1,
             y = PHATE2)) +
  geom_point(alpha=0.1, size=1) +
  interior_annotation("c")

nldr_gau4 <- pacmap_data_gau |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2)) +
  geom_point(alpha=0.1, size=1) +
  interior_annotation("d")

nldr_gau5 <- trimap_data_gau |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2)) +
  geom_point(alpha=0.1, size=1) +
  interior_annotation("e")
```

```{r}
#| label: fig-NLDR-variety-gau
#| echo: false
#| fig-cap: "Five different NLDR representations of the same data. Different techniques and different parameter choices are used. Is there a best representation of the original data or are they all providing  equivalent information?"
#| fig-width: 8
#| fig-height: 2
#| out-width: 100%
#| fig-pos: H
# (a) tSNE (perplexity = 61), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2)
nldr_gau1 + nldr_gau2 + 
  nldr_gau3 + nldr_gau4 + nldr_gau5 + 
  plot_layout(ncol = 5)
```

```{r}
sc_xlims_gau <- c(-0.25, 1.25)
sc_ylims_gau <- c(-0.2, 1.2)
```

<!--trimesh with tsne-->
```{r}
gau1_scaled_obj <- gen_scaled_data(
  data = tsne_data_gau)
tsne_gau_scaled <- gau1_scaled_obj$scaled_nldr

## Compute hexbin parameters
num_bins_x_gau1 <- 12
lim1 <- gau1_scaled_obj$lim1
lim2 <- gau1_scaled_obj$lim2
r2_gau1 <- diff(lim2)/diff(lim1)

gau1_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = tsne_gau_scaled,
  bin1 = num_bins_x_gau1,
  r2 = r2_gau1,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau1 <- gau1_model$df_bin_centroids
df_bin_gau1 <- gau1_model$df_bin

## Triangulate bin centroids
tr1_object_gau1 <- tri_bin_centroids(
  df_bin_centroids_gau1, x = "c_x", y = "c_y")
tr_from_to_df_gau1 <- gen_edges(
  tri_object = tr1_object_gau1)

## Compute 2D distances
distance_gau1 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau1,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau1 <- find_lg_benchmark(
  distance_edges = distance_gau1,
  distance_col = "distance")

trimesh_removed_gau1 <- vis_rmlg_mesh(
  distance_edges = distance_gau1, 
  benchmark_value = benchmark_gau1, 
  tr_coord_df = tr_from_to_df_gau1, 
  distance_col = "distance") +
  xlim(sc_xlims_gau) + ylim(sc_ylims_gau) +
  interior_annotation("a", sc_ltr_pos) 
```

<!--trimesh with umap-->
```{r}
gau2_scaled_obj <- gen_scaled_data(
  data = umap_data_gau)
umap_gau_scaled <- gau2_scaled_obj$scaled_nldr

## Compute hexbin parameters
num_bins_x_gau2 <- 44
lim1 <- gau2_scaled_obj$lim1
lim2 <- gau2_scaled_obj$lim2
r2_gau2 <- diff(lim2)/diff(lim1)

gau2_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = umap_gau_scaled,
  bin1 = num_bins_x_gau2,
  r2 = r2_gau2,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau2 <- gau2_model$df_bin_centroids
df_bin_gau2 <- gau2_model$df_bin

## Triangulate bin centroids
tr1_object_gau2 <- tri_bin_centroids(
  df_bin_centroids_gau2, x = "c_x", y = "c_y")
tr_from_to_df_gau2 <- gen_edges(
  tri_object = tr1_object_gau2)

## Compute 2D distances
distance_gau2 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau2,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau2 <- find_lg_benchmark(
  distance_edges = distance_gau2,
  distance_col = "distance")

trimesh_removed_gau2 <- vis_rmlg_mesh(
  distance_edges = distance_gau2, 
  benchmark_value = benchmark_gau2, 
  tr_coord_df = tr_from_to_df_gau2, 
  distance_col = "distance") +
  xlim(sc_xlims_gau) + ylim(sc_ylims_gau) +
  interior_annotation("b", sc_ltr_pos) 
```

<!--trimesh with phate-->
```{r}
# gau3_scaled_obj <- gen_scaled_data(
#   data = phate_data_gau)
# phate_gau_scaled <- gau3_scaled_obj$scaled_nldr
# 
# ## Compute hexbin parameters
# num_bins_x_gau3 <- 250
# lim1 <- gau3_scaled_obj$lim1
# lim2 <- gau3_scaled_obj$lim2
# r2_gau3 <- diff(lim2)/diff(lim1)
# 
# gau3_model <- fit_highd_model(
#   training_data = training_data_gau,
#   emb_df = phate_gau_scaled,
#   bin1 = num_bins_x_gau3,
#   r2 = r2_gau3,
#   is_bin_centroid = TRUE,
#   is_rm_lwd_hex = FALSE,
#   col_start_highd = "x"
# )
# 
# df_bin_centroids_gau3 <- gau3_model$df_bin_centroids
# df_bin_gau3 <- gau3_model$df_bin

df_bin_centroids_gau3 <- read_rds("data/five_gau_clusters/df_bin_centroids_gau_phate.rds")
df_bin_gau3 <- read_rds("data/five_gau_clusters/df_bin_gau_phate.rds")

## Triangulate bin centroids
tr1_object_gau3 <- tri_bin_centroids(
  df_bin_centroids_gau3, x = "c_x", y = "c_y")
tr_from_to_df_gau3 <- gen_edges(
  tri_object = tr1_object_gau3)

## Compute 2D distances
distance_gau3 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau3,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau3 <- find_lg_benchmark(
  distance_edges = distance_gau3,
  distance_col = "distance")

trimesh_removed_gau3 <- vis_rmlg_mesh(
  distance_edges = distance_gau3, 
  benchmark_value = benchmark_gau3, 
  tr_coord_df = tr_from_to_df_gau3, 
  distance_col = "distance") +
  xlim(sc_xlims_gau) + ylim(sc_ylims_gau) +
  interior_annotation("c", sc_ltr_pos) 
```

<!--trimesh with pacmap-->
```{r}
gau4_scaled_obj <- gen_scaled_data(
  data = pacmap_data_gau)
pacmap_gau_scaled <- gau4_scaled_obj$scaled_nldr

## Compute hexbin parameters
num_bins_x_gau4 <- 18
lim1 <- gau4_scaled_obj$lim1
lim2 <- gau4_scaled_obj$lim2
r2_gau4 <- diff(lim2)/diff(lim1)

gau4_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = pacmap_gau_scaled,
  bin1 = num_bins_x_gau4,
  r2 = r2_gau4,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau4 <- gau4_model$df_bin_centroids
df_bin_gau4 <- gau4_model$df_bin

## Triangulate bin centroids
tr1_object_gau4 <- tri_bin_centroids(
  df_bin_centroids_gau4, x = "c_x", y = "c_y")
tr_from_to_df_gau4 <- gen_edges(
  tri_object = tr1_object_gau4)

## Compute 2D distances
distance_gau4 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau4,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau4 <- find_lg_benchmark(
  distance_edges = distance_gau4,
  distance_col = "distance")

trimesh_removed_gau4 <- vis_rmlg_mesh(
  distance_edges = distance_gau4, 
  benchmark_value = benchmark_gau4, 
  tr_coord_df = tr_from_to_df_gau4, 
  distance_col = "distance") +
  xlim(sc_xlims_gau) + ylim(sc_ylims_gau) +
  interior_annotation("d", sc_ltr_pos) 
```

<!--trimesh with trimap-->
```{r}
gau5_scaled_obj <- gen_scaled_data(
  data = trimap_data_gau)
trimap_gau_scaled <- gau5_scaled_obj$scaled_nldr

## Compute hexbin parameters
num_bins_x_gau5 <- 25
lim1 <- gau5_scaled_obj$lim1
lim2 <- gau5_scaled_obj$lim2
r2_gau5 <- diff(lim2)/diff(lim1)

gau5_model <- fit_highd_model(
  training_data = training_data_gau,
  emb_df = trimap_gau_scaled,
  bin1 = num_bins_x_gau5,
  r2 = r2_gau5,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "x"
)

df_bin_centroids_gau5 <- gau5_model$df_bin_centroids
df_bin_gau5 <- gau5_model$df_bin

## Triangulate bin centroids
tr1_object_gau5 <- tri_bin_centroids(
  df_bin_centroids_gau5, x = "c_x", y = "c_y")
tr_from_to_df_gau5 <- gen_edges(
  tri_object = tr1_object_gau5)

## Compute 2D distances
distance_gau5 <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_gau5,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_gau5 <- find_lg_benchmark(
  distance_edges = distance_gau5,
  distance_col = "distance")
benchmark_gau5 <- 0.2 

trimesh_removed_gau5 <- vis_rmlg_mesh(
  distance_edges = distance_gau5, 
  benchmark_value = benchmark_gau5, 
  tr_coord_df = tr_from_to_df_gau5, 
  distance_col = "distance") +
  xlim(sc_xlims_gau) + ylim(sc_ylims_gau) +
  interior_annotation("e", sc_ltr_pos) 
```

<!--add trimesh for all five-->

```{r}
#| label: fig-trimesh-gau
#| echo: false
#| fig-cap: "Model generated with five different NLDR methods in $2\\text{-}D$ with approximately $65$ non-empty bins in each."
#| fig-width: 8
#| fig-height: 2
#| out-width: 100%
#| fig-pos: H

trimesh_removed_gau1 + trimesh_removed_gau2 +
  trimesh_removed_gau3 + trimesh_removed_gau4 + trimesh_removed_gau5 +
  plot_layout(ncol = 5)
```

<!--add three screenshots from each technique with you tube links-->

To investigate which is the reasonable representation to visualize the five spherical Gaussian cluster data or all NLDR methods provide equivalent information, we visualize all the models in \pD{} space. Models from all NLDR methods show five well-separated clusters (see @fig-gau1-sc, @fig-gau2-sc, @fig-gau3-sc, @fig-gau4-sc, and @fig-gau5-sc). This suggests that for the five Gaussian cluster dataset, all NLDR methods effectively preserve the global structure. tSNE displays clusters with varying densities, indicating their ability to capture within-cluster variation (see @fig-gau1-sc). On the other hand, both UMAP, PHATE, PaCMAP and TriMAP show clusters with flat surfaces, suggesting a failure to capture within-cluster variation (see @fig-gau2-sc, @fig-gau3-sc, @fig-gau4-sc and @fig-gau5-sc). Therefore, UMAP, PHATE, PaCMAP and TriMAP do not capture the local structure as effectively as other methods.

::: {#fig-gau1-sc layout-ncol="3" fig-pos="H"}
![](figures/five_gau_clusters/sc_tsne_1.png)

![](figures/five_gau_clusters/sc_tsne_2.png)

![](figures/five_gau_clusters/sc_tsne_3.png)

Screen shots of the **langevitour** of the five Gaussian clusters dataset, shows the model with tSNE in \pD{}, a video of the tour animation is available at (<https://youtu.be/RASEE7N5MbM>).
:::

::: {#fig-gau2-sc layout-ncol="3" fig-pos="H"}
![](figures/five_gau_clusters/sc_umap_1.png)

![](figures/five_gau_clusters/sc_umap_2.png)

![](figures/five_gau_clusters/sc_umap_3.png)

Screen shots of the **langevitour** of the five Gaussian clusters dataset, shows the model with UMAP in \pD{}, a video of the tour animation is available at (<https://youtu.be/iG4bCPkJilw>).
:::

::: {#fig-gau3-sc layout-ncol="3" fig-pos="H"}
![](figures/five_gau_clusters/sc_phate_1.png)

![](figures/five_gau_clusters/sc_phate_2.png)

![](figures/five_gau_clusters/sc_phate_3.png)

Screen shots of the **langevitour** of the five Gaussian clusters dataset, shows the model with PHATE in \pD{}, a video of the tour animation is available at (<https://youtu.be/L_PVLGwfOS0>).
:::

::: {#fig-gau4-sc layout-ncol="3" fig-pos="H"}
![](figures/five_gau_clusters/sc_pacmap_1.png)

![](figures/five_gau_clusters/sc_pacmap_2.png)

![](figures/five_gau_clusters/sc_pacmap_3.png)

Screen shots of the **langevitour** of the five Gaussian clusters dataset, shows the model with PaCMAP in \pD{}, a video of the tour animation is available at (<https://youtu.be/z07cKXi8EJQ>).
:::

::: {#fig-gau5-sc layout-ncol="3" fig-pos="H"}
![](figures/five_gau_clusters/sc_trimap_1.png)

![](figures/five_gau_clusters/sc_trimap_2.png)

![](figures/five_gau_clusters/sc_trimap_3.png)

Screen shots of the **langevitour** of the five Gaussian clusters dataset, shows the model with TriMAP in \pD{}, a video of the tour animation is available at (<https://youtu.be/Chs1lYAoX2w>).
:::

When compare the NLDR representations and generated models, tSNE with perplexity $61$ appears to be a reasonable representation for visualizing the five Gaussian cluster dataset. This is supported by investigating the model generated with tSNE in the data space, which provides evidence that it preserves both local and global structures. Also, the NLDR representation with tSNE shows five well-separated clusters.

## Applications {#sec-applications}

### Single-cell gene expression
<!--
- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use our method to get a better representation
-->

In the field of single-cell studies, a common analytical task involves clustering to identify groups of cells with similar expression profiles. Analysts often turn to NLDR techniques to verify and identify these clusters and explore developmental trajectories. To illustrate the importance of NLDR techniques and parameter selection in identifying clusters, Human Peripheral Blood Mononclear Cells (PBMC3k) dataset [@chen2023] is used. In a study by @chen2023, this dataset was used to demonstrate how UMAP represents clusters (see @fig-umap-author). As shown in @fig-umap-author, there are three distant and well-separated clusters. 

<!-- UMAP layout with author's suggested parameter choice-->
```{r}
## Import data
training_data_pbmc <- read_rds("data/pbmc3k/pbmc_pca_50.rds")
training_data_pbmc <- training_data_pbmc[, 1:9] |>
  mutate(ID = 1:NROW(training_data_pbmc))

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc)
umap_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

umap_pbmc <- umap_pbmc_scaled |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.5) 
```

<!-- UMAP parameters suggested by the author (n_neighbors = 30, min_dist = 0.3)-->
```{r}
#| fig-cap: "$2\\text{-}D$ layout from UMAP applied for the PBMC3k dataset. Is this a best representation of the original data? The parameter setting is n_neighbors = 30, min_dist = 0.3."
#| label: fig-umap-author
#| out-height: 30%
#| fig-pos: H

umap_pbmc 
```

To determine whether the UMAP representation with the parameter choice suggested by @chen2023 preserves the original data structure, we visualize the model constructed with UMAP overlaid on the \pD{} data. The figures in @fig-pbmc1-sc show three well-separated clusters, indicating that the suggested UMAP representation preserves the global structure (see @fig-umap-author). However, as shown in @fig-pbmc1-sc, these clusters are close to each other in \pD{}. Also, non-linear continuity patterns and high-density patches within the clusters are observed (see @fig-pbmc1-sc). Therefore, the suggested UMAP representation (see @fig-umap-author) does not accurately preserve the local structure of the PBMC3k dataset.  

<!--Fit the best model for author suggestion and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_pbmc <- 30
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC_"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

trimesh_removed_pbmc <- vis_rmlg_mesh(
  distance_edges = distance_pbmc,
  benchmark_value = benchmark_pbmc,
  tr_coord_df = tr_from_to_df_pbmc,
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "PC_")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    .default = "error greter than 20"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error greter than 20")))

## To join embedding
error_df <- error_df |>
  bind_cols(umap_pbmc_scaled |>
              select(-ID))

error_plot_pbmc <- error_df |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

<!--bin1 = 30, bin2 = 29, b = 870, non_empty = 132-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in $2\\text{-}D$ with UMAP, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows three well-separated distant clusters. The $p\\text{-}D$ model errors are distributed along clusters."
#| label: fig-model-pbmc-author
#| fig-pos: H

trimesh_removed_pbmc + error_plot_pbmc + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```

::: {#fig-pbmc1-sc layout-ncol="3" fig-pos="H"}
![](figures/pbmc3k/sc_1.png)

![](figures/pbmc3k/sc_2.png)

![](figures/pbmc3k/sc_3.png)

Screen shots of the **langevitour** of the PBMC3k data set, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/VqqWuE0Jj6A>).
:::

In order to find a reasonable NLDR representation for the PBMC3k dataset, we calculated the absolute error for different numbers of non-empty bins using various NLDR techniques and different parameter settings (see @fig-pbmc-abserror). After analyzing the results, we found that tSNE with a perplexity set to $30$ had the lowest error when the number of non-empty bins was $137$. Therefore, tSNE with a perplexity of $30$, which is the default parameter setting, is considered as a reasonable representation for the PBMC3k dataset.

<!--compute absolute error for different parameter choices-->
```{r}
error_df_umap <- read_csv("data/pbmc3k/error_df_umap.csv", col_names = FALSE)
names(error_df_umap) <- c("error", "mse", "bin1", "bin2", "b", "b_non_empty", 
                     "method", "n_neighbors", "min_dist")

error_df_umap <- error_df_umap |>
  mutate(type = paste0("n_neighbors: ", n_neighbors, ", min_dist: ", min_dist)) |>
  select(-c(n_neighbors, min_dist))

error_df_tsne <- read_csv("data/pbmc3k/error_df_tsne.csv", col_names = FALSE)
names(error_df_tsne) <- c("error", "mse", "bin1", "bin2", "b", "b_non_empty", 
                          "method", "perplexity")

error_df_tsne <- error_df_tsne |>
  mutate(type = paste0("perplexity: ", perplexity)) |>
  select(-perplexity)

error_df <- bind_rows(error_df_umap, error_df_tsne)

error_plot_pbmc <- ggplot(error_df, 
                              aes(x = b_non_empty, 
                                  y = log(error), 
                                  group = type, 
                                  colour = type)) + 
  geom_point(size = 1) +
  geom_line() + 
  geom_vline(xintercept = 137, linetype="solid", 
             color = "black", linewidth=0.8, alpha = 0.5) +
  scale_color_manual(values=c('#e41a1c','#377eb8','#4daf4a','#984ea3',
                              '#ff7f00','#a6cee3','#a65628','#f781bf')) +
  ylab("log(absolute error)") +
  xlab("number of non-empty bins") +
  theme(axis.text.x = element_text(size = 5),
        axis.text.y = element_text(size = 5),
        axis.title.x = element_text(size = 5),
        axis.title.y = element_text(size = 5, angle = 90))

```

```{r}
#| fig-cap: "Absolute error from UMAP and tSNE applied to training PBMC3k dataset with diffferent parameter choices. What is the best parameter choice to create the model? The residual plot have a steep slope at the beginning, indicating that a smaller number of non-empty bins causes a larger amount of error. Then, the slope gradually declines or level off, indicating that a higher number of non-empty bins generates a smaller error. Using the elbow method, it was observed that when the number of non-empty bins is set to $137$, the lowest error occurred with the parameters perplexity: $30$."
#| label: fig-pbmc-abserror
#| fig-pos: H
#| out-width: 50%

error_plot_pbmc
```

As shown in @fig-tsne-suggest, there are three well-separated clusters, although they are located close to each other. Additionally, non-linear structures can also be observed within the clusters (see @fig-model-pbmc (a)). In this manner, tSNE was able to capture the data structure for the PBMC3k dataset that UMAP failed to do.

<!--best choice-->

```{r}
tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = tsne_pbmc)
tsne_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

tsne_pbmc <- tsne_pbmc_scaled |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.5)
```

<!--tSNE with perplexity:30-->
```{r}
#| echo: false
#| fig-cap: "$2\\text{-}D$ layout from tSNE applied for the PBMC3k dataset.  Is this a best representation of the original data? The parameter setting is perplexity=30."
#| label: fig-tsne-suggest
#| fig-pos: H
#| out-width: 40%

tsne_pbmc 
```

We then fit the model for tSNE, and visualize the resultant model in the \pD{} data space. The model shows a quirk, as shown in @fig-pbmc2-sc. All three clusters are connected by an edge except the small and large clusters. Because the clusters are so close in \gD{}, they attempt to maintain the structure in \pD{} as well. This is evident that tSNE with perplexity $30$ provides a reasonable representation of PBMC3k data.

<!--Fit the best model and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_pbmc <- 15
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = tsne_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  r2 = r2_pbmc,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC_"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")
benchmark_pbmc <- 0.1

trimesh_removed_pbmc <- vis_rmlg_mesh(
  distance_edges = distance_pbmc,
  benchmark_value = benchmark_pbmc,
  tr_coord_df = tr_from_to_df_pbmc,
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "tSNE",
  col_start = "PC_")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    row_wise_abs_error <= 25 ~ "error 20-25",
    .default = "error greter than 25"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error 20-25", 
    "error greter than 25")))

## To join embedding
error_df <- error_df |>
  bind_cols(tsne_pbmc_scaled |>
              select(-ID))

error_plot_pbmc <- error_df |>
  ggplot(aes(x = tSNE1,
             y = tSNE2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

<!--bin1 = 15, bin2 = 20, b = 300, non_empty = 137-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in $2\\text{-}D$ with tSNE, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows three well-separated distant clusters. The $p\\text{-}D$ model errors are distributed along clusters, but most low $p\\text{-}D$ model errors present in the large cluster."
#| label: fig-model-pbmc
#| fig-pos: H
#| out-height: 30%

trimesh_removed_pbmc + error_plot_pbmc + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```

::: {#fig-pbmc2-sc layout-ncol="3" fig-pos="H"}
![](figures/pbmc3k/sc_4.png)

![](figures/pbmc3k/sc_5.png)

![](figures/pbmc3k/sc_6.png)

Screen shots of the **langevitour** of the PBMC3k data set, shows the model in high-D, a video of the tour animation is available at (<https://youtu.be/5Y1hE4i7N2k>).
:::


### Hand-written digits
<!--
- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 
-->

The MNIST dataset consists of grayscale images of handwritten digits [@lecun2010]. @yingfan2021 used this dataset to demonstrate how PaCMAP preserves local structure. We selected the \gD{} embedding of PaCMAP for the handwritten digit 1 to assess whether this is a reasonable representation using our method. As shown in @fig-pacmap-author-img, the angle of the digit 1 images varies along the \gD{} structure.

<!-- PaCMAP layout with author's suggested parameter choice-->
```{r}
## Import data
training_data_mnist <- read_rds("data/mnist/mnist_10_pcs_of_digit_1.rds")
training_data_mnist <- training_data_mnist |>
  mutate(ID = 1:NROW(training_data_mnist))

pacmap_minst <- read_rds("data/mnist/mnist_pacmap.rds") |> 
  select(PaCMAP1, PaCMAP2, ID)
mnist_scaled_obj <- gen_scaled_data(
  data = pacmap_minst)
pacmap_minst_scaled <- mnist_scaled_obj$scaled_nldr 

position_df <- pacmap_minst_scaled |> 
  filter(ID %in% c(6475, 3311, 6489, 2347, 387, 
                          5239, 728, 517, 6369, 3055, 6345, 1761)) |>
  mutate(position = c(5, 9, 7, 8, 4, 11, 2, 6, 12, 10, 1, 3))

pacmap_plot_mnist <- pacmap_minst_scaled |> 
  ggplot(aes(x = PaCMAP1, 
             y = PaCMAP2)) + 
  geom_point(alpha=0.1) #+ 
  # geom_text(data = position_df, 
  #           aes(x = PaCMAP1, y = PaCMAP2, label = position), 
  #           colour = "red", 
  #           size = 5)  
```

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->
```{r}
#| fig-cap: "$2\\text{-}D$ layout from PaCMAP applied for the digit 1 of the MNIST dataset. Is this the best representation of the digit 1? The parameter setting is n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0."
#| label: fig-pacmap-author
#| fig-pos: H
#| out-height: 22%

pacmap_plot_mnist
```

<!--Render the images in different selected location in 2D layout-->
```{r}
## Data with pixel values
mnist_data <- read_rds("data/mnist/mnist_digit_1.rds")

pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% c(3710, 2391, 2385, 5030, 6475,
                         7679, 3568, 1312, 3311, 1097,
                         3552, 7853, 6489, 7689, 6690,
                        1380, 6057, 2347, 5946, 3355,
                        4175, 3997, 5378, 387, 1854,
                        614, 3079, 1762, 5239, 3723,
                        5748, 728, 7419, 7794, 6233,
                        33, 2485, 5998, 318, 1761,
                        5690, 165, 517, 6935, 2682,
                        4962, 2264, 5563, 6369, 559,
                        5188, 4849, 2666, 3448, 3055,
                        3120, 6869, 6345, 4470, 7147)) |>
  mutate(instance.labs =  case_when(
    instance %in% c(3710, 2391, 2385, 5030, 6475) ~ "1",
    instance %in% c(7679, 3568, 1312, 3311, 1097) ~ "2",
    instance %in% c(3552, 7853, 6489, 7689, 6690) ~ "3",
    instance %in% c(1380, 6057, 2347, 5946, 3355) ~ "4",
    instance %in% c(4175, 3997, 5378, 387, 1854) ~ "5",
    instance %in% c(614, 3079, 1762, 5239, 372) ~ "6",
    instance %in% c(5748, 728, 7419, 7794, 6233) ~ "7",
    instance %in% c(5690, 165, 517, 6935, 2682) ~ "8",
    instance %in% c(4962, 2264, 5563, 6369, 559) ~ "9",
    instance %in% c(5188, 4849, 2666, 3448, 3055) ~ "10",
    instance %in% c(3120, 6869, 6345, 4470, 7147) ~ "11",
    .default = "12"
    )) 

## To generate labels
instance.labs <- as.character(rep(1:12, each = 5))

img_sample <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +
  facet_wrap(~ factor(instance, levels = c(3710, 2391, 2385, 5030, 6475,
                         7679, 3568, 1312, 3311, 1097,
                         3552, 7853, 6489, 7689, 6690,
                        1380, 6057, 2347, 5946, 3355,
                        4175, 3997, 5378, 387, 1854,
                        614, 3079, 1762, 5239, 3723,
                        5748, 728, 7419, 7794, 6233,
                        33, 2485, 5998, 318, 1761,
                        5690, 165, 517, 6935, 2682,
                        4962, 2264, 5563, 6369, 559,
                        5188, 4849, 2666, 3448, 3055,
                        3120, 6869, 6345, 4470, 7147)), nrow = 5) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(), 
        legend.position = "none")
```

```{r}
#| fig-cap: "Images of the handwritten digit 1 are ordered from the bottom-right to the top-left of the $2\\text{-}D$ structure. The angle of the digit varies along this structure. Images at the bottom-right of the $2\\text{-}D$ layout show the digit 1 angled more to the right, while images at the top-left show the digit 1 angled more to the left. This demonstrates how the angle changes from right to left along the $2\\text{-}D$ structure."
#| label: fig-pacmap-author-img
#| fig-pos: H
#| out-height: 50%

img_sample
```

<!-- Fit the model and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_mnist <- 37
lim1 <- mnist_scaled_obj$lim1
lim2 <- mnist_scaled_obj$lim2
r2_mnist <- diff(lim2)/diff(lim1) 

mnist_model <- fit_highd_model(
  training_data = training_data_mnist,
  emb_df = pacmap_minst_scaled,
  bin1 = num_bins_x_mnist,
  r2 = r2_mnist,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC"
)

df_bin_centroids_mnist <- mnist_model$df_bin_centroids
df_bin_mnist <- mnist_model$df_bin

## Triangulate bin centroids
tr1_object_mnist <- tri_bin_centroids(
  df_bin_centroids_mnist, x = "c_x", y = "c_y")
tr_from_to_df_mnist <- gen_edges(
  tri_object = tr1_object_mnist)

## Compute 2D distances
distance_mnist <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_mnist, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_mnist <- find_lg_benchmark(
  distance_edges = distance_mnist, 
  distance_col = "distance")

sc_ltr_pos_mnist <- c(0.96, 0.96)

trimesh_removed_mnist <- vis_rmlg_mesh(
  distance_edges = distance_mnist, 
  benchmark_value = benchmark_mnist, 
  tr_coord_df = tr_from_to_df_mnist, 
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos_mnist)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_mnist, 
  df_bin = df_bin_mnist, 
  training_data = training_data_mnist, 
  newdata = NULL, 
  type_NLDR = "PaCMAP", 
  col_start = "PC") 

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 2 ~ "error 0-2",
    row_wise_abs_error <= 4 ~ "error 2-4",
    row_wise_abs_error <= 6 ~ "error 4-6",
    row_wise_abs_error <= 6 ~ "error 6-8",
    row_wise_abs_error <= 10 ~ "error 8-10",
    .default = "error greter than 10"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-2", "error 2-4", "error 4-6", "error 6-8", "error 8-10", 
    "error greter than 10")))

## To join embedding
error_df <- error_df |>
  bind_cols(pacmap_minst_scaled |> 
              select(-ID))
  
error_plot_mnist <- error_df |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2, 
             color = type, 
             group = ID)) +
  geom_point(alpha=0.5, 
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos_mnist)
```

<!--bin1 = 37, bin2 = 26, b = 962, non_empty = 398-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in $2\\text{-}D$, and (b) $p\\text{-}D$ model error in $2\\text{-}D$. The $2\\text{-}D$ model shows a non-linear continuous structure. Most low $p\\text{-}D$ model errors are distributed along the lower edge of the $2\\text{-}D$ structure, while most high p-D model errors are concentrated along the upper edge."
#| label: fig-model-mnist
#| fig-pos: H

trimesh_removed_mnist + error_plot_mnist + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```

According to @fig-mnist1-sc1, the non-linear continuous structure observed in the \gD{} representation of PaCMAP (see @fig-pacmap-author) is also visible when visualizing the model overlaid on the data space. This indicates that PaCMAP accurately captures the structure of the \pD{} data. Additionally, the model shows a twisted pattern within the non-linear structure in \pD{} space (see @fig-mnist1-sc2), which is an additional pattern not visible in the \gD{} representation (see @fig-pacmap-author). Furthermore, as shown in @fig-mnist1-sc3, some long edges exist in the \pD{} space that are not recognized as long edges in the \gD{} representation. However, PaCMAP is a reasonable \gD{} representation of MNIST digit 1 data. Because PaCMAP preserves the local structure. 

<!--add langevitour screenshots and youtube animation link-->

::: {#fig-mnist1-sc layout-ncol="3" fig-pos="H"}
![](figures/mnist/sc_1.png){#fig-mnist1-sc1}

![](figures/mnist/sc_2.png){#fig-mnist1-sc2}

![](figures/mnist/sc_3.png){#fig-mnist1-sc3}

Screen shots of the **langevitour** of the MNIST digit 1 data set, shows the model-in-data space, a video of the tour animation is available at (<https://youtu.be/zcg_GXBmqjA>).
:::
<!-- need to update the youtube recording-->

There are certain data points that exhibit high error rates due to their deviation from the usual \pD{} data structure, which makes them anomalies (see @fig-model-mnist (b)). These anomalies can be classified into two types: those that are anomalies within the non-linear structure and those that lie outside of it. The images associated with high model error points within the non-linear structure display different patterns of the digit 1, as shown in @fig-mnist-anomalies (a). However, when comparing these images to the ones found outside of the non-linear structure, it becomes evident that the latter display different patterns of the digit 1 (see @fig-mnist-anomalies (b)). 


<!--images that occur large error-->
<!-- within the nonlinear structure-->
```{r}
pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% c( 4699, 4785, 5173, 6671, 2186, 4636, 4882, 
                          773, 2825, 6169, 6948, 5987, 1282, 7715, 
                          622, 2989, 6912, 4370, 1227, 4042, 2444, 
                          836, 1654)) 

imge_error_sample_within <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +  
  facet_wrap(~ factor(instance, levels = c( 4699, 4785, 5173, 6671, 2186, 4636, 
                                            4882, 773, 2825, 6169, 6948, 5987, 
                                            1282, 7715, 622, 2989, 6912, 4370, 
                                            1227, 4042, 2444, 836, 1654)), 
             ncol = 4) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")
```

<!--outside the nonlinear structure-->
```{r}
img_error <- error_df |> 
  filter(row_wise_abs_error >= 8) |> 
  filter((!ID %in% c(4699, 4785, 5173, 6671, 2186, 4636, 4882, 773, 
                            2825, 6169, 6948, 5987, 1282, 7715, 622, 2989, 
                            6912, 4370, 1227, 4042, 2444, 836, 1654))) |>
  pull(ID)

pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_error) 

imge_error_sample <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +  
  facet_wrap(~ instance, ncol = 2) +
  coord_fixed() +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none") 
```

```{r}
#| echo: false
#| fig-cap: "Some images of handwritten digit 1 which occur high model error (a) within the non-linear strcuture, and (b) outside the non-linear structure. The images shows different patterns of digit 1."
#| label: fig-mnist-anomalies
#| out-height: 35%
#| fig-pos: H

imge_error_sample_within + imge_error_sample +
  plot_layout(ncol=2) +
  plot_annotation(tag_levels = 'a')
```

     
## Discussion {#sec-discussion}

<!-- - Summarise contributions -->
<!-- - Explain where it is expected or not expected to work, eg higher dimensional relationships -->
<!-- - Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views (need to add) -->
<!-- - Predicting new observations in $k$-D -->
<!-- - Extending layouts beyond $k$-D, when 2D is clearly inadequate. -->
<!-- - Diagnostic app to explore differences in distances (need to add) -->
<!-- - What might be useful enhancements -->


This study makes several important contributions to the field of NLDR. We have developed an algorithm to evaluate the most useful NLDR method and (hyper-)parameter choices for creating a reasonable \gD{} layout of high-dimensional data. Our objective is to fit a model for the \gD{} layout that preserves the relationships between neighboring points and turns it into a high-dimensional wireframe, which can be overlaid on the data and visualized using a tour. This approach is defined as *model-in-data-space*. Viewing a model in the data space is an ideal way to examine the fit.

The effectiveness of this approach is illustrated through various examples. For instance, the S-curve example demonstrates how the model accurately fits the points, capturing both local and global structures in high-dimensional space. Our simulation case study further, five Gaussian cluster example shows that while all observed NLDR methods preserve the global structure, only tSNE effectively maintains the local structure, highlighting the specific strengths and quirks of different methods.

Human behavior often shows a desire for more certainty and a tendency to prefer well-separated views. This emphasizes the importance of clear and distinct clusters. For example, in the UMAP layout of the **pbmc** dataset suggested by @chen2023, three distant, well-separated clusters are shown. However, our model reveals that these clusters are actually close to each other in \pD{}. Additionally, the model discovers non-uniform data distribution and non-linear structures within the clusters that are not visible in the UMAP layout, demonstrating the ability of our model in uncovering hidden data characteristics.

Evaluating the error or unexplained variance is important for assessing how well the model fits the data. By examining the error for different numbers of bins, we found that tSNE with a perplexity of $30$ provides a reasonable representation for the **pbmc** dataset. Connecting the closest clusters with line segments in the fitted model further supports the preservation of neighborhood relationships.

The **digit: 1** example further illustrates the model's ability to accurately capture non-linear structures and provide additional information. Key findings include a twisted pattern that compresses the structure in some projections and long line segments that detect anomalies.

Predicting new observations in \kD{} is particularly valuable due to the limitations of some NLDR methods, like tSNE, which don't provide a straightforward method for prediction. As a result, our approach offers a solution that capable of generating predicted \kD{} embedding regardless of the NLDR method employed, effectively addressing this functional gap.

In conclusion, while our method effectively captures and represents high-dimensional data structures, further enhancements could involve introducing approaches to bind the data, indicate line segments beyond \gD{}, and diagnose the fitted model. These improvements would help in creating a more accurate representation of the data when \gD{} layout is inadequate.

## Supplementary Materials

Code, and data for reproducing this paper are available at [https://github.com/JayaniLakshika/paper-nldr-vis-algorithm](https://github.com/JayaniLakshika/paper-nldr-vis-algorithm).

  
## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
