---
title: "Visualising How Non-linear Dimension Reduction Warps Your Data"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Non-Linear Dimension Reduction (NLDR) techniques have emerged as powerful tools to visualize high-dimensional data. However, their complexity and parameter choices may lead to distrustful or misleading results. To address this challenge, we propose a novel approach that combines the tour technique with a low-dimensional manifold generated using NLDR techniques, hexagonal binning, and triangulation. This integration enables a clear examination of the low-dimensional representation in the original high-dimensional space. Our approach not only preserves the advantages of both tours and NLDR but also offers a more intuitive perception of complex data structures and facilitates accurate data transformation assessments. The method and example data sets are available in the **quollr** R package.
  
keywords: [high-dimensional data, dimension reduction, triangulation, hexagonal binning, low-dimensional manifold, manifold learning, tour, data vizualization]
keywords-formatted: [high-dimensional data, dimension reduction, triangulation, hexagonal binning, low-dimensional manifold, manifold learning, tour, data vizualization]

bibliography: bibliography.bib  
---
  
```{r}
#| warning: false
#| echo: false
library(dplyr)
# remotes::install_github("jlmelville/snedata")
library(snedata)
library(ggflowchart)
library(purrr) ## map function
library(gridExtra) ## for grid.arrange
library(rsample)
library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)

library(Rtsne)
library(umap)
library(phateR)
library(patchwork)
library(langevitour)
library(colorspace)

library(grid)


# install.packages("Seurat")
# remotes::install_github("satijalab/seurat-data")
#library(SeuratData) ## For the application

# use_python("~/miniforge3/envs/pcamp_env/bin/python")
# use_condaenv("pcamp_env")
# 
# reticulate::source_python(paste0(here::here(), "/examples_for_the_paper/function_scripts/Fit_PacMAP_code.py"))
# reticulate::source_python(paste0(here::here(), "/examples_for_the_paper/function_scripts/Fit_TriMAP_code.py"))

set.seed(20240110)

source("quollr_code.R", local = TRUE)
source("nldr_code.R", local = TRUE)
```

```{=html}
<!-- 
  Notes
* Use American spelling
-->
```

## Introduction {#sec-intro}

High-dimensional (high-D) data is prevalent across various fields, such as ecology and bioinformatics [@Guo2023], due to advancements in data collection technologies [@Johnstone2009; @ayesha2020overview]. However, visualization of high-D data introduces significant challenges, because the complexity of visualizing data beyond two dimensions [@Jia2022]. In recent years, interactive and dynamic graphics systems like **liminal** [@article21] —which employs interactive tools like brushing and linking [@article58]—and software tools such as **XGobi**, **GGobi** [@article60], **tourr**  [@article61], **detourr** [@article22], and **langevitour**  [@article09], involving dynamic methods like tours [@Asimov1985], have played a key role in visualizing high-D data (data-vis).

To create low-dimensional representations (typically in 2D) of high-D data, it is common to apply dimension reduction (DR) techniques. Approaches for DR involve linear methods such as principal component analysis (PCA) [@Karl1901], non-linear methods such as multi-dimensional scaling (MDS) [@Torgerson1967]. In the past decade, many new non-linear dimension reduction (NLDR) techniques have emerged, such as t-distributed stochastic neighbor embedding (tSNE) [@Laurens2008] and uniform manifold approximation and projection (UMAP) [@Leland2018], designed to capture the complex and non-linear relationships present within high-D data [@Johnstone2009]. NLDR techniques are the 2D models of high-D data in our context.  

Visualization of NLDR techniques (m-vis) [@article59] for the same high-D data is particularly important in understanding and finding the best representation. If we have done so, the 2D models can be quite different from the data structure shown in high-D space. Therefore, visualizing the 2D model in high-D space (m-in-dis) is more useful to answer different types of questions:

- Is there a best representation of high-D data or are they all providing  equivalent information? Is there a best parameter choice to fit the 2D model? How does the model change when it's parameters change?

- How well the does the 2D models capture the data structure? Is the model fitting able to capture different data structure like non-linear, clustering?

If we can't easily ask and answer these questions, the ability of understanding the models are restricted. To find the best 2D model and the parameter choices, better understanding of the underlying science is really important.

Also, the importance of m-vis along with data-vis has been recognized and incorporated into interactive software, **liminal** [@article21]. But the 2D model and high-D visualize side by side and interactive like brushing and linking connect the data in the two panels. To address this challenge, we propose a novel approach by combining the tour technique with a low-dimensional manifold. This manifold is created through the synergistic use of NLDR techniques, hexagonal binning, and triangulation. This integration facilitates a more understanding of the data structure, how well (or how poorly) NLDR techniques perform.

The outline of this paper is as follows. The @sec-background provides an detailed overview of dimension reduction methods, and tours. Building upon this foundation, the @sec-methods delves into the proposed algorithm, its implementation details, how to tune the model, model summaries, and a synthetic example to illustrate the functionality of the algorithm. Subsequently, @sec-applications showcases applications of the algorithm on different data sets, particularly in single-cell RNA-seq data. These applications reveal insights into the performance and trustworthiness of NLDR algorithms. We analyze the results to identify situations where NLDR techniques may lead to misleading interpretations. Finally, @sec-conclusions concludes by summarizing the findings and emphasizing the significance of the proposed approach in tackling the challenges of high-dimensional data visualization.


<!--High-dimensional (high-D) data is widespread in many fields including ecology and bioinformatics [@Guo2023], in part because of new data collection technologies [@Johnstone2009; @ayesha2020overview]. Working with high-dimensional data poses considerable challenges due to the difficulty in visualizing beyond two dimensions [@Jia2022]. High-dimensional data also presents difficulties for model fitting [@Johnstone2009], both computationally and interpretation, each of which benefits from visualization.

To create visual representations of high-dimensional data, it is common to apply dimension reduction techniques. 

However, projecting high-dimensional data has limitations, such as information loss and potential distortion of essential structures and patterns [@Jia2022, @article53]. The choice of technique and parameters further impacts the accuracy of the visualization, necessitating careful consideration for meaningful interpretation (see @fig-nldervis).

Interactive and dynamic graphics systems have also been developed over the years to enable visualizing high dimensions. One method, called a tour [@Asimov1985], shows a sequence of linear projections is shown as a movie, allowing exploration without warping the space [@lee2021review]. Interactive tools like **XGobi** and **GGobi** have been successful in incorporating tours for exploring high-dimensional data [@article60]. The R package **tourr** [@article61] further enhances tour visualization within R, although it may face limitations in frame rate and interactive features compared to **GGobi**.

To overcome these limitations, the R package **detourr** [@article22] has been developed, leveraging a Javascript widget via htmlwidgets [@Ramnath2023] to achieve higher frame rates and enhanced interactivity. Additionally, the R package **langevitour** [@article09] utilizes Langevin Dynamics to generate a continuous path of projections, eliminating the need for interpolation between projections for animation. The tour technique has proven valuable in exploring statistical model fits [@article58] and factorial experimental designs [@article59]. Augmenting the results of non-linear dimensional reduction methods with the tour, as demonstrated in the **liminal** R package [@article21], further enhances data exploration.

The earlier approaches for DR involve linear methods such as PCA [@Karl1901]. PCA aims to maintain the second-order statistics of the data by projecting the points into the low dimensional space that preserves the maximum amount of variance among all such projections. As a result, PCA has been shown to be effective in preserving the global structure of the data (Silva & Tenenbaum, 2003).
The global structure includes the overall shape of the dataset, placement of the clusters, and the existence of potential outliers. Unlike PCA, much of the focus of the more recent non-linear methods, including t-SNE (Maaten & Hinton,2008), LargeVis (Tang et al., 2016), and UMAP (McInnes et al., 2018) has been on preserving the local neighborhood structure of each individual point. Mainly,  


While tours [@Asimov1985] preserve space without warping [@lee2021review], they require integrating multiple low-dimensional views mentally to perceive high-dimensional structures. To address this challenge, we propose a novel approach by combining the tour technique with a low-dimensional manifold. This manifold is created through the synergistic use of Non-Linear Dimension Reduction (NLDR) techniques, hexagonal binning, and triangulation. By merging these techniques, our approach offers a comprehensive and efficient means to visualize and explore high-dimensional data while retaining the advantages of both tours and NLDR. This integration facilitates a more intuitive perception of complex data structures and empowers analysts with a robust tool for assessing the accuracy of data transformations. The implementation of our approach is available as an R package called **quollr**.

The outline of this paper is as follows. The @sec-background provides an detailed overview of dimension reduction methods, triangulation, and tours. Building upon this foundation, the @sec-methods delves into the proposed algorithm, **quollr**, and its implementation details. In @sec-prediction, discusses the effectiveness of the learned low-dimensional manifold in accurately representing the complex high-dimensional data. Following that, @sec-simpleex presents simple examples from simulations to illustrate the functionality of the algorithm. Subsequently, @sec-applications showcases real-world applications of **quollr** on different data sets, particularly in single-cell RNA-seq data. These applications reveal insights into the performance and trustworthiness of NLDR algorithms. We analyze the results to identify situations where NLDR techniques may lead to misleading interpretations. Finally, @sec-conclusions concludes by summarizing the findings and emphasizing the significance of the proposed approach in tackling the challenges of high-dimensional data visualization. -->

## Background {#sec-background}

### Dimension Reduction

Consider the high-dimensional data a rectangular matrix $X$, where $X = \begin{bmatrix} \textbf{x}_{1} & \textbf{x}_{2} & \cdots & \textbf{x}_{n}\\ \end{bmatrix}^\top$, with $n$ observations in $p$ dimensions. The objective is to discover a low-dimensional projection $Y = \begin{bmatrix} \textbf{y}_{1} & \textbf{y}_{2} & \cdots & \textbf{y}_{n}\\ \end{bmatrix}^\top$, represented as an $n$ × $d$ matrix, where $d \ll p$. The reduction process seeks to remove noise from the original data set while retaining essential information.

There are two main categories of dimension reduction techniques: linear and non-linear methods. Linear techniques involve a linear transformation of the data, with one popular example being PCA. PCA performs an eigen-decomposition of the sample covariance matrix to obtain orthogonal principal components that capture the variance of the data [@Karl1901]. However, linear methods may not fully capture complex non-linear relationships present in the data.

In contrast, NLDR techniques generate the low-dimensional representation $Y$ from the high-dimensional data $X$, often using pre-processing techniques like $k$-nearest neighbors graph or kernel transformations. Multidimensional Scaling (MDS) is a class of NLDR methods that aims to construct an embedding $Y$ in a low-dimensional space, approximating the pair-wise distances in $X$ [@Torgerson1967]. Variants of MDS include non-metric scaling [@article62] and Isomap, which estimate geodesic distances to create the low-dimensional representation [@article63]. Other approaches based on diffusion processes, like diffusion maps [@article64] and the PHATE algorithm [@article03], also fall under NLDR methods.

A challenge with NLDR methods is selecting and tuning appropriate parameters. One specific technique we focus on is Pairwise Controlled Manifold Approximation (PaCMAP). Similar considerations apply to related methods like tSNE [@Laurens2008], UMAP [@Leland2018], and TrMAP [@article02].

It is important to note that methods like PCA and auto-encoders [@article65] provide a reverse mapping from the low-dimensional space back to the high-dimensional space, enabling data reconstruction. However, many non-linear methods, including tSNE, prioritize visualization and exploration over reconstruction. Their focus is on capturing complex structures that may not be easily represented in the original space, making a straightforward reverse mapping challenging.

#### Non-linear dimension reduction techniques

Non-linear dimension reduction techniques play a crucial role in the analysis and visualization of high-dimensional data, where the complexities of relationships among variables may not be adequately captured by linear methods. Among these techniques, tSNE stands out for its emphasis on preserving pairwise distances. By minimizing the divergence between probability distributions in both the high and low-dimensional spaces, t-SNE effectively reveals intricate structures and patterns within the data. Its application is widespread in tasks requiring the visualization of clusters and local relationships, though it does require careful consideration of the perplexity parameter for optimal results.

UMAP is another powerful non-linear technique that strikes a balance between preserving local and global structures. Constructing a fuzzy topological representation using a weighted k-nearest neighbors graph, UMAP optimizes the low-dimensional embedding to resemble this representation. Known for its efficiency and scalability, UMAP is versatile across various scales of relationships in the data, although parameter sensitivity, particularly concerning the choice of neighbors, must be taken into account.

For trajectory data, PHATE provides specialized capabilities. It models the affinity between data points, simulating a heat diffusion process to capture developmental processes, particularly in single-cell genomics. While PHATE excels in revealing trajectory structures and offering insights into cellular development, it necessitates careful parameter tuning due to its specialized nature.

TriMAP adopts a unique approach by approximating the data manifold through the construction of a triangulated graph representation. This technique efficiently captures both global and local structures by representing the data as a network of triangles. TriMAP's strength lies in its ability to efficiently capture complex structures, albeit with sensitivity to parameter choices, including the number of neighbors.

In contrast, PaCMAP introduces supervised learning to create a low-dimensional representation while preserving pair-wise relationships. Constructing a graph based on pair-wise distances, PaCMAP optimizes an embedding using a customizable loss function. Particularly notable is PaCMAP's flexibility in incorporating class labels or additional information to guide the embedding process, offering users a means to customize its behavior and performance.


```{r}
#| warning: false
#| echo: false

data <- read_rds("data/s_curve/s_curve.rds")
```

```{r}
#| warning: false
#| echo: false

# data <- data |>
#   mutate(ID = row_number())
# 
# data_split <- initial_split(data)
# training_data <- training(data_split) |>
#   arrange(ID)
# test_data <- testing(data_split) |>
#   arrange(ID)
training_data <- read_rds("data/s_curve/s_curve_training.rds")
test_data <- read_rds("data/s_curve/s_curve_test.rds")
```

```{r}
#| warning: false
#| echo: false

#plot_list <- list()

tSNE_data <- read_rds("data/s_curve/s_curve_tsne_27.rds")
#(perplexity: ", calculate_effective_perplexity(data), ")
plot_list1 <- plot_tSNE_2D(tSNE_data) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

UMAP_data <- read_rds(file = "data/s_curve/s_curve_umap.rds")

#(n-neighbors: 50)
plot_list2 <- plot_UMAP_2D(UMAP_data) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```


```{r}
#| warning: false
#| echo: false

#(knn: 5)
# PHATE_data <- Fit_PHATE(training_data, knn = 5, with_seed = 20240110)
# write_csv(PHATE_data, paste0(here::here(), "/data/phate_data_s_curve.csv"))

PHATE_data <- read_rds(file = "data/s_curve/s_curve_phate.rds")

plot_list3 <- plot_PHATE_2D(PHATE_data) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

# tem_dir <- tempdir()
# 
# Fit_TriMAP_data(training_data, tem_dir)
# 
# path <- file.path(tem_dir, "df_2_without_class.csv")
# path2 <- file.path(tem_dir, "dataset_3_TriMAP_values.csv")
# 
# Fit_TriMAP(as.integer(2), as.integer(5), as.integer(4), as.integer(3), path, path2)
# 
# TriMAP_data <- read_csv(path2)
# write_csv(TriMAP_data, paste0(here::here(), "/data/trimap_data_s_curve.csv"))

TriMAP_data <- read_rds(file = "data/s_curve/s_curve_trimap.rds")

#(n-inliers: 5, \n n-outliers: 4, n-random: 3)
plot_list4 <- plot_TriMAP_2D(TriMAP_data) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

```


```{r}
#| warning: false
#| echo: false

# tem_dir <- tempdir()
# 
# Fit_PacMAP_data(training_data, tem_dir)
# 
# path <- file.path(tem_dir, "df_2_without_class.csv")
# path2 <- file.path(tem_dir, "dataset_3_PaCMAP_values.csv")
# 
# Fit_PaCMAP(as.integer(2), as.integer(10), "random", 0.9, as.integer(2), path, path2)
# 
# PacMAP_data <- read_csv(path2)
# write_csv(PacMAP_data, paste0(here::here(), "/data/pacmap_data_s_curve.csv"))

PaCMAP_data <- read_rds(file = "data/s_curve/s_curve_pacmap.rds")

#(knn: 10, init: random, \n MN-ratio: 0.9, FP-ratio: 2)
plot_list5 <- plot_PaCMAP_2D(PaCMAP_data) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied the same data: (a) tSNE (perplexity = 27), (b) UMAP (n_neighbors = 50), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis
#| out-width: 100%

plot_list1 + plot_list2 + plot_list3 + plot_list4 + plot_list5 +
  plot_layout(ncol=5)
```


### Linear overviews using tours

A tour is a powerful visualization technique used to explore high-dimensional data by generating a sequence of projections, typically into two dimensions. There are two main types of tours: the Grand Tour and the Guided Tour. The Grand Tour explores the data's shape and global structure by using random projections [@Asimov1985]. In contrast, the Guided Tour focuses on specific patterns by moving towards interesting projections defined by a predefined index function [@article29].

The process begins with a real data matrix $X$ containing $n$ observations in $p$ dimensions. It generates a sequence of $p$ × $d$ orthonormal projection matrices (bases), usually 1 or 2 dimensions. For each pair of orthonormal bases $A_t$ and $A_{t+1}$, a geodesic path is interpolated to create smooth animation between projections.

In the Grand Tour, new orthonormal bases are randomly chosen to explore the $d$-dimensional subspace. The data is often sphered via principal components to reduce dimensionality. The Guided Tour uses a predefined index function to generate a sequence of 'interesting' projections. The resulting tour continuously visualizes the projected data $Y_t$ = $XA_t$ as it interpolates between successive bases.

While both tours can be used to visualize data, examples often focus on using the Grand Tour to observe global structures. However, software like **langevitour** can visualize both types of tours, providing flexibility for exploring high-dimensional data with various objectives.

## Methodology {#sec-methods}

<!--In this paper, we introduce a novel approach, which is designed to determine which method, and which parameter choice provide the most useful representation of high dimensional data. Our approach involves dividing the data set into two subsets: a training set, which serves to construct the low-dimensional manifold, and a test set, utilized to generate predictive values and residuals.

For the implementation of our approach, a 2D embedding data set serves as the initiation point for the subsequent analysis and visualization (see @fig-nldervis (b)).-->

<!--### Overview {#sec-algorithm}-->

Our algorithm comprises two main phases: (1) generate the model in the 2D space, and (2) generate the model in the high-D space. These two phases are described in details in this section.

![A flow diagram detailing the steps taken to create the low-dimensional manifold in the high dimensional space. There are two basic phases, one to generate the model in the 2D space, and other to generate the model in the high-D space.](figures/workflow.png){#fig-meth fig-align="center" width="100%" height="100%"}

### Preprocessing steps

In order to reduce the computational complexity associated with performing NLDR techniques to high-D data, and as a initial step of noise reduction of high-D data, PCA [@article67, @article68, @article69] is applied as a preprocessing step. Subsequently, the identified principal components, representing directions of maximum variance, are used to construct the model.

### Constructing the 2D model {#sec-construct2d}

**Step 1: Computing the hexagonal grid configuration**

The hexagonal grid, formed through hexagonal binning [@Carr1987, @article66], serves as a type of bivariate histogram employed to visualize the structure of high-D data. Hexagons, being one of only three regular polygons capable of tessellating a plane [@Carr2013], possess both symmetry of nearest neighbors and the maximum number of sides for a regular tessellation of the plane. This unique combination makes hexagons more efficient in covering the plane compared to other regular tessellations. Additionally, hexagons exhibit lower visual bias when displaying densities, setting them apart from other regular tessellations [@Dan2023]. In our algorithm, hexagonal binning is used as the initial step of constructing the 2D model and the total number of bins ($b$) is the crucial parameter that defines the granularity of the hexagonal grid. 

**(a) Determine the number of bins along the x-axis** ($b_1$)

First, the number of bins along the x-axis ($b_1$) is computed using the relationship between the diameter ($h$) and the area ($A$) of regular hexagons (see @eq-equation3).

$$
 \text{A} = \frac{\sqrt{3}}{2}h^2
$$ {#eq-equation3}

To construct regular hexagons, $A = 1$ (see @fig-binsize) use as the default setting. Then, the diameter ($h$) of the regular hexagons is calculated (see @eq-equation4).

$$
  \text{h} = \sqrt{\frac{2}{\sqrt{3}}A}
$$ {#eq-equation4}
  
@Carr2013 mentioned about the relationship between the diameter ($h$) of regular hexagons and the height ($y$) of the plotting region. According to our algorithm, the height ($y$) of the plotting region is the the range of 2D embedding component 1 ($r_1$) (see @eq-equation6).

$$
h = \frac{r_1}{b_1}
$$ {#eq-equation6}
      
After rearranging the @eq-equation6 as @eq-equation5, $b_1$ is computed. The $b_1$ value is rounded up to the nearest whole number to have an integer value.

$$
b_1 = \frac{r_1}{h}
$$ {#eq-equation5}
          

**(b) Determine the shape parameter** ($s$)
          
In this step, we determine the shape parameter ($s$) for the hexagonal bins, which significantly influences their shape and arrangement within the grid. The $s$ in the hexagonal binning algorithm is defined as the ratio of the height ($y$) to the width ($x$) of the plotting region as defined in @eq-equation1 [@Carr2013]. 
          
$$
s = \frac{y}{x}
$$ {#eq-equation1}
            
The shape parameter ($s$) of our algorithm is calculated as the ratio of the ranges of 2D embedding components, where $r_1$ and $r_2$ represent the ranges of 2D embedding components 1 and component 2, respectively (see @eq-equation2).
            
$$
s = \frac{r_2}{r_1}
$$ {#eq-equation2}
   
**(c) Determine the number of bins along the y-axis** ($b_2$)

Next, the number of bins along the y-axis is computed based on the number of bins along the x-axis ($b_1$) and the shape parameter ($s$) (see @eq-equation12) [@Carr2013].

$$
b_2 = 2 * \left(\frac{(b_1 \times s)}{\sqrt(3)} + 1.5001 \right)
$${#eq-equation12}
   
**(d) Determine the total number of bins** ($b$)

The total number of bins is determined by multiplying the number of bins along the x-axis ($b_1$) with the number of bins along the y-axis ($b_2$) (see @eq-equation13).
  
$$
b = b_1 \times b_2
$${#eq-equation13}
  

             
                
<!--first value-->
                  
```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop1 <- read_rds("data/s_curve/s_curve_hex_3.rds") 

p1 <-  ggplot(data = hex_full_count_df_loop1, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
  geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  coord_cartesian(xlim =c(-5, 7), ylim = c(-10, 10)) +
  theme_void() +
  theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
        legend.title = element_text(size=8), #change legend title font size
        legend.text = element_text(size=6)) +
  guides(fill = guide_colourbar(title = "Standardized count")) +
  annotate(geom = 'text', label = "a", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```
                
                
```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop2 <- read_rds("data/s_curve/s_curve_hex_11.rds") 

p2 <-  ggplot(data = hex_full_count_df_loop2, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
  geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  coord_cartesian(xlim =c(-5, 7), ylim = c(-10, 10)) +
  theme_void() +
  theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
        legend.title = element_text(size=8), #change legend title font size
        legend.text = element_text(size=6)) +
  guides(fill = guide_colourbar(title = "Standardized count")) +
  annotate(geom = 'text', label = "b", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```

```{r}
#| echo: false
#| message: false
#| warning: false
hex_full_count_df_loop3 <- read_rds("data/s_curve/s_curve_hex_20.rds") 

p3 <-  ggplot(data = hex_full_count_df_loop3, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = polygon_id, fill = std_counts)) +
  geom_point(data = UMAP_data, aes(x = UMAP1, y = UMAP2), alpha = 0.5) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  coord_cartesian(xlim =c(-5, 7), ylim = c(-10, 10)) +
  theme_void() +
  theme(legend.position="none", legend.direction="horizontal", plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #change legend key width
        legend.title = element_text(size=8), #change legend title font size
        legend.text = element_text(size=6)) +
  guides(fill = guide_colourbar(title = "Standardized count")) +
  annotate(geom = 'text', label = "c", x = -Inf, y = Inf, hjust = -0.3, vjust = 1, size = 3) 

```

```{r}
#| echo: false
#| label: fig-binsize
#| fig-pos: H
#| fig-cap: "Hexbin plots from different number of bins for the **UMAP** projections of **S-curve** data: (a) b = 32 (4, 8), s = 1.643542, (b) b = 264 (12, 22), s = 1.643542, and (c) b = 840 (21, 40), s = 1.643542. The hexbins are colored based on the density of points, with darker colors indicating higher point density and yellow color representing lower point density within each bin. Does a value of number of bins exist to effectively represent the low-dimensional data?"

p1 + p2 + p3
```
                
**Step 2: Obtain bin centroids**

As a result of hexagonal binning for high-D data, all the high-D data are clustered into hexagons. In this step, the bin centroids ($C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$) (see @fig-meth Step 2) are obtained [@Carr2013].

**Step 3: Triangulate bin centroids**
  
In this step, the algorithm proceeds to triangulate the hexagonal bin centroids (see @fig-meth Step 3). Triangulation is a fundamental process in computational geometry and computer graphics that involves dividing a set of points in a given space into interconnected triangles [@article30]. One common algorithm used for triangulation is Delaunay triangulation [@article26, @article54], where points are connected in a way that maximizes the minimum angles of the resulting triangles, leading to a more regular and well-conditioned triangulation. 

Since we are working with the centroids of regular hexagonal bins, the resulting mesh will predominantly comprise equal-sized regular triangles. However, the triangulation also helps span any gaps that may exist between clusters of points, allowing for a more complete and interconnected representation of the data.

### Lifting the model into high dimensions

#### Lifting the triangular mesh points into high dimensions

**Step1: Cluster 2D points to hexagons**

Expanding upon the information regarding hexagonal binning discussed in Step 1 of @sec-construct2d, the primary objective in this step is to determine the 2D embedding points associated with each hexagon. As the initial process of hexagonal binning, the 2D points are clustered into their respective hexagonal bins. By mapping this information with the hexagonal bin centroids ($C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$) that obtained in Step 2 of the 2D model building (see @sec-construct2d), we can find which 2D points are assigned to which data set (see @fig-wkhighD (a)).

**Step2: Cluster high dimensional points to hexagons**

Following the step 1, the main focus in this step to determine the corresponding high dimensional points for each hexagon. Every 2D embedding point serves as a projection of a data point belonging to high dimensional space. By using this mapping between the high dimensional data and their corresponding projections, the high dimensional points allocated to each hexagons are determined (see video linked in @fig-wkhighD).

**Step3: Compute the mean within hexagon**

Having identified the high-dimensional points associated with hexagons, the final step involves computing the mean within each hexagonal bin. This implies calculating the average of the high-dimensional data points located within each hex bin. These averaged high-dimensional data points, denoted as $C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$, serve as the representative coordinates for the hex bin centroids within the expansive high-dimensional space (see video linked in @fig-wkhighD).

#### Lifting the 2D triangular mesh into high dimensions

Based on the insights gained in Step 3 of @sec-construct2d, where connected edges in the 2D triangular mesh were identified, this step involves lifting these connections into the high dimensions. Having the mappings of all 2D triangular mesh points into high dimensions, the points connected in 2D are also connected in high dimensional space (see video linked in @fig-wkhighD). 


```{r}
#| echo: false

# UMAP_fit <- umap(training_data |> dplyr::select(-ID), n_neighbors = 50, n_components =  2)
# 
# UMAP_data <- UMAP_fit$layout |>
#   as.data.frame()
# names(UMAP_data)[1:(ncol(UMAP_data))] <- paste0(rep("UMAP",(ncol(UMAP_data))), 1:(ncol(UMAP_data)))
# 
# UMAP_data <- UMAP_data |>
#   mutate(ID = training_data$ID)

num_bins_x <- calculate_effective_x_bins(.data = UMAP_data, x = UMAP1,
                                         cell_area = 1)

shape_value <- calculate_effective_shape_value(.data = UMAP_data, 
                                               x = UMAP1, y = UMAP2)

## To extract bin centroids
hexbin_data_object <-extract_hexbin_centroids(nldr_df = UMAP_data, num_bins = num_bins_x, shape_val = shape_value)

df_bin_centroids <- hexbin_data_object$hexdf_data

# ##########
# 
# ## Data set with all possible centroids in the hexagonal grid
# 
# full_centroid_df <- generate_full_grid_centroids(df_bin_centroids)
# 
# ## To map hexID to hexbin centroids in the full grid
# 
# vec1 <- stats::setNames(rep("", 2), c("x", "y"))  ## Define column names
# 
# full_grid_with_hexbin_id <- dplyr::bind_rows(vec1)[0, ]
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::mutate_if(is.character, as.numeric)
# 
# for(i in 1:length(sort(unique(full_centroid_df$y)))){
#   
#   ## Filter the data set with specific y value
#   specific_y_val_df <- full_centroid_df |>
#     dplyr::filter(y == sort(unique(full_centroid_df$y))[i])
#   
#   ordered_x_df <- specific_y_val_df |>
#     dplyr::arrange(x) 
#   
#   full_grid_with_hexbin_id <- dplyr::bind_rows(full_grid_with_hexbin_id, ordered_x_df)
#   
# }
# 
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::mutate(hexID = row_number())
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#   dplyr::rename("c_x" = "x",
#          "c_y" = "y") 
# 
# full_grid_with_hexbin_id <- dplyr::full_join(full_grid_with_hexbin_id, df_bin_centroids, by = c("hexID" = "hexID")) |>
#   dplyr::select(-c(x, y)) #|> 
#   #dplyr::mutate(counts = tidyr::replace_na(counts, 0))
# 
# full_grid_with_hexbin_id <- full_grid_with_hexbin_id |>
#     dplyr::mutate(std_counts = counts/max(counts, na.rm = TRUE))
# 
# ## Generate all coordinates of hexagons
# hex_grid <- full_hex_grid(full_centroid_df)
# 
# full_grid_with_polygon_id_df <- map_polygon_id(full_grid_with_hexbin_id, hex_grid)
# 
# full_grid_with_hexbin_id_rep <- full_grid_with_polygon_id_df |>
#   dplyr::slice(rep(1:n(), each = 6)) |>
#   dplyr::arrange(polygon_id)

hex_full_count_df <- read_rds("data/s_curve/s_curve_hex_11.rds") 

##########

min_std_cell_threshold <- 0.05

df_bin_centroids_all <- hexbin_data_object$hexdf_data ## All the centroids without removing low-density hexagons


df_bin_centroids <- df_bin_centroids |>
  dplyr::mutate(stand_cell_count = counts/max(counts)) |>
  dplyr::filter(stand_cell_count > min_std_cell_threshold)

UMAP_data_with_hb_id <- UMAP_data |> 
  dplyr::mutate(hb_id = hexbin_data_object$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all <- dplyr::bind_cols(training_data |> dplyr::select(-ID), UMAP_data_with_hb_id)

## Averaged on high-D
df_bin <- avg_highD_data(.data = df_all)

## Triangulate bin centroids
tr1_object <- triangulate_bin_centroids(df_bin_centroids, x, y)
tr_from_to_df <- generate_edge_info(triangular_object = tr1_object)

## Compute 2D distances
distance <- cal_2D_dist(.data = tr_from_to_df)

## To find the benchmark value
benchmark <- find_benchmark_value(.data = distance, distance_col = distance)


trimesh <- ggplot(df_bin_centroids, aes(x = x, y = y)) + 
  geom_segment(data = tr_from_to_df, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
  geom_point(size = 2, colour = "#33a02c") +
  coord_equal()

# ggplot(df_bin_centroids, aes(x = x, y = y)) + 
# geom_point(size = 1, colour = "#33a02c") + 
# geom_trimesh() + 
# coord_equal() 

trimesh <- trimesh +
  #ggtitle("(a)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
  ) 
# theme(axis.text = element_text(size = 5),
#       axis.title = element_text(size = 7))

trimesh_gr <- colour_long_edges(.data = distance, benchmark_value = benchmark, 
                                triangular_object = tr1_object, distance_col = distance) 

trimesh_gr <- trimesh_gr + 
  geom_point(size = 2, colour = "#33a02c") + 
  #ggtitle("(b)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
  #coord_equal() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
  ) 

trimesh_removed <- remove_long_edges(.data = distance, benchmark_value = benchmark, 
                                     triangular_object = tr1_object, distance_col = distance)

trimesh_removed <- trimesh_removed + 
  geom_point(size = 2, colour = "#33a02c") + 
  #ggtitle("(b)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
  #coord_equal() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
  ) 



tour1 <- show_langevitour(df_all, df_bin, df_bin_centroids, benchmark_value = benchmark, distance = distance, distance_col = distance)
```

```{r}
#| echo: false


## To plot the distribution of distance
plot_dist <- function(distance_df){
  distance_df$group <- "1"
  dist_plot <- ggplot(distance_df, aes(x = group, y = distance)) +
    geom_quasirandom()+
    ylim(0, max(unlist(distance_df$distance))+ 0.5) + coord_flip()
  return(dist_plot)
}
```

```{r}
#| echo: false

distance_plot <- plot_dist(distance) +
  #ggtitle("(b)" ) + 
  ylab(expression(d^{(2)})) +
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 12))
```



```{r}
#| echo: false
#| warning: false
data <- read_rds("data/s_curve/s_curve.rds")
training_data <- read_rds("data/s_curve/s_curve_training.rds")
UMAP_data <- read_rds(file = "data/s_curve/s_curve_umap.rds")

num_bins_x <- 2

shape_value <- calculate_effective_shape_value(.data = UMAP_data, 
                                               x = UMAP1, y = UMAP2)

## To extract bin centroids
hexbin_data_object <-extract_hexbin_centroids(nldr_df = UMAP_data, num_bins = num_bins_x, shape_val = shape_value)

df_bin_centroids <- hexbin_data_object$hexdf_data

# ## Data set with all possible centroids in the hexagonal grid
# 
# full_centroid_df <- generate_full_grid_centroids(df_bin_centroids)
# 
# ## Generate all coordinates of hexagons
# hex_grid <- full_hex_grid(full_centroid_df)
# 
# hex_full_count_df <- generate_full_grid_info(df_bin_centroids)
# 
# write_rds(hex_full_count_df, "data/s_curve/s_curve_hex_2.rds")

min_std_cell_threshold <- 0.05

df_bin_centroids_all <- hexbin_data_object$hexdf_data ## All the centroids without removing low-density hexagons


hex_full_count_df <- read_rds("data/s_curve/s_curve_hex_2.rds") 

df_bin_centroids <- df_bin_centroids |>
  dplyr::mutate(stand_cell_count = counts/max(counts)) |>
  dplyr::filter(stand_cell_count > min_std_cell_threshold)

UMAP_data_with_hb_id <- UMAP_data |> 
  dplyr::mutate(hb_id = hexbin_data_object$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all <- dplyr::bind_cols(training_data |> dplyr::select(-ID), UMAP_data_with_hb_id)

## Averaged on high-D
df_bin <- avg_highD_data(.data = df_all)

## Triangulate bin centroids
tr1_object <- triangulate_bin_centroids(df_bin_centroids, x, y)
tr_from_to_df <- generate_edge_info(triangular_object = tr1_object)

# Create the tibble with x and y coordinates
tr_df <- tibble::tibble(x = tr1_object$x, y = tr1_object$y)

## Compute 2D distances
distance <- cal_2D_dist(.data = tr_from_to_df)

benchmark_value <- 19.4

# Filter and label small and long edges
distance_df_small_edges <- distance |>
  dplyr::filter(distance < benchmark_value) |>
  dplyr::mutate(type = "small_edges")

distance_df_long_edges <- distance |>
  dplyr::filter(distance >= benchmark_value) |>
  dplyr::mutate(type = "long_edges")

# Combine small and long edges
distance_edges <- dplyr::bind_rows(distance_df_small_edges, distance_df_long_edges)

tr_from_to_df_coord_with_group <- merge(tr_from_to_df, distance_edges, by = c("from", "to"))

UMAP_data_with_hb_id_selected <- UMAP_data_with_hb_id |> dplyr::filter(hb_id %in% c(2, 15)) |>
  dplyr::mutate(select_point = if_else(hb_id == 2, "data1", "data2"))

a1 <- ggplot(tr_df |> dplyr::filter((y == min(tr_df$y)) | (y == max(tr_df$y))), aes(x = x, y = y)) +
  geom_segment(
    aes(x = x_from, y = y_from, xend = x_to, yend = y_to, color = type),
    data = tr_from_to_df_coord_with_group
  ) +
  geom_point() +
  coord_equal() +
  scale_colour_manual(values = c("#000000", "#f6e8c3"))  + 
  geom_point(size = 3, colour = "#33a02c") + 
  #ggtitle("(b)") + 
  xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  theme_light() +
  #coord_equal() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
  ) 


a2 <- ggplot(data = hex_full_count_df, aes(x = x, y = y)) + 
  geom_polygon(color = "#e7e1ef", aes(group = polygon_id), fill = "#ffffff") +
  geom_point(data = UMAP_data_with_hb_id_selected, aes(x = UMAP1, y = UMAP2, colour = select_point), alpha = 0.5) +
  scale_color_manual(values=c("#e6550d", "#000000")) +
  geom_point(data = df_bin_centroids_all |> dplyr::filter(hexID %in% c(2, 15)), aes(x = x, y = y), colour = "#33a02c", size = 3) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff", option = "C") +
  theme_light() +
  theme(legend.position = "none", plot.title = element_text(size = 5, hjust = 0.5, vjust = -0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()#change legend key width
  ) 


## Script to make coloring data points with a line

df1 <- df_all |> 
  dplyr::filter(hb_id == "2") |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data1") ## original dataset

df2 <- df_all |> 
  dplyr::filter(hb_id == "15") |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data2") ## original dataset

df3 <- df_all |> 
  dplyr::filter(!(hb_id %in% c("2", "15"))) |>
  dplyr::select(tidyselect::starts_with("x")) |>
  dplyr::mutate(type = "data") ## original dataset

df_b <- df_bin |> 
  dplyr::filter(hb_id %in% df_bin_centroids$hexID) |>
  dplyr::filter(hb_id %in% c("2", "15")) |>
  dplyr::select(-hb_id) |>
  dplyr::mutate(type = "model") ## Data with summarized mean

df_exe <- dplyr::bind_rows(df_b, df1, df2, df3)

lg_high <- langevitour::langevitour(df_exe[1:(length(df_exe)-1)], group = df_exe$type, pointSize = 3, levelColors = c("#6a3d9a", "#e6550d", "#000000", "#33a02c"), lineFrom = 1, lineTo = 2, lineColors = "#e31a1c")

```

```{r}
#| warning: false
#| echo: false
#| label: fig-wkhighD
#| fig-cap: How the 2D model lift into high dimensions? (a) visualize the points and the hexagonal bin centroids related 2nd and 15th hexagons, (b) visualization of the edge connected the 2nd and 15th hexagons (colored in red) in the triangular mesh. A video of tour animation is available at <https://www.youtube.com/watch?v=vBvM30Plt24>.

a2 + a1 +
  plot_annotation(tag_levels = 'a') +
  plot_layout(ncol = 3) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
```

### Tunning the model

In our model tuning process, we strategically adjust three key parameters to optimize the performance and accuracy of our approach. They are (i) the total number of bins ($b$), (ii) a benchmark value to remove low-density hexagons, and (iii) a benchmark value to remove long edges.

#### Total number of bins

Adjusting the parameter $b_1$ provides control over the total number of bins $b$, allowing us to fine-tune the hexagonal grid.


#### Benchmark value to remove low-density hexagons

Addressing low-density hexagons is a systematic process to handle sparsely represented data in certain regions. For each hex bin, we identify the six nearest hex bins using an equal 2D distance metric. Then, we calculate the mean density, as outlined in the equations:

$$
\text{standard count} = \frac{\text{count}}{\text{max count}} 
$${#eq-equationp2}

$$
\text{mean density} = \frac{\text{standard count}}{6} 
$${#eq-equationp3}

The standard count is derived from the number of observations in the hex bins. By examining the distribution of mean densities and designating the first quartile as the benchmark value, hex bins with mean densities below this benchmark are removed. This process ensures the elimination of regions with insufficient data density, focusing on areas with more significant data representation and preserving the overall structure in the low-dimensional space.

#### Benchmark value to remove long edges

The removal of long edges is a critical step to create a smoother representation  by iteratively eliminating hexagons with excessive distances between centroids. This process eliminates outliers and noise while preserving essential local relationships within the data. To achieve this, distances between vertices are sorted, and unique distance values are extracted. By setting a threshold based on the largest difference between consecutive distance values, long edges are identified and removed. This refinement step contributes to enhancing the quality of the triangular mesh, ensuring a more accurate representation of the data structure.

### Model summaries {#sec-summary}

#### Predicted values and residuals

The approach involves employing the K-nearest neighbors (KNN) algorithm to identify the nearest hexagonal bin centroid in the 2D space. Subsequently, the coordinates of this centroid are assigned as the low-dimensional predicted values for the test data in 2D space. It is noteworthy that traditional NLDR methods, such as tSNE, often lack a direct predict function, making our approach valuable for generating predicted values in the absence of such functionalities.

The concept of "residuals" is pivotal in evaluating the accuracy of representing bin centroids in high dimensions. To quantify this accuracy, we introduce an error metric, which measures the sum of squared differences between the high-dimensional data ($x_{ij}$) and the predicted bin centroid data in high-dimensional space ($C_{x_ij}$) across all bins and dimensions. Mathematically, this error is expressed as:

$$
\text{Error} = \sum_{j = 1}^{n}\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2
$$ {#eq-equation11}


Here, $n$ represents the number of bins, $p$ represents the dimensions, $x_{ij}$ is the actual high-dimensional data, and $C_{x_ij}$ is the predicted bin centroid data in high dimensions.

The error metric outlined above provides valuable insights into the overall accuracy of our predictive model. By quantifying the squared deviations between the actual and predicted values across all bins and dimensions, we gain a comprehensive understanding of how well our method captures and represents the underlying structure of the data in the reduced 2D space. This assessment is crucial for evaluating the efficacy of our NLDR technique in preserving the essential information present in the original high-dimensional data.


#### Goodness of fit statistics

Moving on to the assessment of prediction accuracy, we calculate the Mean Squared Error (MSE). The MSE helps measure the average squared differences between the actual high-dimensional data ($x_{ij}$) and the predicted bin centroid data in high-D ($C_{x_ij}$) values across all bins. Mathematically, this is expressed as:

$$
\text{MSE} = \sum_{j = 1}^{n} \frac{\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2}{b}
$$ {#eq-equation9}

Here, $b$ signifies the total number of bins, $p$ denotes the number of dimensions in the high-dimensional data, and $n$ represents the number of observations.

Additionally, we gauge the model's performance using the Akaike Information Criterion (AIC), calculated by the formula:
                    
$$
\text{AIC} = 2bp + np * log(\text{MSE})
$$ {#eq-equation10}
  
  These metrics, MSE and AIC, collectively offer valuable insights into the model's predictive performance, considering both accuracy and complexity in the predictions.


```{r}
#| warning: false
#| echo: false

data <- read_rds("data/s_curve/s_curve.rds")

## tSNE

shape_value_curve <- calculate_effective_shape_value(.data = tSNE_data,
                                                   x = tSNE1, y = tSNE2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = tSNE_data, nldr_df_test = tSNE_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "tSNE1", y = "tSNE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_1 <- eval_data_training |>
  dplyr::mutate(method = "tSNE")
```


```{r}
#| warning: false
#| echo: false

## UMAP
## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                   x = UMAP1, y = UMAP2)

num_bins_vec <- 1:14 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_2 <- eval_data_training |>
  dplyr::mutate(method = "UMAP")
```

```{r}
#| warning: false
#| echo: false
## PAHTE
## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = PHATE_data,
                                                   x = PHATE1, y = PHATE2)

num_bins_vec <- 1:18 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = PHATE_data, nldr_df_test = PHATE_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "PHATE1", y = "PHATE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_3 <- eval_data_training |>
  dplyr::mutate(method = "PHATE")

```

```{r}
#| warning: false
#| echo: false

## TriMAP

## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = TriMAP_data,
                                                   x = TriMAP1, y = TriMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = TriMAP_data, nldr_df_test = TriMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "TriMAP1", y = "TriMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_4 <- eval_data_training |>
  dplyr::mutate(method = "TriMAP")

```

```{r}
#| warning: false
#| echo: false

## PaCMAP

## Prediction

shape_value_curve <- calculate_effective_shape_value(.data = PaCMAP_data,
                                                   x = PaCMAP1, y = PaCMAP2)

num_bins_vec <- 1:18 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = PaCMAP_data, nldr_df_test = PaCMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "PaCMAP1", y = "PaCMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_5 <- eval_data_training |>
  dplyr::mutate(method = "PaCMAP")

```

```{r}
#| warning: false
#| echo: false

MSE_df <- dplyr::bind_rows(MSE_df_1, MSE_df_2, MSE_df_3, MSE_df_4, MSE_df_5)

MSE_df$method <- factor(MSE_df$method, levels = c("tSNE", "UMAP", "PHATE", "TriMAP", "PaCMAP"))

## To draw with AIC
aic_plot <- ggplot(MSE_df |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot <- ggplot(MSE_df, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training S-curve dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltScurve
#| out-width: 100%

aic_plot + mse_plot +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```


<!--UMAP with different param choices-->

```{r}
#| warning: false
#| echo: false

UMAP_data_7 <- read_rds(file = "data/s_curve/s_curve_umap_7.rds")

#(n-neighbors: 50)
plot_list1_umap <- plot_UMAP_2D(UMAP_data_7) + #ggtitle("(b)") +
  geom_point(color = "#8dd3c7") +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

UMAP_data_15 <- read_rds(file = "data/s_curve/s_curve_umap_15.rds")

#(n-neighbors: 50)
plot_list2_umap <- plot_UMAP_2D(UMAP_data_15) + #ggtitle("(b)") + 
  geom_point(color = "#a65628") +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

UMAP_data_32 <- read_rds(file = "data/s_curve/s_curve_umap_32.rds")

#(n-neighbors: 50)
plot_list3_umap <- plot_UMAP_2D(UMAP_data_32) + #ggtitle("(b)") +
  geom_point(color = "#f781bf") +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```

```{r}
#| warning: false
#| echo: false

UMAP_data <- read_rds(file = "data/s_curve/s_curve_umap.rds")

#(n-neighbors: 50)
plot_list4_umap <- plot_UMAP_2D(UMAP_data) + #ggtitle("(b)") +
  geom_point(color = "#999999") +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

```


```{r}
#| echo: false
#| fig-cap: "2D layouts from UMAP applied for the S-curve data: (a) UMAP (n_neighbors = 7), (b) UMAP (n_neighbors = 15), (c) UMAP (n_neighbors = 32), (d) UMAP (n_neighbors = 50). Is there a best parameter choice in representing UMAP or are they all providing  equivalent information?"
#| label: fig-nldervisUMAP
#| out-width: 100%

plot_list1_umap + plot_list2_umap + plot_list3_umap + plot_list4_umap +
  plot_layout(ncol=4)
```


```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 7
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_7.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_7_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_1 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 7")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 15
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_15.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_15_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_2 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 15")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 32
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap_32.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_32_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_3 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 32")
```

```{r}
#| warning: false
#| echo: false

## UMAP with n_neighbor: 50
## Prediction

UMAP_data <- read_rds("data/s_curve/s_curve_umap.rds")
predict_UMAP_df <- read_rds("data/s_curve/s_curve_umap_predict.rds")

shape_value_curve <- calculate_effective_shape_value(.data = UMAP_data,
                                                     x = UMAP1, y = UMAP2)

num_bins_vec <- 1:10 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_test <- dplyr::bind_rows(vec)[0, ]
eval_data_test <- eval_data_test |>
  dplyr::mutate_if(is.character, as.numeric)

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = UMAP_data, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = data, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  pred_df_test_object <- predict_hex_id(training_data = training_data, nldr_df = UMAP_data, nldr_df_test = predict_UMAP_df, num_bins = num_bins_vec[i], shape_val = shape_value_curve, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_test <- pred_df_test_object$pred_data
  centroid_df_test <- pred_df_test_object$df_bin_centroids
  avg_df_test <- pred_df_test_object$df_bin
  
  eval_df_test <- generate_eval_df(data = data, prediction_df = pred_df_test, df_bin_centroids = centroid_df_test, df_bin = avg_df_test, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  eval_data_test <- dplyr::bind_rows(eval_data_test, eval_df_test)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

eval_data_test <- eval_data_test |>
  mutate(data_type = "test")

MSE_df_umap_4 <- bind_rows(eval_data_training, eval_data_test) |>
  dplyr::mutate(param = "n_neighbors: 50")
```


```{r}
#| warning: false
#| echo: false

MSE_df_umap <- dplyr::bind_rows(MSE_df_umap_1, MSE_df_umap_2, MSE_df_umap_3, MSE_df_umap_4)
MSE_df_umap$param <- factor(MSE_df_umap$param, levels = c("n_neighbors: 7", "n_neighbors: 15", "n_neighbors: 32", "n_neighbors: 50"))

## To draw with AIC
aic_plot_param <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                         y = total_error,
                                                                         color = param
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_color_manual(values = c("#8dd3c7", "#a65628", "#f781bf", "#999999")) +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_param_training <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                               y = total_mse,
                               color = param
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_manual(values = c("#8dd3c7", "#a65628", "#f781bf", "#999999")) +
  ylab("MSE") +
  xlab("Total number of bins")

mse_plot_param_test <- ggplot(MSE_df_umap |> dplyr::filter(data_type == "test"), aes(x = number_of_bins,
                                                                                             y = total_mse,
                                                                                             color = param
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), legend.text = element_text(size=7), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_color_manual(values = c("#8dd3c7", "#a65628", "#f781bf", "#999999")) +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different n_neighbors parameter of UMAP applied to training S-curve dataset. What is the best parameter choice in UMAP to represent the original data in 2D?
#| label: fig-diagnosticpltDiffParam
#| out-width: 100%

aic_plot_param + mse_plot_param_training + mse_plot_param_test  +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 3) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

<!--lwd smoothing criteria with s-curve-->

```{r}
#| warning: false
#| echo: false

MSE_df_1_lwd <- read_rds("data/s_curve/s_curve_summary_lwd_tsne.rds")
MSE_df_2_lwd <- read_rds("data/s_curve/s_curve_summary_lwd_umap.rds")
MSE_df_3_lwd <- read_rds("data/s_curve/s_curve_summary_lwd_phate.rds")
MSE_df_4_lwd <- read_rds("data/s_curve/s_curve_summary_lwd_trimap.rds")
MSE_df_5_lwd <- read_rds("data/s_curve/s_curve_summary_lwd_pacmap.rds")

MSE_df_lwd <- dplyr::bind_rows(MSE_df_1_lwd, MSE_df_2_lwd, MSE_df_3_lwd, MSE_df_4_lwd, MSE_df_5_lwd)

MSE_df_lwd$method <- factor(MSE_df_lwd$method, levels = c("tSNE", "UMAP", "PHATE", "TriMAP", "PaCMAP"))

## To draw with AIC
aic_plot_lwd <- ggplot(MSE_df_lwd |> dplyr::filter(data_type == "training"), aes(x = benchmark_rm_hex,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("AIC") +
  xlab("Benchmark value")
## Effective number of bins along x-axis

mse_plot_lwd <- ggplot(MSE_df_lwd, aes(x = benchmark_rm_hex,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("MSE") +
  xlab("Benchmark value")

```

```{r}
#| echo: false
#| warning: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training S-curve dataset with different benchmark values to remove the low-density heaxgons. What is the effective benchark valuw to remove the low-density heaxgons?
#| label: fig-diagnosticpltScurvelwd
#| out-width: 100%

aic_plot_lwd + mse_plot_lwd +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

### Prediction {#sec-prediction}

In this context, "prediction" denotes the 2D embedding generated for the NLDR technique.
The methodology encompasses identifying the nearest averaged high-D points in the high-D space for the test data, by computing high-D Euclidean distances. As the averaged high-D point corresponds to the lifting of the 2D model, determining its nearest counterpart allows us to map its hexagonal bin centroid coordinates. Consequently, these centroid coordinates serve as the assigned low-dimensional predicted values for the test data in the 2D space. 

Some NLDR techniques, such as tSNE, often lack a direct prediction, making our approach valuable for generating predicted values in the absence of such functionalities.

### Model interpretation

The visualizations of NLDR techniques for the S-curve dataset shows different representations in 2D, as showcased in Figure @fig-nldervis. The question at hand is determining the best NLDR technique to represent the S-curve data. Examining the 2D layouts in @fig-nldervis suggests that TriMAP or PaCMAP may be best representations. However, the challenge is to decide the best representation with considerable evidence.

After considering the models constructed from the algorithm for each NLDR techniques and visualizing them in high-D space with the original data, shows that all the NLDR techniques capture the non-linearity of the S-curve (see videos of the models linked in @fig-modelScurve). But thorough examination of each model in high-D space shows some surprising findings. The model of PHATE is flattened and lack of capturing the width of S-curve (see videos of the model with PHATE linked in @fig-modelScurve). This nature of PHATE is also evicted by the considerably higher AIC and MSE values for PHATE as shown in @fig-diagnosticpltScurve. Furthermore, there are two edges that are not connected to any points in the model of TriMAP (see see videos of the model with TriMAP linked in @fig-modelScurve). This model shows how the patterns in 2D representation of TriMAP capture by the model (see @fig-nldervis (d)). As shown in @fig-nldervis (b), UMAP have four clusters in a curvilinear structure. However, the model reveals that even though UMAP shows a curvilinear in 2D, UMAP actually capture the structure of the S-curve without any disjoint in the model (see see videos of the model with UMAP linked in @fig-modelScurve). In the 2D layouts of tSNE and PaCMAP (see @fig-nldervis (a), (e)), there are some sparse areas that are captured by the models effectively (see see videos of the model with tSNE and PaCMAP linked in @fig-modelScurve). 

Different NLDR techniques perform differently on the S-curve data and tSNE provides the best representation in 2D according to AIC and MSE values (see @fig-diagnosticpltScurve). 

<!--model for S-curve with 5 methods-->

```{r}
#| warning: false
#| echo: false

## Import data

tSNE_s_curve <- read_rds("data/s_curve/s_curve_tsne_27.rds")
UMAP_s_curve <- read_rds("data/s_curve/s_curve_umap.rds")
PHATE_s_curve <- read_rds("data/s_curve/s_curve_phate.rds")
TriMAP_s_curve <- read_rds("data/s_curve/s_curve_trimap.rds")
PaCMAP_s_curve <- read_rds("data/s_curve/s_curve_pacmap.rds")


## tSNE

num_bins_tsne_s_curve <- 8
shape_val_tsne_s_curve <- calculate_effective_shape_value(.data = tSNE_s_curve,
                                                          x = tSNE1, y = tSNE2) ## 1.259938
## To extract bin centroids
hexbin_data_object_tsne_s_curve <- extract_hexbin_centroids(nldr_df = tSNE_s_curve, num_bins = num_bins_tsne_s_curve, shape_val = shape_val_tsne_s_curve, x = tSNE1, y = tSNE2)

df_bin_centroids_tsne_s_curve <- hexbin_data_object_tsne_s_curve$hexdf_data

tSNE_data_with_hb_id_s_curve <- tSNE_s_curve |>
  dplyr::mutate(hb_id = hexbin_data_object_tsne_s_curve$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_tsne_s_curve <- dplyr::bind_cols(training_data |> dplyr::select(-ID), tSNE_data_with_hb_id_s_curve)

## Averaged on high-D
df_bin_tsne_s_curve <- avg_highD_data(.data = df_all_tsne_s_curve)

## Triangulate bin centroids
tr1_object_tsne_s_curve <- triangulate_bin_centroids(df_bin_centroids_tsne_s_curve, x, y)
tr_from_to_df_tsne_s_curve <- generate_edge_info(triangular_object = tr1_object_tsne_s_curve)

# ggplot(df_bin_centroids_tsne_s_curve, aes(x = x, y = y)) +
#   geom_segment(data = tr_from_to_df_tsne_s_curve, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
#   geom_point(size = 2, colour = "#33a02c") +
#   coord_equal()


## Compute 2D distances
distance_tsne_s_curve <- cal_2D_dist(.data = tr_from_to_df_tsne_s_curve)

## To find the benchmark value
benchmark_tsne_s_curve <- find_benchmark_value(.data = distance_tsne_s_curve, distance_col = distance)

# colour_long_edges(.data = distance_tsne_s_curve, benchmark_value = benchmark_tsne_s_curve,
#                   triangular_object = tr1_object_tsne_s_curve, distance_col = distance)

trimesh_removed_tsne_s_curve <- remove_long_edges(.data = distance_tsne_s_curve, benchmark_value = benchmark_tsne_s_curve,
                                                  triangular_object = tr1_object_tsne_s_curve, distance_col = distance)

trimesh_removed_tsne_s_curve <- trimesh_removed_tsne_s_curve +
  # xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7)) +
  geom_point(colour = "#33a02c", size = 0.5) +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3) 

# tour_tsne_s_curve <- show_langevitour(df_all_tsne_s_curve, df_bin_tsne_s_curve, df_bin_centroids_tsne_s_curve, benchmark_value = benchmark_tsne_s_curve, distance = distance_tsne_s_curve, distance_col = distance)



## UMAP

num_bins_umap_s_curve <- 6
shape_val_umap_s_curve <- calculate_effective_shape_value(.data = UMAP_s_curve,
                                                          x = UMAP1, y = UMAP2) ## 1.259938
## To extract bin centroids
hexbin_data_object_umap_s_curve <- extract_hexbin_centroids(nldr_df = UMAP_s_curve, num_bins = num_bins_umap_s_curve, shape_val = shape_val_umap_s_curve, x = UMAP1, y = UMAP2)

df_bin_centroids_umap_s_curve <- hexbin_data_object_umap_s_curve$hexdf_data

UMAP_data_with_hb_id_s_curve <- UMAP_s_curve |>
  dplyr::mutate(hb_id = hexbin_data_object_umap_s_curve$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_umap_s_curve <- dplyr::bind_cols(training_data |> dplyr::select(-ID), UMAP_data_with_hb_id_s_curve)

## Averaged on high-D
df_bin_umap_s_curve <- avg_highD_data(.data = df_all_umap_s_curve)

## Triangulate bin centroids
tr1_object_umap_s_curve <- triangulate_bin_centroids(df_bin_centroids_umap_s_curve, x, y)
tr_from_to_df_umap_s_curve <- generate_edge_info(triangular_object = tr1_object_umap_s_curve)

# ggplot(df_bin_centroids_umap_s_curve, aes(x = x, y = y)) +
#   geom_segment(data = tr_from_to_df_umap_s_curve, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
#   geom_point(size = 2, colour = "#33a02c") +
#   coord_equal()


## Compute 2D distances
distance_umap_s_curve <- cal_2D_dist(.data = tr_from_to_df_umap_s_curve)

## To find the benchmark value
benchmark_umap_s_curve <- find_benchmark_value(.data = distance_umap_s_curve, distance_col = distance)

# colour_long_edges(.data = distance_umap_s_curve, benchmark_value = benchmark_umap_s_curve,
#                   triangular_object = tr1_object_umap_s_curve, distance_col = distance)

trimesh_removed_umap_s_curve <- remove_long_edges(.data = distance_umap_s_curve, benchmark_value = benchmark_umap_s_curve,
                                     triangular_object = tr1_object_umap_s_curve, distance_col = distance)

trimesh_removed_umap_s_curve <- trimesh_removed_umap_s_curve +
  # xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7)) +
  geom_point(colour = "#33a02c", size = 0.5) +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)

# tour_umap_s_curve <- show_langevitour(df_all_umap_s_curve, df_bin_umap_s_curve, df_bin_centroids_umap_s_curve, benchmark_value = benchmark_umap_s_curve, distance = distance_umap_s_curve, distance_col = distance)


## TriMAP

num_bins_trimap_s_curve <- 6
shape_val_trimap_s_curve <- calculate_effective_shape_value(.data = TriMAP_s_curve,
                                                            x = TriMAP1, y = TriMAP2) ## 1.259938
## To extract bin centroids
hexbin_data_object_trimap_s_curve <- extract_hexbin_centroids(nldr_df = TriMAP_s_curve, num_bins = num_bins_trimap_s_curve, shape_val = shape_val_trimap_s_curve, x = TriMAP1, y = TriMAP2)

df_bin_centroids_trimap_s_curve <- hexbin_data_object_trimap_s_curve$hexdf_data

TriMAP_data_with_hb_id_s_curve <- TriMAP_s_curve |>
  dplyr::mutate(hb_id = hexbin_data_object_trimap_s_curve$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_trimap_s_curve <- dplyr::bind_cols(training_data |> dplyr::select(-ID), TriMAP_data_with_hb_id_s_curve)

## Averaged on high-D
df_bin_trimap_s_curve <- avg_highD_data(.data = df_all_trimap_s_curve)

## Triangulate bin centroids
tr1_object_trimap_s_curve <- triangulate_bin_centroids(df_bin_centroids_trimap_s_curve, x, y)
tr_from_to_df_trimap_s_curve <- generate_edge_info(triangular_object = tr1_object_trimap_s_curve)

# ggplot(df_bin_centroids_trimap_s_curve, aes(x = x, y = y)) +
#   geom_segment(data = tr_from_to_df_trimap_s_curve, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
#   geom_point(size = 2, colour = "#33a02c") +
#   coord_equal()


## Compute 2D distances
distance_trimap_s_curve <- cal_2D_dist(.data = tr_from_to_df_trimap_s_curve)

## To find the benchmark value
benchmark_trimap_s_curve <- find_benchmark_value(.data = distance_trimap_s_curve, distance_col = distance)

# colour_long_edges(.data = distance_trimap_s_curve, benchmark_value = benchmark_trimap_s_curve,
#                   triangular_object = tr1_object_trimap_s_curve, distance_col = distance)

trimesh_removed_trimap_s_curve <- remove_long_edges(.data = distance_trimap_s_curve, benchmark_value = benchmark_trimap_s_curve,
                                                    triangular_object = tr1_object_trimap_s_curve, distance_col = distance)

trimesh_removed_trimap_s_curve <- trimesh_removed_trimap_s_curve +
  # xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7)) +
  geom_point(colour = "#33a02c", size = 0.25) +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3) 

# tour_trimap_s_curve <- show_langevitour(df_all_trimap_s_curve, df_bin_trimap_s_curve, df_bin_centroids_trimap_s_curve, benchmark_value = benchmark_trimap_s_curve, distance = distance_trimap_s_curve, distance_col = distance)



## PacMAP

num_bins_pacmap_s_curve <- 10
shape_val_pacmap_s_curve <- calculate_effective_shape_value(.data = PaCMAP_s_curve,
                                                            x = PaCMAP1, y = PaCMAP2) ## 1.259938
## To extract bin centroids
hexbin_data_object_pacmap_s_curve <- extract_hexbin_centroids(nldr_df = PaCMAP_s_curve, num_bins = num_bins_pacmap_s_curve, shape_val = shape_val_pacmap_s_curve, x = PaCMAP1, y = PaCMAP2)

df_bin_centroids_pacmap_s_curve <- hexbin_data_object_pacmap_s_curve$hexdf_data

PaCMAP_data_with_hb_id_s_curve <- PaCMAP_s_curve |>
  dplyr::mutate(hb_id = hexbin_data_object_pacmap_s_curve$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_pacmap_s_curve <- dplyr::bind_cols(training_data |> dplyr::select(-ID), PaCMAP_data_with_hb_id_s_curve)

## Averaged on high-D
df_bin_pacmap_s_curve <- avg_highD_data(.data = df_all_pacmap_s_curve)

## Triangulate bin centroids
tr1_object_pacmap_s_curve <- triangulate_bin_centroids(df_bin_centroids_pacmap_s_curve, x, y)
tr_from_to_df_pacmap_s_curve <- generate_edge_info(triangular_object = tr1_object_pacmap_s_curve)

# ggplot(df_bin_centroids_pacmap_s_curve, aes(x = x, y = y)) +
#   geom_segment(data = tr_from_to_df_pacmap_s_curve, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
#   geom_point(size = 2, colour = "#33a02c") +
#   coord_equal()


## Compute 2D distances
distance_pacmap_s_curve <- cal_2D_dist(.data = tr_from_to_df_pacmap_s_curve)

## To find the benchmark value
benchmark_pacmap_s_curve <- find_benchmark_value(.data = distance_pacmap_s_curve, distance_col = distance)

# colour_long_edges(.data = distance_pacmap_s_curve, benchmark_value = benchmark_pacmap_s_curve,
#                   triangular_object = tr1_object_pacmap_s_curve, distance_col = distance)

trimesh_removed_pacmap_s_curve <- remove_long_edges(.data = distance_pacmap_s_curve, benchmark_value = benchmark_pacmap_s_curve,
                                     triangular_object = tr1_object_pacmap_s_curve, distance_col = distance)

trimesh_removed_pacmap_s_curve <- trimesh_removed_pacmap_s_curve +
  # xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7)) +
  geom_point(colour = "#33a02c", size = 0.5) +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

# tour_pacmap_s_curve <- show_langevitour(df_all_pacmap_s_curve, df_bin_pacmap_s_curve, df_bin_centroids_pacmap_s_curve, benchmark_value = benchmark_pacmap_s_curve, distance = distance_pacmap_s_curve, distance_col = distance)

## PHATE

num_bins_phate_s_curve <- 8
shape_val_phate_s_curve <- calculate_effective_shape_value(.data = PHATE_s_curve,
                                                           x = PHATE1, y = PHATE2) ## 1.259938
## To extract bin centroids
hexbin_data_object_phate_s_curve <- extract_hexbin_centroids(nldr_df = PHATE_s_curve, num_bins = num_bins_phate_s_curve, shape_val = shape_val_phate_s_curve, x = PHATE1, y = PHATE2)

df_bin_centroids_phate_s_curve <- hexbin_data_object_phate_s_curve$hexdf_data

## Identify bins with low-density
identify_rm_bins <- find_low_density_hexagons(df_bin_centroids_phate_s_curve, num_bins_phate_s_curve, benchmark_rm_hex = 0.06)

df_bin_centroids_phate_s_curve <- df_bin_centroids_phate_s_curve |>
  filter(!(hexID %in% identify_rm_bins))

PHATE_data_with_hb_id_s_curve <- PHATE_s_curve |>
  dplyr::mutate(hb_id = hexbin_data_object_phate_s_curve$hb_data@cID)

## To generate a data set with high-D and 2D training data
df_all_phate_s_curve <- dplyr::bind_cols(training_data |> dplyr::select(-ID), PHATE_data_with_hb_id_s_curve)

## Averaged on high-D
df_bin_phate_s_curve <- avg_highD_data(.data = df_all_phate_s_curve)

## Triangulate bin centroids
tr1_object_phate_s_curve <- triangulate_bin_centroids(df_bin_centroids_phate_s_curve, x, y)
tr_from_to_df_phate_s_curve <- generate_edge_info(triangular_object = tr1_object_phate_s_curve)

# ggplot(df_bin_centroids_phate_s_curve, aes(x = x, y = y)) +
#   geom_segment(data = tr_from_to_df_phate_s_curve, aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +
#   geom_point(size = 2, colour = "#33a02c") +
#   coord_equal()


## Compute 2D distances
distance_phate_s_curve <- cal_2D_dist(.data = tr_from_to_df_phate_s_curve)

## To find the benchmark value
benchmark_phate_s_curve <- find_benchmark_value(.data = distance_phate_s_curve, distance_col = distance)

# colour_long_edges(.data = distance_phate_s_curve, benchmark_value = benchmark_phate_s_curve,
#                   triangular_object = tr1_object_phate_s_curve, distance_col = distance)

trimesh_removed_phate_s_curve <- remove_long_edges(.data = distance_phate_s_curve, benchmark_value = benchmark_phate_s_curve,
                                                   triangular_object = tr1_object_phate_s_curve, distance_col = distance)

trimesh_removed_phate_s_curve <- trimesh_removed_phate_s_curve +
  # xlab(expression(C[x]^{(2)})) + ylab(expression(C[y]^{(2)})) +
  # theme(axis.text = element_text(size = 5),
  #       axis.title = element_text(size = 7)) +
  geom_point(colour = "#33a02c", size = 0.5) +
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3) 

# tour_phate_s_curve <- show_langevitour(df_all_phate_s_curve, df_bin_phate_s_curve, df_bin_centroids_phate_s_curve, benchmark_value = benchmark_phate_s_curve, distance = distance_phate_s_curve, distance_col = distance)

```


```{r}
#| warning: false
#| echo: false
#| label: fig-modelScurve
#| fig-cap: Is there a best model to represent the original data in 2D space or are they all providing equivalent information?, (a) Model in the 2D space with tSNE (<https://youtu.be/uy9SkAo6gAE>), (b) Model in the 2D space with UMAP (<https://youtu.be/0MJDhHrh_Ug>), (c) Model in the 2D space with PHATE (<https://youtu.be/HbVv0uy0QWk>), (d) Model in the 2D space with TriMAP (<https://youtu.be/2OGwipAjzc8>), and (e) Model in the 2D space with PaCMAP (<https://youtu.be/pkokI8d-cBk>).

trimesh_removed_tsne_s_curve + trimesh_removed_umap_s_curve + trimesh_removed_phate_s_curve + trimesh_removed_trimap_s_curve + trimesh_removed_pacmap_s_curve +
  #plot_annotation(tag_levels = 'a') +
  plot_layout(ncol = 5) &
  theme(legend.position='none', plot.tag = element_text(size = 8))
```






  

### Simulated data example {#sec-simpleex}

In this section, we showcase the effectiveness of our methodology using simulated data. The dataset comprises five spherical Gaussian clusters in 4-$d$, with each cluster containing an equal number of points and consistent within variation.  

We *strongly* recommend viewing the linked videos for each study while reading. Links to the videos are available in the figures for each example. The videos show the visual appearance of the **langevitour** interface with low-dimensional view and how we can interact with the tour via the controls.


```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/five_gau_clusters/data_five_gau.rds")
training_data_1 <- read_rds("data/five_gau_clusters/data_five_gau_training.rds")
test_1 <- read_rds("data/five_gau_clusters/data_five_gau_test.rds")

tSNE_data_gau <- read_rds("data/five_gau_clusters/tsne_data_five_gau_61.rds")
UMAP_data_gau <- read_rds("data/five_gau_clusters/umap_data_five_gau.rds")
PHATE_data_gau <- read_rds("data/five_gau_clusters/phate_data_five_gau.rds")
TriMAP_data_gau <- read_rds("data/five_gau_clusters/trimap_data_five_gau.rds")
PaCMAP_data_gau <- read_rds("data/five_gau_clusters/pacmap_data_five_gau.rds")

## Visualise embeddings

plot_list1_gau <- plot_tSNE_2D(tSNE_data_gau) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_gau <- plot_UMAP_2D(UMAP_data_gau) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_gau <- plot_PHATE_2D(PHATE_data_gau) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_gau <- plot_TriMAP_2D(TriMAP_data_gau) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_gau <- plot_PaCMAP_2D(PaCMAP_data_gau) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied the same data: (a) tSNE (perplexity = 61), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5Gau
#| out-width: 100%

plot_list1_gau + plot_list2_gau + plot_list3_gau + plot_list4_gau + plot_list5_gau +
  plot_layout(ncol=5)
```

```{r}
#| warning: false
#| echo: false

## tSNE

shape_value_gau <- calculate_effective_shape_value(.data = tSNE_data_gau,
                                                   x = tSNE1, y = tSNE2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = tSNE_data_gau, nldr_df_test = tSNE_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "tSNE1", y = "tSNE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_1_gau <- eval_data_training |>
  dplyr::mutate(method = "tSNE")
```


```{r}
#| warning: false
#| echo: false

## UMAP
## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = UMAP_data_gau,
                                                   x = UMAP1, y = UMAP2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = UMAP_data_gau, nldr_df_test = UMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "UMAP1", y = "UMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_2_gau <- eval_data_training |>
  dplyr::mutate(method = "UMAP")
```

```{r}
#| warning: false
#| echo: false
## PAHTE
## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = PHATE_data_gau,
                                                   x = PHATE1, y = PHATE2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = PHATE_data_gau, nldr_df_test = PHATE_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "PHATE1", y = "PHATE2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_3_gau <- eval_data_training |>
  dplyr::mutate(method = "PHATE")

```

```{r}
#| warning: false
#| echo: false

## TriMAP

## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = TriMAP_data_gau,
                                                   x = TriMAP1, y = TriMAP2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = TriMAP_data_gau, nldr_df_test = TriMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "TriMAP1", y = "TriMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_4_gau <- eval_data_training |>
  dplyr::mutate(method = "TriMAP")

```

```{r}
#| warning: false
#| echo: false

## PaCMAP

## Prediction

shape_value_gau <- calculate_effective_shape_value(.data = PaCMAP_data_gau,
                                                   x = PaCMAP1, y = PaCMAP2)

num_bins_vec <- 1:20 ## Number of bins along the x-axis

vec <- stats::setNames(rep("", 4), c("number_of_bins", "number_of_observations", "total_error", "total_mse"))  ## Define column names

eval_data_training <- dplyr::bind_rows(vec)[0, ]
eval_data_training <- eval_data_training |>
  dplyr::mutate_if(is.character, as.numeric)

for (i in 1:length(num_bins_vec)) {
  
  pred_df_training_object <- predict_hex_id(training_data = training_data_1, nldr_df = PaCMAP_data_gau, nldr_df_test = PaCMAP_data_gau, num_bins = num_bins_vec[i], shape_val = shape_value_gau, x = "PaCMAP1", y = "PaCMAP2", col_start = "x")
  pred_df_training <- pred_df_training_object$pred_data
  centroid_df_training <- pred_df_training_object$df_bin_centroids
  avg_df_training <- pred_df_training_object$df_bin
  
  eval_df_training <- generate_eval_df(data = df_2, prediction_df = pred_df_training, df_bin_centroids = centroid_df_training, df_bin = avg_df_training, num_bins = num_bins_vec[i], col_start = "x")
  
  eval_data_training <- dplyr::bind_rows(eval_data_training, eval_df_training)
  
  
}


## Add new column with data types

eval_data_training <- eval_data_training |>
  mutate(data_type = "training")

MSE_df_5_gau <- eval_data_training |>
  dplyr::mutate(method = "PaCMAP")

```

```{r}
#| warning: false
#| echo: false

MSE_df_gau <- dplyr::bind_rows(MSE_df_1_gau, MSE_df_2_gau, MSE_df_3_gau, MSE_df_4_gau, MSE_df_5_gau)

MSE_df_gau$method <- factor(MSE_df_gau$method, levels = c("tSNE", "UMAP", "PHATE", "TriMAP", "PaCMAP"))


## To draw with AIC
aic_gau_plot <- ggplot(MSE_df_gau |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                             y = total_error,
                                                             color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_gau_plot <- ggplot(MSE_df_gau, aes(x = number_of_bins,
                   y = total_mse,
                   color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training five spherical Gaussian cluster dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltGau
#| out-width: 100%

aic_gau_plot + mse_gau_plot +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```




## Applications {#sec-applications}

### Single-cell RNA-seq data of human

In the field of single-cell studies, a common analysis task involves clustering to identify groups of cells with similar expression profiles. Analysts often turn to NLDR techniques to verify and identify these clusters and explore developmental trajectories (e.g., example 1). In clustering workflows, the main objective is to verify the existence of clusters and subsequently identify them as specific cell types by examining the expression of "known" marker genes. In this context, a "faithful" embedding should ideally preserve the topology of the data, ensuring that cells corresponding to the same cell type are situated close to the high-dimensional space.
  
To begin our analysis, we installed the Peripheral Blood Mononuclear Cells (pbmc) data set obtained from 10x Genomics using the `SeuratData` R package [@Rahul2019], which facilitates the distribution of data sets in the form of Seurat objects [@Yuhan2021]. This data set contains 13,714 features across 2,700 samples within a single assay. The active assay is RNA, with 13,714 features representing different gene expressions. After loading the data set, we obtained the principal components (PCs) and assessed the variance explained by each PC. Based on this evaluation, we selected seven PCs, representing approximately 50% of the variance in gene expression, for further analysis.
  
  Next, we employed the UMAP technique with default parameter settings. As illustrated in @fig-pbmc, the cell types B and Platelet are well-separated in the UMAP layout. Moreover, CD14+ Mono, FCGR3A+ Mono, and DC form a distinct cluster, while Naive CD4 T, NK, Memory CD4 T, and CD8 T are grouped together in another cluster. The values utilized to construct the smooth low-dimensional manifold are presented in @tbl-table02. The linked video, demonstrating the tour with the model, showcases the generation of a smooth surface for this application, enabling a comprehensive exploration of the data's structure and relationships (see @fig-pbmc_sc).


```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/pbmc/pbmc.rds")
training_data_1 <- read_rds("data/pbmc/pbmc_training.rds")
test_1 <- read_rds("data/pbmc/pbmc_test.rds")

tSNE_pbmc <- read_rds("data/pbmc/pbmc_tsne_30.rds")
UMAP_pbmc <- read_rds("data/pbmc/pbmc_umap.rds")
PHATE_pbmc <- read_rds("data/pbmc/pbmc_phate.rds")
TriMAP_pbmc <- read_rds("data/pbmc/pbmc_trimap.rds")
PaCMAP_pbmc <- read_rds("data/pbmc/pbmc_pacmap.rds")

## Visualise embeddings

plot_list1_pbmc <- plot_tSNE_2D(tSNE_pbmc) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_pbmc <- plot_UMAP_2D(UMAP_pbmc) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_pbmc <- plot_PHATE_2D(PHATE_pbmc) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_pbmc <- plot_TriMAP_2D(TriMAP_pbmc) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_pbmc <- plot_PaCMAP_2D(PaCMAP_pbmc) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied for the training PBMC dataset: (a) tSNE (perplexity = 30), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5PBMC
#| out-width: 100%

plot_list1_pbmc + plot_list2_pbmc + plot_list3_pbmc + plot_list4_pbmc + plot_list5_pbmc +
  plot_layout(ncol=5)
```


```{r}
#| warning: false
#| echo: false

MSE_df_pbmc <- read_rds("data/pbmc/summary_pbmc.rds")
MSE_df_pbmc$method <- factor(MSE_df_pbmc$method, levels = c("tSNE", "UMAP", "PHATE", "TriMAP", "PaCMAP"))


## To draw with AIC
aic_plot_pbmc <- ggplot(MSE_df_pbmc |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_pbmc <- ggplot(MSE_df_pbmc, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training PBMC dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltPBMC
#| out-width: 100%

aic_plot_pbmc + mse_plot_pbmc +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```


### Single-Cell Tagged Reverse Transcription sequencing data of mouse

The Zeisel mouse brain dataset, obtained through Spatial Transcriptomics (STRT-Seq). Within this dataset, information is collected from a substantial 2,816 individual mouse brain cells. Each of these cells acts as a molecular snapshot, capturing the distinctive genetic activity within various cell types. This diversity spans neurons, glial cells, and other essential components of the brain, offering a comprehensive view of the cellular tapestry.

What makes this dataset particularly valuable is its ability to shed light on the spatial distribution of cells. Researchers can explore how gene expression patterns vary across different regions of the mouse brain, unlocking insights into the functional specialization of these regions and the intricate networks that underpin neural processes.

```{r}
#| warning: false
#| echo: false

## Import data
df_2 <- read_rds("data/zeisel/zeisel.rds")
training_data_1 <- read_rds("data/zeisel/zeisel_training.rds")
test_1 <- read_rds("data/zeisel/zeisel_test.rds")

tSNE_zeisel <- read_rds("data/zeisel/zeisel_tsne_30.rds")
UMAP_zeisel <- read_rds("data/zeisel/zeisel_umap.rds")
PHATE_zeisel <- read_rds("data/zeisel/zeisel_phate.rds")
TriMAP_zeisel <- read_rds("data/zeisel/zeisel_trimap.rds")
PaCMAP_zeisel <- read_rds("data/zeisel/zeisel_pacmap.rds")

## Visualise embeddings

plot_list1_zeisel <- plot_tSNE_2D(tSNE_zeisel) + #ggtitle("(a)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'a', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list2_zeisel <- plot_UMAP_2D(UMAP_zeisel) + #ggtitle("(b)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'b', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)


plot_list3_zeisel <- plot_PHATE_2D(PHATE_zeisel) + #ggtitle("(c)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'c', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)


plot_list4_zeisel <- plot_TriMAP_2D(TriMAP_zeisel) + #ggtitle("(d)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'd', x = Inf, y = Inf, hjust = 1.5, vjust = 1.5, size = 3)

plot_list5_zeisel <- plot_PaCMAP_2D(PaCMAP_zeisel) + #ggtitle("(e)") + 
  theme_linedraw() +
  theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotate(geom = 'text', label = 'e', x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, size = 3)
```

```{r}
#| echo: false
#| fig-cap: "2D layouts from different NLDR techniques applied for the training Zeisel mouse brain dataset: (a) tSNE (perplexity = 30), (b) UMAP (n_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n_inliers = 5, n_outliers = 4, n_random = 3), and (e) PaCMAP (n_neighbors = 10, init = random, MN_ratio = 0.9, FP_ratio = 2). Is there a best representation of the original data or are they all providing  equivalent information?"
#| label: fig-nldervis5Mouse
#| out-width: 100%

plot_list1_zeisel + plot_list2_zeisel + plot_list3_zeisel + plot_list4_zeisel + plot_list5_zeisel +
  plot_layout(ncol=5)
```


```{r}
#| warning: false
#| echo: false

MSE_df_zei <- read_rds("data/zeisel/summary_zei.rds")
MSE_df_zei$method <- factor(MSE_df_zei$method, levels = c("tSNE", "UMAP", "PHATE", "TriMAP", "PaCMAP"))

## To draw with AIC
aic_plot_zei <- ggplot(MSE_df_zei |> dplyr::filter(data_type == "training"), aes(x = number_of_bins,
                                                                                 y = total_error,
                                                                                 color = method
)) +
  geom_point() +
  geom_line() +
  #geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  #annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=-5000, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("AIC") +
  xlab("Total number of bins")
## Effective number of bins along x-axis

mse_plot_zei <- ggplot(MSE_df_zei, aes(x = number_of_bins,
                                       y = total_mse,
                                       color = method
)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.title = element_blank(), plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7)) +
  # geom_vline(xintercept = NROW(full_grid_with_hexbin_id)) +
  # annotate("text", x= (NROW(full_grid_with_hexbin_id) - 10), y=0.25, label=paste0("effective number of bins = ", as.character(NROW(full_grid_with_hexbin_id))), angle=90) +
  scale_colour_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00")) +
  ylab("MSE") +
  xlab("Total number of bins")

```

```{r}
#| echo: false
#| fig-cap: Goodness of fit statistics from different NLDR techniques applied to training Zeisel mouse brain dataset. What is the best NLDR technique to represent the original data in 2D?
#| label: fig-diagnosticpltZEI
#| out-width: 100%

aic_plot_zei + mse_plot_zei +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='collect', ncol = 2) &
  theme(legend.position='bottom', plot.tag = element_text(size = 8))
```

## Conclusion {#sec-conclusion}

<!--Our research introduces a comprehensive framework that leverages tours for interactive exploration of high-dimensional data coupled with a low-dimensional manifold, facilitated by the `quollr` R package. Regardless of the Non-Linear Dimension Reduction (NLDR) technique in use, our approach demonstrates effectiveness through simulation examples, particularly in the iterative removal of long edges for a smoother representation and capturing cluster variance.

In the example with doublets, our method successfully captures the tweak within each cluster, indicating the variance present within them. However, the model may not appear smooth in high-dimensional space due to considerable noise when the data has a piecewise linear geometry, such as the tree simulation.

The practical application of our framework, as showcased with the UMAP view, enables visual inspection of well-separated clusters. Furthermore, the combined tour and model provide a robust assessment of whether UMAP preserves the data structure and accurately transforms the data.

The advantages of our approach include its versatility across various NLDR techniques and the ability to generate interactive visualizations for detailed exploration. The tour provides an intuitive way to navigate and comprehend high-dimensional data while assessing the accuracy of dimensionality reduction.

However, one limitation is that the approach may be less effective in cases with significant noise, as seen in the tree simulation example. Additionally, while our method aids in visual verification, quantifying the accuracy of embeddings might require further evaluation metrics.

In conclusion, our framework presents a powerful tool for researchers and analysts in single-cell studies to assess their embeddings by visually inspecting them alongside the original data. By leveraging the advantages of tours and low-dimensional manifolds, our approach offers valuable insights into the data transformation process, empowering users to make informed decisions in analyzing high-dimensional data. Future work could enhance the method's robustness in the presence of noise and explore additional evaluation metrics for quantifying embedding accuracy.-->
  
  
## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
