---
title: "Looking at Non-Linear Dimension Reductions as Models in the Data Space"
format: 
    jasa-pdf:
        keep-tex: true
    jasa-html: default
author:
  - name: Jayani P.G. Lakshika
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-6265-6481
    email: jayani.piyadigamage@monash.edu
    url: https://jayanilakshika.netlify.app/
  - name: Dianne Cook
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3813-7155
    email: dicook@monash.edu 
    url: http://www.dicook.org/
  - name: Paul Harrison
    affiliations:
      - name: Monash University
        department: MGBP, BDInstitute
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0002-3980-268X
    email: 	paul.harrison@monash.edu
    url: 
  - name: Michael Lydeamore
    affiliations:
      - name: Monash University
        department: Econometrics & Business Statistics
        address: Clayton
        city: VIC 
        country: Austria
        postal-code: 3800
    orcid: 0000-0001-6515-827X
    email: michael.lydeamore@monash.edu
    url: 
  - name: Thiyanga S. Talagala
    affiliations:
      - name: University of Sri Jayewardenepura
        department: Statistics
        address: Gangodawila
        city: Nugegoda 
        country: Sri Lanka
        postal-code: 10100
    orcid: 0000-0002-0656-9789
    email: ttalagala@sjp.ac.lk 
    url: https://thiyanga.netlify.app/
tbl-cap-location: bottom
abstract: |
  Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional (high-D) data using non-linear transformation. The methods and parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. NLDR often exaggerates random patterns, sometimes due to the samples observed. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of high-D distributions. To help evaluate the NLDR we have developed an algorithm to show the 2D NLDR model in the high-D space, viewed with a tour. One can see if the model fits everywhere or better in some subspaces, or completely mismatches the data. It is used to evaluate which 2D layout is the best representation of the high-D distribution and see how different methods may have similar summaries or quirks.
  
keywords: [high-dimensional data, dimension reduction, hexagonal binning, low-dimensional manifold, tour, data vizualization, model in the data space]
keywords-formatted: [high-dimensional data, dimension reduction, hexagonal binning, low-dimensional manifold, tour, data vizualization, model in the data space]

bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(quollr)
library(dplyr)
# remotes::install_github("jlmelville/snedata")
library(snedata)
library(ggflowchart)
library(purrr) ## map function
library(gridExtra) ## for grid.arrange
library(rsample)
library(DT)
library(ggbeeswarm)
library(ggplot2)
library(readr)
library(tidyr)

library(Rtsne)
library(uwot)
library(phateR)
library(patchwork)
library(langevitour)
library(colorspace)
library(kableExtra)
library(grid)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label, position = c(0.92, 0.92)) {
  annotation_custom(grid::textGrob(label = label,
      x = unit(position[1], "npc"), y = unit(position[2], "npc"),
      gp = grid::gpar(cex = 1, col="grey70")))
}
```

```{r}
#| label: code-setup
set.seed(20240110)
source("nldr_code.R", local = TRUE)
```

<!-- 
Check-list before submission
* Is it all American spelling
* Spelling checked generally
* Code all runs given fresh workspace
* Code has a readme, explaining how the paper results are reproduced
* Re-write abstract
-->

\spacingset{1.0} <!--% command in JASA style, comment to go back to double spacing-->

## Introduction

Non-linear dimension reduction (NLDR) is popular for making a convenient low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data. Recently developed methods include t-distributed stochastic neighbor embedding (tSNE) [@laurens2008], uniform manifold approximation and projection (UMAP) [@leland2018], potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm [@moon2019], large-scale dimensionality reduction Using triplets (TriMAP) [@amid2022], and pairwise controlled manifold approximation (PaCMAP) [@yingfan2021]. However, the representation generated can vary dramatically from method to method, and with different choices of parameters or random seeds made using the same method (@fig-NLDR-variety). The dilemma for the analyst is then, **which representation to use**. The choice might result in different procedures used in the downstream analysis, or different inferential conclusions. The research described here provides new visual tools to aid with this decision. 

<!-- - What's the problem:

  Non-linear dimension reduction being used to summarise high-dimensional data.

  - Summary of literature

  Relevant high-d vis, NLDR history
-->

```{r}
# label: read-pbmc-nldr
# Read a variety of different NLDR representations of PBMC
# and plot them on same aspect ratio
clr_choice <- "#0077A3"
umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")

nldr1 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("a")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_5_min_dist_0.01.rds")

nldr2 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("b")

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_15_min_dist_0.99.rds")
nldr3 <- umap_pbmc |>
  ggplot(aes(x = UMAP1,
             y = UMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("c")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_5.rds")

nldr4 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("d")

tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_51.rds")

nldr5 <- tsne_pbmc |>
  ggplot(aes(x = tSNE1,
             y = tSNE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("e")

phate_pbmc <- read_rds("data/pbmc3k/pbmc_phate_5.rds")
nldr6 <- phate_pbmc |>
  ggplot(aes(x = PHATE1,
             y = PHATE2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("f")

trimap_pbmc <- read_rds("data/pbmc3k/pbmc_trimap_12_4_3.rds")
nldr7 <- trimap_pbmc |>
  ggplot(aes(x = TriMAP1,
             y = TriMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("g")

pacmap_pbmc <- read_rds("data/pbmc3k/pbmc_pacmap_30_random_0.9_5.rds")
nldr8 <- pacmap_pbmc |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2))+
  geom_point(alpha=0.1, size=1, colour=clr_choice) +
  interior_annotation("h")
```


```{r}
#| label: fig-NLDR-variety
#| echo: false
#| fig-cap: "Six different NLDR representations of the same data. Different techniques and different parameter choices are used. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical parameter choice. Would they make different decisions downstream in the analysis depending on which version seen? Which is the most accurate representation of the structure in high dimensions?"
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
# (a) UMAP (n_neighbors = 30, min_dist = 0.3), (b) UMAP (n_neighbors = 5, min_dist = 0.01), (c) UMAP (n_neighbors = 15, min_dist = 0.99), (f) tSNE (perplexity = 5), (g) tSNE (perplexity = 51), (l) TriMAP (n_inliers = 12, n_outliers = 4, n_random = 3), (q) PaCMAP (n_neighbors = 30, init = random, MN_ratio = 0.9, FP_ratio = 5)
nldr1 + nldr2 + nldr3 + nldr4 +
  nldr5 + nldr6 + nldr7 + nldr8 +
  plot_layout(ncol = 4)
```

The paper is organised as follows. @sec-background provides a summary of the literature on NLDR, and high-dimensional data visualization methods. @sec-method contains the details of the new methodology, including simulated data examples. Two applications illustrating the use of the new methodology for bioinformatics and image classification are in @sec-applications. Limitations and future directions are provided in @sec-discussion.

## Background {#sec-background}

<!-- - Connection between NLDR and MDS-->
Historically, \kD{}   representations of \pD{}   data have been computed using multidimensional scaling (MDS) [@borg2005], which includes principal components analysis (PCA) [@jolliffe2011] as a special case.  The \kD{}   representation can be considered to be a layout of points in \kD{}   produced by an embedding procedure that maps the data from \pD{}. In MDS, the \kD{}   layout is constructed by minimizing a stress function that differences distances between points in \pD{}   with potential distances between points in \kD{}. Various formulations of the stress function result in non-metric scaling [@saeed2018] and isomap [@silva2002]. Challenges in working with high-dimensional data, including visualization, are outlined in @johnstone2009. 

Many new methods for NLDR have emerged in recent years, all designed to better capture specific structures potentially existing in \pD{}. Here we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}   minimizing the divergence between two distributions, where the distributions are modeling the inter-point distances. PHATE, TriMAP and PaCMAP are examples of diffusion processes [@coifman2005] spreading to capture geometric shapes, that include both global and local structure.

The array of layouts in @fig-NLDR-variety illustrate what can emerge from the choices of method and parameters, and the random seed that initiates the computation. Key structures interpreted from these views suggest: (1) highly **separated clusters** (a, b, e, g, h) with the number ranging from 3-6; (2) **stringy branches** (f), and (3) **barely separated clusters** (c, d) which would **contradict** the other representations. 

It happens because these methods and parameter choices provide different lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to use linear projections. PCA is the classical approach, resulting in a set of new variables which are linear combinations of the original variables. Tours, defined by @lee2021, broaden the scope by providing movies of linear projections, that provide views the data from all directions. @lee2021 provides an review of the main developments in tours. There are many tour algorithms implemented, with many available in the R package `tourr` [@wickham2011], and versions enabling better interactivity in `langevitour` [@harisson2024] and `detourr` [@hart2022]. Linear projections are a safe way to view high-dimensional data, because they do not warp the space, so they are more faithful representations of the structure. 
However, linear projections can be cluttered, and global patterns can obscure local structure. The simple activity of projecting data from \pD{}   suffers from piling [@laa2022], where data concentrates in the center of projections. NLDR is designed to escape these issues, to exaggerate structure so that it can be observed. But as a result NLDR can hallucinate wildly, to suggest patterns that are not actually present in the data. 

The solution is to use the tour to examine how the NLDR is warping the space. This approach follows what @wickham2015 describes as *model-in-the-data-space*. The fitted model should be overlaid on the data, to examine the fit relative the spread of the observations. While this is straightforward, and commonly done when data is 2D, it is also possible in \pD{}, for many models, when a tour is used. 

@wickham2015 provides several examples of models overlaid on the data in \pD{}. In hierarchical clustering, a representation of the dendrogrom using points and lines can be constructed by augmenting the data with points marking merging of clusters. Showing the movie of linear projections reveals shows how the algorithm sequentially fitted the cluster model to the data. For linear discriminant analysis or model-based clustering the model can be indicated by $(p-1)\text{-}D$ ellipses. It is possible to see whether the elliptical shapes appropriately matches the variance of the relevant clusters, and to compare and contrast different fits. For PCA, one can display the \kD{} plane of the reduced dimension using wireframes of transformed cubes. Using a wireframe is the approach we take here, to represent the NLDR model in \pD{}.

<!-- Linked brushing as done by @article21

- Model-in-the-data-space: how can we represent the model, eg plane for PCA, grid of values for classification boundaries, ellipses for LDA and mclust, nets for SOM.--> 

## Method {#sec-method}

### What is the NLDR model?

At first glance, thinking of NLDR as a modeling technique might seem strange. It is a simplified representation or abstraction of a system, process, or phenomenon in the real world. The \pD{}   observations are the realization of the phenomenon, and the \kD{}   NLDR layout is the simplified representation. From a statistical perspective we can consider the distances between points in the \kD{}   layout to be variance that the model explains, and the (relative) difference with their distances in \pD{}   is the error, or unexplained variance. We can also imagine that the positioning of points in 2D represent the fitted values, that will have some prescribed position in \pD{}   that can be compared with their observed values. This is the conceptual framework underlying the more formal versions of factor analysis [@cfa69] and multidimensional scaling (MDS) [@borg2005]. (Note that, for this thinking the full \pD{}   data needs to be available, not just the interpoint distances.)
<!--### Notation -->

<!-- @tbl-notation summarises the notation used to explain the new methodology. The observed data is denoted as $\mathbfit{x}_{n \times p}$ where $x_{ij}$ would indicate the $i^{th}$ observation on the $j^{th}$ variable sampled from a population $\mathbfit{X}$. To refer to variable $j$, we would use $X_j$.--> 

<!--
$X_{n \times p} = \begin{bmatrix} \textbf{x} _{1} & \textbf{x}_ {2} & \cdots & \textbf{x}_{n} \\  \end{bmatrix}^\top$

$Y_{n \times d} = \begin{bmatrix} \textbf{y} _{1} & \textbf{y}_ {2} & \cdots & \textbf{y}_{n} \\  \end{bmatrix}^\top$

$C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})$

$C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})$ $p$-D mappings of 2D hexagonal bin centroids of the $k^{th}$ hexagon

-->

```{r}
#| label: tbl-notation
#| tbl-cap: "Summary of notation for describing new methodology."
# Notations used in the paper

notation_df <- read_csv("misc/notation.csv")

# Create the table
kable(notation_df, 
      format = "latex", 
      booktabs = TRUE, escape = FALSE) |>
  kable_styling(position = "center", 
                full_width = FALSE, 
                font_size = 12) |>
  row_spec(0, bold = TRUE) |>
  column_spec(1:2, width = c("3cm", "12cm"))
```

We define the NLDR as a function $g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}$, with (hyper-)parameters $\mathbfit{\theta}$. The parameters, $\mathbfit{\theta}$, depend on the choice of $g$, and can be considered part of model fitting in the traditional sense. Common choices for $g$ include functions used in tSNE, UMAP, PHATE, TriMAP, PaCMAP, or MDS, although in theory any function that does this mapping is suitable. <!--Any input requirements for the data (such as normalization, or preprocessing through the use of PCA or similar) is considered part of the function $g$.-->

With our goal being to make a representation of this 2D layout that can be lifted into high-dimensional space, the layout needs to be augmented to include neighbour information. A simple approach would be to triangulate the points and add edges. A more stable approach is to first hexagonally bin the data, reducing it from $n$ to $m\leq n$ observations, and connect the bin centroids. This process serves to reduce some noisiness in the resulting surface shown in \pD{}. The steps in this process are shown in @fig-NLDR-scurve, and documented below.


<!--UMAP applied for Scurve data-->
```{r}
umap_scurve <- read_rds(file = "data/s_curve/s_curve_umap.rds") 

scurve_scaled_obj <- gen_scaled_data(
  data = umap_scurve)

umap_scurve_scaled <- scurve_scaled_obj$scaled_nldr
lim1 <- scurve_scaled_obj$lim1
lim2 <- scurve_scaled_obj$lim2
r2 <- diff(lim2)/diff(lim1)

sc_ltr_pos <- c(0.08, 0.96)
sc_xlims <- c(-0.25, 1.25)
sc_ylims <- c(-0.4, 1.9)
# sc_xlims <- c(-0.25, 1.25)
# sc_ylims <- c(-0.25, 2)

nldr_scurve <- umap_scurve_scaled |> 
  ggplot(aes(x = UMAP1, y = UMAP2)) + 
  geom_point(alpha=0.5, colour="#000000", size = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)
```

<!--Full hexagonal grid with UMAP data-->

```{r}
## Compute hexbin parameters
num_bins_x_scurve <- 6

## Hexagonal binning to have regular hexagons
hb_obj_scurve <- hex_binning(
  data = umap_scurve_scaled, 
  bin1 = num_bins_x_scurve, 
  q = 0.1,
  r2 = r2)

## Data set with all centroids
all_centroids_df <- hb_obj_scurve$centroids

## Generate all coordinates of hexagons
hex_grid <- hb_obj_scurve$hex_poly

## To obtain the standardise counts within hexbins
counts_df <- hb_obj_scurve$std_cts
df_bin_centroids <- extract_hexbin_centroids(
  centroids_df = all_centroids_df, 
  counts_df = counts_df)

hex_grid_with_counts <- 
  dplyr::left_join(hex_grid,
                   counts_df, 
                   by = c("hex_poly_id" = "hb_id"))

hex_grid_scurve <- ggplot(
  data = hex_grid_with_counts, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = umap_scurve_scaled, 
             aes(x = UMAP1, y = UMAP2), 
             alpha = 0.5) +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("b", sc_ltr_pos)
```

<!--Non-empty bins with bin centroids-->

```{r}
hex_grid_nonempty <- hex_grid |>
  filter(hex_poly_id %in% df_bin_centroids$hexID)

hex_grid_nonempty_scurve <-  ggplot(
  data = hex_grid_nonempty, 
  aes(x = x, y = y)) +
  geom_polygon(color = "black", 
               aes(group = hex_poly_id), 
               fill = "#ffffff") +
  geom_point(data = df_bin_centroids, 
             aes(x = c_x, y = c_y), 
             color = "#33a02c") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("c", sc_ltr_pos) 
```

<!--2D model-->

```{r}
## Triangulate bin centroids
tr1_object_scurve <- tri_bin_centroids(
  df_bin_centroids, x = "c_x", y = "c_y")
tr_from_to_df_scurve <- gen_edges(
  tri_object = tr1_object_scurve)

## Compute 2D distances
distance_scurve <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_scurve, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_scurve <- find_lg_benchmark(
  distance_edges = distance_scurve, 
  distance_col = "distance")

trimesh_removed_scurve <- vis_rmlg_mesh(
  distance_edges = distance_scurve, 
  benchmark_value = benchmark_scurve, 
  tr_coord_df = tr_from_to_df_scurve, 
  distance_col = "distance") +
  xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("d", sc_ltr_pos) 
```

```{r}
#| label: fig-NLDR-scurve
#| echo: false
#| fig-cap: "Key steps for constructing the model on the UMAP layout ($k=2$) of the S-curve data: (a) data, (b) hexagon bins, (c) bin centroids, and (d) triangulated centroids."
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
 
nldr_scurve + hex_grid_scurve + 
  hex_grid_nonempty_scurve + 
  trimesh_removed_scurve +
  plot_layout(ncol = 4)
```

To illustrate the method, we use $7\text{-}D$ simulated data, which we call the "S-curve". It is constructed by setting $X_1 = \sin(a), X_2 = U(0, 2), X_3 = \text{sign}(a) \times (\cos(a) - 1), \forall a \in [-3\pi/2, 3\pi/2]$. The remaining variables $X_4, X_5, X_6, X_7$ are all uniform error, with small variance. We would consider $T=(X_1, X_2, X_3)$ to be the true model.

#### Scaling the data

It is beneficial to define the algorithm on data having a standard scale. Here the variables are scaled to [0, 1], but the upper bound can incorporate the aspect ratio produced by the NLDR $(r_1, r_2, ..., r_k)$, by setting them to $(y_{1,\text{max}}, y_{2,\text{max}}, ..., y_{k,\text{max}})$. When $k=2$ which is assumed for hexagon binning, $y_{1,\text{max}}=1$ and $y_{2,\text{max}} = \frac{r_2}{r_1}$, as observed in @fig-NLDR-scurve. <!-- \times \frac{2}{\sqrt{3}}$. (The $\frac{2}{\sqrt{3}}$ accounts for the different height ($a_1$) and width ($a_2$) of a regular hexagon.) The scaling of data should take the size of the hexagons into account, but choice of number of bins should. -->

#### Computing hexagon grid configurations

The 2D hexagon grid is defined by the number of bins in each direction $(b_1, b_2)$, as given by the centroids of each hexagon $(c_1, c_2)$ and the lower left position where the grid starts at $(s_1, s_2)$, which correspond to the lowest left centroid. The values of $s_i$ need to be below their respective minimum variable values, and could be a full bin lower, to allow a buffer ($q$) corresponding to a full hexagon width ($a_1$) and height ($a_2$) around the data. The values of $b_i$ are variables to be computed that define the reduction in size of the data ($n$ to $m$).

The value for $b_2$ is computed by fixing $b_1$. Considering the lower bound of the NLDR, $a_1 > 2 \times s_1$, and $a_1 > \frac{1-s_1}{b_1 -1}$. Similarly, according to the upper bound of the NLDR, $a_1 > \frac{2(r_2 - s_2)}{\sqrt{3}(b_2 - 1)}$, because $a_2 = \frac{\sqrt(3)}{2}a_1$ for regular hexagons. Therefore, $b_2 = \Big\lceil1 +\frac{2(r_2 - s_2)(b_1 - 1)}{\sqrt{3}(1-s_1)}\Big\rceil$.       

```{r}
hex_param_vis <- ggplot(data = hex_grid, aes(x = x, y = y)) + 
    geom_polygon(fill = "white", color = "#bdbdbd", aes(group = hex_poly_id)) +
    geom_point(data = all_centroids_df, aes(x = c_x, y = c_y), color = "#31a354") +
    geom_point(data = all_centroids_df |> dplyr::filter(hexID == 1), aes(x = c_x, y = c_y), color = "black") + ## starting point
    geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = r2), fill = "white", color = "black", alpha = 0, linewidth = 1) +
    geom_segment(x = 0, y = -0.35, xend = 1, yend = -0.35,
                 arrow = arrow(length = unit(0.03, "npc"), ends = "both"), color = "black")+
    annotate("text", x=0.5, y=-0.45, label= "r1", color = "black") +
    geom_segment(x = -0.25, y = 0, xend = -0.25, yend = r2,
                 arrow = arrow(length = unit(0.03, "npc"), ends = "both"), color = "black")+ 
    annotate("text", x=-0.35, y=1, label= "r2", color = "black") +
    geom_segment(x = -0.1 + 0.2087578, y = -0.15, xend = -0.1 + 0.2087578*2, yend = -0.15,
                   arrow = arrow(length = unit(0.03, "npc"), ends = "both"), color = "black")+ # a1 = 0.2087578
    annotate("text", x=0.2, y=-0.25, label= "a1", color = "black") +
    geom_segment(x = -0.15, y = -0.1*r2 + 0.1807896*2, xend = -0.15, yend = -0.1*r2 + 0.1807896*3,
                     arrow = arrow(length = unit(0.03, "npc"), ends = "both"), color = "black")+ # a2 = 0.1807896
    annotate("text", x=-0.15, y=0.45, label= "a2", color = "black") +
    annotate("text", x=-0.15, y=-0.35, label= "(s1, s2)", color = "black") ## starting point
```

```{r}
#| label: fig-hex-param
#| fig-cap: "Notations for hexagonal grid configurations."
#| fig-width: 3
#| fig-height: 3
#| fig-pos: H
 
hex_param_vis
```

<!-- Number of bins is set by fixing $b_1$, which determines the binwidth accounting the offset $q_1$ and breaks on $x_1$, is calculated as  -->

<!-- $$ -->
<!-- a_1 = \frac{r_1 + q_1}{b_1}. -->
<!-- $$ -->

<!-- The computed binwidth then determines the number of vertical bins, $b_2$, and vertical binwidth, which are computed based on $r_2$, the offset $q_2$, and the height/width ratio of a regular hexagon, $\frac{2}{\sqrt{3}}$ as -->

<!-- $$ -->
<!-- a_2 = \frac{r_2 + q_2}{0.75 \times b_2}. -->
<!-- $$ -->

<!-- To define a hexagonal grid across the \kD{} space, with hexagons of fixed height ($a_1$), width ($a_2$),  it is necessary to determine how many hexagons should be represented along each axis in the \kD{} space.

When $k=2$, the number of bins along the $x$ and $y$ axes, $b_1$ and $b_2$, is computed by considering the scaling factors ${r_1, r_2}$, along with the offset along the axes ($q_1$, $q_2$), and the height ($a_1$) and width ($a_2$) of a regular hexagon (@fig-NLDR-scurve (b)). ($b_1 = \frac{r_1 + q_1}{a_2}$, and $b_2 = \frac{r_2 + q_2}{0.75 \times a_1}$) where $0.75 \times a_1$ accommodate to have hexagons fill the entire 2D space without leaving any gaps between them.)-->

<!-- XXX We don't need parameters for the regular hexagon, these are fixed constants. -->


#### Binning the data

Points are allocated to the bin they fall into based on the nearest centroid. In situations where a point is equidistant from multiple centroids, tie-breaking rules are applied. If multiple centroids are in the same row, the point is assigned to the leftmost centroid. If multiple centroids are in different rows, the point is assigned to the bottom centroid.

<!-- XXX How are you doing this? Do you check the bounds of the hexagon? Or do you use distance to centroid? -->

<!--
Define a hexagonal grid across the \kD{} space, using hexagons with fixed height ($a_2$), width ($a_1$). Each of the \kD{} points will belong to a hexagonal bin. That is, for each $y \in \mathbfit{Y}$, we can (uniquely) identify the hexagon that the point belongs to. This identification is done finding the nearest bin centroid for the \kD{} points by considering the \kD{} Euclidean distance.    

When $k=2$, the starting coordinates $(s_1, s_2)$ mark the lower left of the grid. This is the bottom left bin centroid. By starting from there, points are generated to fill the grid accounting $b_1$, $b_2$, $a_1$, and $a_2$. 
-->

<!--We deliberately separate out the creation of the hexagonal grid from the mapping of points on the grid.-->
#### Obtaining bin representations

Let $h : \mathbb{R}^{n\times k} \rightarrow \mathbb{R}^{m\times k}$ be the function that maps a point in \kD{} space to it's bin representation ($C^{(k)}$). This is the model points in \kD{} space.

When $k = 2$, one of the representations of a bin is the hexagonal centroid (@fig-NLDR-scurve (c)).

#### Generating edges

Delaunay triangulation on bin representations generates edges by specifying which bin representations should be connected to generate the model in \kD{} space.

When $k = 2$ Delaunay triangulation on $C^{(2)}$ generates the model in 2D space, which is a triangular mesh (@fig-NLDR-scurve (d)). It generates convex hulls of $C^{(2)}$ such that the circumcircle of every triangle in the triangulation contains no other points from $C^{(2)}$.

### Displaying the model in \pD{}

The final step of the algorithm is _lifting_ the \kD{} model back to \pD{}, completing the *model-in-the-data-space* picture. For this, define the set $H^k$ to be the set of points that belong to centroid $k$. That is, $H^k = \{ y \; | \; g(y) = (h_x^k, h_y^k)\}$. We use the \pD{} Euclidean mean of the points in $H^k$ to map the centroid $(h_x^k, h_y^k)$ to a point in \pD{}. Let the $i$-th component of the \pD{} mean be

$$f_i = \frac{1}{\left|H^k\right|}\sum_{y \in H^k} y_i,$$

with corresponding \pD{} vector $\vec{f_i}$. Then, $f(h\circ g)(x)$ maps a \pD{} point to the \pD{} model estimate, completing the model cycle.

Furthermore, edges that exist between \kD{} representations should also generate edges in \pD{} by connecting \pD{} mapping of the corresponding \kD{} representations.

### Measuring the fit

### What can be learned

- Overview: Generate a form that maps the model, that is the interpoint distances. What is the model?
- Notation
- Create a representation of the model 
    - using hex-binning in 2D, 
    - parameters, 
    - tuning, 
    - pre-processing
- How does this map to the representation in high-d
    - Centroids, 
    - Edges
- Measuring fit
    - Fitted values
    - Error calculation
- What is learned about simulated examples
    - Interesting organisation of points in UMAP
    - 

## Applications {#sec-applications}

### pbmc

- NLDR view used to illustrate clusters
- Use our method to assess is it a reasonable representation
- Demonstrate that it is not
- Illustrate how to use our method to get a better representation

<!-- UMAP layout with author's suggested parameter choice-->
```{r}
## Import data
training_data_pbmc <- read_rds("data/pbmc3k/pbmc_pca_50.rds")
training_data_pbmc <- training_data_pbmc[, 1:9] |>
  mutate(ID = 1:NROW(training_data_pbmc))

umap_pbmc <- read_rds("data/pbmc3k/pbmc_umap_30_min_dist_0.3.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = umap_pbmc)
umap_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

umap_pbmc <- umap_pbmc_scaled |>
  ggplot(aes(x = UMAP1,
             y = UMAP2)) +
  geom_point(alpha=0.5) 
```

<!-- UMAP parameters suggested by the author(n_neighbors = 30, min_dist = 0.3)-->
```{r}
#| fig-cap: "2D layout from UMAP applied for the PBMC3k dataset. Is this a best representation of the original data?"
#| label: fig-umapauthor
#| fig-width: 3
#| fig-height: 3
#| fig-pos: H

umap_pbmc 
```

<!--Fit the best model for author suggestion and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_pbmc <- 30
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = umap_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  q = 0.1,
  r2 = r2_pbmc,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC_"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")

trimesh_removed_pbmc <- vis_rmlg_mesh(
  distance_edges = distance_pbmc,
  benchmark_value = benchmark_pbmc,
  tr_coord_df = tr_from_to_df_pbmc,
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "UMAP",
  col_start = "PC_")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    .default = "error greter than 20"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error greter than 20")))

## To join embedding
error_df <- error_df |>
  bind_cols(umap_pbmc_scaled |>
              select(-ID))

error_plot_pbmc <- error_df |>
  ggplot(aes(x = UMAP1,
             y = UMAP2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

<!--bin1 = 30, bin2 = 29, b = 870, non_empty = 132-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in the 2D space overlaid on UMAP data, and (b) high-D model error in model space."
#| label: fig-model-pbmc-author
#| fig-pos: H

trimesh_removed_pbmc + error_plot_pbmc + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```

<!--compute absolute error for different parameter choices-->
```{r}
error_df_umap <- read_csv("data/pbmc3k/error_df_umap.csv", col_names = FALSE)
names(error_df_umap) <- c("error", "mse", "bin1", "bin2", "b", "b_non_empty", 
                     "method", "n_neighbors", "min_dist")

error_df_umap <- error_df_umap |>
  mutate(type = paste0("n_neighbors: ", n_neighbors, ", min_dist: ", min_dist)) |>
  select(-c(n_neighbors, min_dist))

error_df_tsne <- read_csv("data/pbmc3k/error_df_tsne.csv", col_names = FALSE)
names(error_df_tsne) <- c("error", "mse", "bin1", "bin2", "b", "b_non_empty", 
                          "method", "perplexity")

error_df_tsne <- error_df_tsne |>
  mutate(type = paste0("perplexity: ", perplexity)) |>
  select(-perplexity)

error_df <- bind_rows(error_df_umap, error_df_tsne)

error_plot_pbmc <- ggplot(error_df, 
                              aes(x = b_non_empty, 
                                  y = log(error), 
                                  group = type, 
                                  colour = type)) + 
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 137, linetype="solid", 
             color = "black", size=0.8, alpha = 0.5) +
  scale_color_manual(values=c('#e41a1c','#377eb8','#4daf4a','#984ea3',
                              '#ff7f00','#a6cee3','#a65628','#f781bf')) +
  ylab("log(absolute error)") +
  xlab("number of non-empty bins")

```

```{r}
#| fig-cap: "Absolute error from UMAP and tSNE applied to training PBMC3k dataset with diffferent parameter choices. What is the best parameter choice to create the model? The residual plot have a steep slope at the beginning, indicating that a smaller number of non-empty bins causes a larger amount of error. Then, the slope gradually declines or level off, indicating that a higher number of non-empty bins generates a smaller error. Using the elbow method, it was observed that when the number of non-empty bins is set to 137, the lowest error occurred with the parameters perplexity: 30."
#| label: fig-abserrorPBMC
#| fig-pos: H
#| out-width: 100%

error_plot_pbmc
```

<!--best choice-->

```{r}
tsne_pbmc <- read_rds("data/pbmc3k/pbmc_tsne_30.rds")
pbmc_scaled_obj <- gen_scaled_data(
  data = tsne_pbmc)
tsne_pbmc_scaled <- pbmc_scaled_obj$scaled_nldr

tsne_pbmc <- tsne_pbmc_scaled |>
  ggplot(aes(x = tSNE1,
             y = tSNE2)) +
  geom_point(alpha=0.5)
```

<!--Fit the best model and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_pbmc <- 15
lim1 <- pbmc_scaled_obj$lim1
lim2 <- pbmc_scaled_obj$lim2
r2_pbmc <- diff(lim2)/diff(lim1) 

pbmc_model <- fit_highd_model(
  training_data = training_data_pbmc,
  emb_df = tsne_pbmc_scaled,
  bin1 = num_bins_x_pbmc,
  q = 0.1,
  r2 = r2_pbmc,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC_"
)

df_bin_centroids_pbmc <- pbmc_model$df_bin_centroids
df_bin_pbmc <- pbmc_model$df_bin

## Triangulate bin centroids
tr1_object_pbmc <- tri_bin_centroids(
  df_bin_centroids_pbmc, x = "c_x", y = "c_y")
tr_from_to_df_pbmc <- gen_edges(
  tri_object = tr1_object_pbmc)

## Compute 2D distances
distance_pbmc <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_pbmc,
  start_x = "x_from",
  start_y = "y_from",
  end_x = "x_to",
  end_y = "y_to",
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_pbmc <- find_lg_benchmark(
  distance_edges = distance_pbmc,
  distance_col = "distance")
benchmark_pbmc <- 0.1

trimesh_removed_pbmc <- vis_rmlg_mesh(
  distance_edges = distance_pbmc,
  benchmark_value = benchmark_pbmc,
  tr_coord_df = tr_from_to_df_pbmc,
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_pbmc,
  df_bin = df_bin_pbmc,
  training_data = training_data_pbmc,
  newdata = NULL,
  type_NLDR = "tSNE",
  col_start = "PC_")

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 5 ~ "error 0-5",
    row_wise_abs_error <= 10 ~ "error 5-10",
    row_wise_abs_error <= 15 ~ "error 10-15",
    row_wise_abs_error <= 20 ~ "error 15-20",
    row_wise_abs_error <= 25 ~ "error 20-25",
    .default = "error greter than 25"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-5", "error 5-10", "error 10-15", "error 15-20", "error 20-25", 
    "error greter than 25")))

## To join embedding
error_df <- error_df |>
  bind_cols(tsne_pbmc_scaled |>
              select(-ID))

error_plot_pbmc <- error_df |>
  ggplot(aes(x = tSNE1,
             y = tSNE2,
             color = type,
             group = ID)) +
  geom_point(alpha=0.5,
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)

```

<!--bin1 = 15, bin2 = 20, b = 300, non_empty = 137-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in the 2D space overlaid on tSNE data, and (b) high-D model error in model space."
#| label: fig-model-pbmc
#| fig-pos: H

trimesh_removed_pbmc + error_plot_pbmc + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```


### digits: 1

- NLDR is used to illustrate different ways 1's are drawn
- Use our method to assess is it a reasonable representation
- Demonstrate that it is, except for the anomalies 

<!-- PaCMAP layout with author's suggested parameter choice-->
```{r}
## Import data
training_data_mnist <- read_rds("data/mnist/mnist_10_pcs_of_digit_1.rds")
training_data_mnist <- training_data_mnist |>
  mutate(ID = 1:NROW(training_data_mnist))

pacmap_minst <- read_rds("data/mnist/mnist_pacmap.rds") |> 
  select(PaCMAP1, PaCMAP2, ID)
mnist_scaled_obj <- gen_scaled_data(
  data = pacmap_minst)
pacmap_minst_scaled <- mnist_scaled_obj$scaled_nldr 

position_df <- pacmap_minst_scaled |> 
  filter(ID %in% c(6475, 3311, 6489, 2347, 387, 
                          5239, 728, 517, 6369, 3055, 6345, 1761)) |>
  mutate(position = c(5, 9, 7, 8, 4, 11, 2, 6, 12, 10, 1, 3))

pacmap_plot_mnist <- pacmap_minst_scaled |> 
  ggplot(aes(x = PaCMAP1, 
             y = PaCMAP2)) + 
  geom_point(alpha=0.1) + 
  geom_text(data = position_df, 
            aes(x = PaCMAP1, y = PaCMAP2, label = position), 
            colour = "red", 
            size = 5)  
```

<!--Render the images in different selected location in 2D layout-->
```{r}
## Data with pixel values
mnist_data <- read_rds("data/mnist/mnist_digit_1.rds")

pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% c(3710, 2391, 2385, 5030, 6475,
                         7679, 3568, 1312, 3311, 1097,
                         3552, 7853, 6489, 7689, 6690,
                        1380, 6057, 2347, 5946, 3355,
                        4175, 3997, 5378, 387, 1854,
                        614, 3079, 1762, 5239, 3723,
                        5748, 728, 7419, 7794, 6233,
                        33, 2485, 5998, 318, 1761,
                        5690, 165, 517, 6935, 2682,
                        4962, 2264, 5563, 6369, 559,
                        5188, 4849, 2666, 3448, 3055,
                        3120, 6869, 6345, 4470, 7147)) |>
  mutate(instance.labs =  case_when(
    instance %in% c(3710, 2391, 2385, 5030, 6475) ~ "1",
    instance %in% c(7679, 3568, 1312, 3311, 1097) ~ "2",
    instance %in% c(3552, 7853, 6489, 7689, 6690) ~ "3",
    instance %in% c(1380, 6057, 2347, 5946, 3355) ~ "4",
    instance %in% c(4175, 3997, 5378, 387, 1854) ~ "5",
    instance %in% c(614, 3079, 1762, 5239, 372) ~ "6",
    instance %in% c(5748, 728, 7419, 7794, 6233) ~ "7",
    instance %in% c(5690, 165, 517, 6935, 2682) ~ "8",
    instance %in% c(4962, 2264, 5563, 6369, 559) ~ "9",
    instance %in% c(5188, 4849, 2666, 3448, 3055) ~ "10",
    instance %in% c(3120, 6869, 6345, 4470, 7147) ~ "11",
    .default = "12"
    )) 

## To generate labels
instance.labs <- as.character(rep(1:12, each = 5))

img_sample <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +
  facet_wrap(~ factor(instance, levels = c(3710, 2391, 2385, 5030, 6475,
                         7679, 3568, 1312, 3311, 1097,
                         3552, 7853, 6489, 7689, 6690,
                        1380, 6057, 2347, 5946, 3355,
                        4175, 3997, 5378, 387, 1854,
                        614, 3079, 1762, 5239, 3723,
                        5748, 728, 7419, 7794, 6233,
                        33, 2485, 5998, 318, 1761,
                        5690, 165, 517, 6935, 2682,
                        4962, 2264, 5563, 6369, 559,
                        5188, 4849, 2666, 3448, 3055,
                        3120, 6869, 6345, 4470, 7147)), ncol = 5) +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(), 
        legend.position = "none")
```

<!--PaCMAP param: n_components=2, n_neighbors=10, init=random, MN_ratio=0.9, FP_ratio=2.0-->
```{r}
#| fig-cap: "(a) 2D layout from PaCMAP applied for the digit 1 of the MNIST dataset. Is this a best representation of the original data?, and (b) Images of the handwritten digit 1. The angle of the digit 1 varies along non-linear structure."
#| label: fig-pacmapauthor
#| fig-pos: H

pacmap_plot_mnist + img_sample +
  plot_annotation(tag_levels = 'a')
```

<!-- Fit the model and compute error-->
```{r}
## Compute hexbin parameters
num_bins_x_mnist <- 37
lim1 <- mnist_scaled_obj$lim1
lim2 <- mnist_scaled_obj$lim2
r2_mnist <- diff(lim2)/diff(lim1) 

mnist_model <- fit_highd_model(
  training_data = training_data_mnist,
  emb_df = pacmap_minst_scaled,
  bin1 = num_bins_x_mnist,
  q = 0.1,
  r2 = r2_mnist,
  is_bin_centroid = TRUE,
  is_rm_lwd_hex = FALSE,
  col_start_highd = "PC"
)

df_bin_centroids_mnist <- mnist_model$df_bin_centroids
df_bin_mnist <- mnist_model$df_bin

## Triangulate bin centroids
tr1_object_mnist <- tri_bin_centroids(
  df_bin_centroids_mnist, x = "c_x", y = "c_y")
tr_from_to_df_mnist <- gen_edges(
  tri_object = tr1_object_mnist)

## Compute 2D distances
distance_mnist <- cal_2d_dist(
  tr_coord_df = tr_from_to_df_mnist, 
  start_x = "x_from", 
  start_y = "y_from", 
  end_x = "x_to", 
  end_y = "y_to", 
  select_vars = c("from", "to", "distance"))

## To find the benchmark value
benchmark_mnist <- find_lg_benchmark(
  distance_edges = distance_mnist, 
  distance_col = "distance")

trimesh_removed_mnist <- vis_rmlg_mesh(
  distance_edges = distance_mnist, 
  benchmark_value = benchmark_mnist, 
  tr_coord_df = tr_from_to_df_mnist, 
  distance_col = "distance") +
  #xlim(sc_xlims) + ylim(sc_ylims) +
  interior_annotation("a", sc_ltr_pos)

## Compute error
error_df <- augment(
  df_bin_centroids = df_bin_centroids_mnist, 
  df_bin = df_bin_mnist, 
  training_data = training_data_mnist, 
  newdata = NULL, 
  type_NLDR = "PaCMAP", 
  col_start = "PC") 

## Categorize error

error_df <- error_df |>
  mutate(type = case_when(
    row_wise_abs_error <= 2 ~ "error 0-2",
    row_wise_abs_error <= 4 ~ "error 2-4",
    row_wise_abs_error <= 6 ~ "error 4-6",
    row_wise_abs_error <= 6 ~ "error 6-8",
    row_wise_abs_error <= 10 ~ "error 8-10",
    .default = "error greter than 10"
  )) |>
  mutate(type = factor(type, levels = c(
    "error 0-2", "error 2-4", "error 4-6", "error 6-8", "error 8-10", 
    "error greter than 10")))

## To join embedding
error_df <- error_df |>
  bind_cols(pacmap_minst_scaled |> 
              select(-ID))
  
error_plot_mnist <- error_df |>
  ggplot(aes(x = PaCMAP1,
             y = PaCMAP2, 
             color = type, 
             group = ID)) +
  geom_point(alpha=0.5, 
             size = 0.1) +
  interior_annotation("b", sc_ltr_pos)
```

<!--bin1 = 37, bin2 = 26, b = 962, non_empty = 398-->
```{r}
#| echo: false
#| fig-cap: "(a) Model generated in the 2D space overlaid on PaCMAP data, and (b) high-D model error in model space."
#| label: fig-model-mnist
#| fig-pos: H

trimesh_removed_mnist + error_plot_mnist + 
  plot_layout(guides='collect', ncol=2) &
  theme(legend.position='bottom')
```

<!--images that occur large error-->
<!-- within the nonlinear structure-->
```{r}
pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% c( 4699, 4785, 5173, 6671, 2186, 4636, 4882, 
                          773, 2825, 6169, 6948, 5987, 1282, 7715, 
                          622, 2989, 6912, 4370, 1227, 4042, 2444, 
                          836, 1654)) 

imge_error_sample_within <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +  
  facet_wrap(~ factor(instance, levels = c( 4699, 4785, 5173, 6671, 2186, 4636, 
                                            4882, 773, 2825, 6169, 6948, 5987, 
                                            1282, 7715, 622, 2989, 6912, 4370, 
                                            1227, 4042, 2444, 836, 1654)), 
             ncol = 5) +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")
```

<!--outside the nonlinear structure-->
```{r}
img_error <- error_df |> 
  filter(row_wise_abs_error >= 8) |> 
  filter((!ID %in% c(4699, 4785, 5173, 6671, 2186, 4636, 4882, 773, 
                            2825, 6169, 6948, 5987, 1282, 7715, 622, 2989, 
                            6912, 4370, 1227, 4042, 2444, 836, 1654))) |>
  pull(ID)

pixels_gathered <-  mnist_data |>
  mutate(instance = row_number()) |>
  gather(pixel, value, -Label, -instance) |>
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) |>
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28) |>
  filter(instance %in% img_error) 

imge_error_sample <- pixels_gathered |> 
  ggplot(aes(x, y, fill = value)) +  
  geom_tile() +  
  facet_wrap(~ instance, ncol = 2) +
  scale_fill_continuous_sequential(palette = "Grays") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "none")
```

```{r}
#| echo: false
#| fig-cap: "(a) Images of handwritten digit 1 which occur large model error within the non-linear strcuture, and (b) Images of handwritten digit 1 which occur large model error outside the non-linear structure."
#| label: fig-anomalies
#| fig-width: 4
#| fig-height: 4
#| out-width: 100%
#| fig-pos: H

imge_error_sample_within + imge_error_sample +
  plot_layout(ncol=2) +
  plot_annotation(tag_levels = 'a')
```

     
## Discussion {#sec-discussion}

- Summarise contributions
- Explain where it is expected or not expected to work, eg higher dimensional relationships
- Human behaviour, the desire to have more certainty, and a tendency to prefer the well-separated views
- Predicting new observations in $k$-D
- Extending layouts beyond $k$-D, when 2D is clearly inadequate.
- Diagnostic app to explore differences in distances
- What might be useful enhancements

  
## References {.unnumbered}
  
::: {#refs}
:::
      
{{< pagebreak >}}
    
