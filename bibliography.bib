@article{article01,
  author  = {Yingfan Wang and Haiyang Huang and Cynthia Rudin and Yaron Shaposhnik},
  title   = {Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {201},
  pages   = {1--73},
  url     = {http://jmlr.org/papers/v22/20-1061.html}
}

@misc{article02,
      title = {TriMap: Large-scale Dimensionality Reduction Using Triplets},
      author = {Ehsan Amid and Manfred K. Warmuth},
      year = {2022},
      eprint = {1910.00204},
      archivePrefix = {arXiv},
      primaryClass = {cs.LG}
}

@article{article03,
  title = {Visualizing structure and transitions in high-dimensional biological data},
  author = {Kevin R. Moon and David van Dijk and Zheng Wang and Scott A. Gigante and Daniel B. Burkhardt and William S. Chen and Kristina Yim and Antonia van den Elzen and Matthew J. Hirn and Ronald R. Coifman and Natalia B. Ivanova and Guy Wolf and Smita Krishnaswamy},
  journal = {Nature Biotechnology},
  year = {2019},
  volume = {37},
  pages = {1482 - 1492}
}

@article{Leland2018,
  title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  author={Leland McInnes and John Healy},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.03426}
}

@article{Laurens2008,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605}
}

@article{article06,
 ISSN = {1364503X},
 URL = {http://www.jstor.org/stable/40485732},
 abstract = {We propose to furnish visual statistical methods with an inferential framework and protocol, modelled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests. Statistical significance of ' discoveries' is measured by having the human viewer compare the plot of the real dataset with collections of plots of simulated datasets. A simple but rigorous protocol that provides inferential validity is modelled after the 'lineup' popular from criminal legal procedures. Another protocol modelled after the 'Rorschach' inkblot test, well known from (pop-) psychology, will help analysts acclimatize to random variability before being exposed to the plot of the real data. The proposed protocols will be useful for exploratory data analysis, with reference datasets simulated by using a null assumption that structure is absent. The framework is also useful for model diagnostics in which case reference datasets are simulated from the model in question. This latter point follows up on previous proposals. Adopting the protocols will mean an adjustment in working procedures for data analysts, adding more rigour, and teachers might find that incorporating these protocols into the curriculum improves their students' statistical thinking.},
 author = {Andreas Buja and Dianne Cook and Heike Hofmann and Michael Lawrence and Eun-Kyung Lee and Deborah F. Swayne and Hadley Wlckham},
 journal = {Philosophical Transactions: Mathematical, Physical and Engineering Sciences},
 number = {1906},
 pages = {4361--4383},
 publisher = {Royal Society},
 title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
 urldate = {2023-06-14},
 volume = {367},
 year = {2009}
}


@article{article07,
  title={Attention by design: Using attention checks to detect inattentive respondents and improve data quality},
  author={James D. Abbey and Margaret G. Meloy},
  journal={Journal of Operations Management},
  year={2017},
  pages={63-70}
}



@article{article08,
	abstract = {Abstract Visualization can help in model building, diagnosis, and in developing an understanding about how a model summarizes data. This paper proposes three strategies for visualizing statistical models: (i) display the model in the data space, (ii) look at all members of a collection, and (iii) explore the process of model fitting, not just the end result. Each strategy is accompanied by examples, including manova, classification algorithms, hierarchical clustering, ensembles of linear models, projection pursuit, self-organizing maps, and neural networks.},
	author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike},
	doi = {https://doi.org/10.1002/sam.11271},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11271},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	keywords = {model visualization, exploratory data analysis, data mining, classification, high-dimensional data},
	number = {4},
	pages = {203-225},
	title = {Visualizing statistical models: Removing the blindfold},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11271},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11271},
	bdsk-url-2 = {https://doi.org/10.1002/sam.11271}}


@article{article09,
	abstract = {langevitour displays interactive animated 2D projections of high-dimensional datasets. Langevin Dynamics is used to produce a smooth path of projections. Projections are initially explored at random. A ``guide'' can be activated to look for an informative projection, or variables can be manually positioned. After a projection of particular interest has been found, continuing small motions provide a channel of visual information not present in a static scatter-plot. langevitour is implemented in Javascript, allowing for a high frame rate and responsive interaction, and can be used directly from the R environment or embedded in HTML documents produced using R. The widget is demonstrated using single-cell RNA sequencing (scRNA-Seq) data. langevitour's linear projections provide a less distorted view of this data than commonly used non-linear dimensionality reductions such as UMAP.Competing Interest StatementThe authors have declared no competing interest.},
	author = {{Paul Harrison}},
	doi = {10.1101/2022.08.24.505207},
	journal = {bioRxiv},
	month = jan,
	pages = {2022.08.24.505207},
	title = {langevitour: smooth interactive touring of high dimensions, demonstrated with {scRNA}-{Seq} data},
	url = {http://biorxiv.org/content/early/2022/08/26/2022.08.24.505207.abstract},
	year = {2022},
	bdsk-url-1 = {http://biorxiv.org/content/early/2022/08/26/2022.08.24.505207.abstract},
	bdsk-url-2 = {https://doi.org/10.1101/2022.08.24.505207}}


@Manual{article10,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Carson Sievert and Barret Schloerke and Yihui Xie and Jeff Allen and Jonathan McPherson and Alan Dipert and Barbara Borges},
  year = {2023},
  note = {R package version 1.7.4.9002},
  url = {https://shiny.rstudio.com/},
}

@Manual{article11,
  title = {googledrive: An Interface to Google Drive},
  author = {Lucy {D'Agostino McGowan} and Jennifer Bryan},
  year = {2023},
  note = {https://googledrive.tidyverse.org,
https://github.com/tidyverse/googledrive},
}

@Manual{article12,
  title = {googlesheets4: Access Google Sheets using the Sheets API V4},
  author = {Jennifer Bryan},
  year = {2023},
  note = {https://googlesheets4.tidyverse.org,
https://github.com/tidyverse/googlesheets4},
}


@Article{article13,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and
    Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
}

@Manual{article14,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2023},
    url = {https://www.R-project.org/},
}


@article{article15,
	abstract = {How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize `best-practice' data analysis procedures for scientists facing this challenge.},
	author = {Bolker, Benjamin M. and Brooks, Mollie E. and Clark, Connie J. and Geange, Shane W. and Poulsen, John R. and Stevens, M. Henry H. and White, Jada-Simone S.},
	doi = {https://doi.org/10.1016/j.tree.2008.10.008},
	issn = {0169-5347},
	journal = {Trends in Ecology \& Evolution},
	number = {3},
	pages = {127--135},
	title = {Generalized linear mixed models: a practical guide for ecology and evolution},
	url = {https://www.sciencedirect.com/science/article/pii/S0169534709000196},
	volume = {24},
	year = {2009},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0169534709000196},
	bdsk-url-2 = {https://doi.org/10.1016/j.tree.2008.10.008}}

@article{article16,
  title={On the histogram as a density estimator:L2 theory},
  author={David A. Freedman and Persi Diaconis},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  year={1981},
  volume={57},
  pages={453-476}
}


@article{article17,
	author = {Andreas Buja and Deborah F Swayne and Michael L Littman and Nathaniel Dean and Heike Hofmann and Lisha Chen},
	doi = {10.1198/106186008X318440},
	eprint = {https://doi.org/10.1198/106186008X318440},
	journal = {Journal of Computational and Graphical Statistics},
	number = {2},
	pages = {444-472},
	publisher = {Taylor & Francis},
	title = {Data Visualization With Multidimensional Scaling},
	url = {https://doi.org/10.1198/106186008X318440},
	volume = {17},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1198/106186008X318440}}


@article{article18,
	author = {Ovchinnikova, Svetlana and Anders, Simon},
	title = {Exploring dimension-reduced embeddings with Sleepwalk},
	elocation-id = {603589},
	year = {2020},
	doi = {10.1101/603589},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Dimension-reduction methods, such as t-SNE or UMAP, are widely used when exploring high-dimensional data describing many entities, e.g., RNA-seq data for many single cells. However, dimension reduction is commonly prone to introducing artefacts, and we hence need means to see where a dimension-reduced embedding is a faithful representation of the local neighbourhood and where it is not.We present Sleepwalk, a simple but powerful tool that allows the user to interactively explore an embedding, using colour to depict original or any other distances from all points to the cell under the mouse cursor. We show how this approach not only highlights distortions, but also reveals otherwise hidden characteristics of the data, and how Sleep-walk{\textquoteright}s comparative modes help integrate multi-sample data and understand differences between embedding and preprocessing methods. Sleepwalk is a versatile and intuitive tool that unlocks the full power of dimension reduction and will be of value not only in single-cell RNA-seq but also in any other area with matrix-shaped big data.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/04/30/603589},
	eprint = {https://www.biorxiv.org/content/early/2020/04/30/603589.full.pdf},
	journal = {bioRxiv}
}

@misc{article19,
  doi = {10.48550/ARXIV.1611.05469},

  url = {https://arxiv.org/abs/1611.05469},

  author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Viégas, Fernanda B. and Wattenberg, Martin},

  keywords = {Machine Learning (stat.ML), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Embedding Projector: Interactive Visualization and Interpretation of Embeddings},

  publisher = {arXiv},

  year = {2016},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{article20,
author={Pezzotti, Nicola and Lelieveldt, Boudewijn P. F. and Maaten, Laurens van der and Höllt, Thomas and Eisemann, Elmar and Vilanova, Anna},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Approximated and User Steerable tSNE for Progressive Visual Analytics},
year={2017},
volume={23},
number={7},
pages={1739-1752},
doi={10.1109/TVCG.2016.2570755}}

@misc{article21,
  doi = {10.48550/ARXIV.2012.06077},

  url = {https://arxiv.org/abs/2012.06077},

  author = {Lee, Stuart and Laa, Ursula and Cook, Dianne},

  keywords = {Other Statistics (stat.OT), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Casting Multiple Shadows: High-Dimensional Interactive Data Visualisation with Tours and Embeddings},

  publisher = {arXiv},

  year = {2020},

  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@Manual{article22,
  title = {detourr: Portable and Performant Tour Animations},
  author = {Casper Hart and Earo Wang},
  year = {2022},
  note = {R package version 0.1.0},
  url = {https://casperhart.github.io/detourr/},
}


@article{Karl1901,
	author = {Karl Pearson F.R.S.},
	doi = {10.1080/14786440109462720},
	eprint = {https://doi.org/10.1080/14786440109462720},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	number = {11},
	pages = {559-572},
	publisher = {Taylor & Francis},
	title = {LIII. On lines and planes of closest fit to systems of points in space},
	url = {https://doi.org/10.1080/14786440109462720},
	volume = {2},
	year = {1901},
	bdsk-url-1 = {https://doi.org/10.1080/14786440109462720}}



@inbook{article24,
	abstract = {Owing to the work of the International Statistical Institute,* and perhaps still more to personal achievements of Professor A.L. Bowley, the theory and the possibility of practical applications of the representative method has attracted the attention of many statisticians in different countries. Very probably this popularity of the representative method is also partly due to the general crisis, to the scarcity of money and to the necessity of carrying out statistical investigations connected with social life in a somewhat hasty way. The results are wanted in some few months, sometimes in a few weeks after the beginning of the work, and there is neither time nor money for an exhaustive research.},
	address = {New York, NY},
	author = {Neyman, Jerzy},
	booktitle = {Breakthroughs in Statistics: Methodology and Distribution},
	doi = {10.1007/978-1-4612-4380-9_12},
	isbn = {978-1-4612-4380-9},
	pages = {123--150},
	publisher = {Springer New York},
	title = {On the Two Different Aspects of the Representative Method: the Method of Stratified Sampling and the Method of Purposive Selection},
	url = {$https://doi.org/10.1007/978-1-4612-4380-9_12$},
	year = {1992},
	bdsk-url-1 = {$https://doi.org/10.1007/978-1-4612-4380-9_12$}}



@inbook{article25,
	abstract = {The problem of testing statistical hypotheses is an old one. Its origins are usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible ``causes'' of a given event.* Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand{\textdagger} and Borel,{\textdaggerdbl} whose differing views serve well to illustrate the point from which we shall approach the subject.},
	address = {New York, NY},
	author = {Neyman, J. and Pearson, E. S.},
	booktitle = {Breakthroughs in Statistics: Foundations and Basic Theory},
	doi = {10.1007/978-1-4612-0919-5_6},
	isbn = {978-1-4612-0919-5},
	pages = {73--108},
	publisher = {Springer New York},
	title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
	url = {$https://doi.org/10.1007/978-1-4612-0919-5_6$},
	year = {1992},
	bdsk-url-1 = {$https://doi.org/10.1007/978-1-4612-0919-5_6$}}


@article{article26,
	abstract = {This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.},
	author = {Lee, D. T. and Schachter, B. J.},
	doi = {10.1007/BF00977785},
	issn = {1573-7640},
	journal = {International Journal of Computer \& Information Sciences},
	month = jun,
	number = {3},
	pages = {219--242},
	title = {Two algorithms for constructing a {Delaunay} triangulation},
	url = {https://doi.org/10.1007/BF00977785},
	volume = {9},
	year = {1980},
	bdsk-url-1 = {https://doi.org/10.1007/BF00977785}}

@article{article27,
  title={API design for machine learning software: experiences from the scikit-learn project},
  author={Lars Buitinck and Gilles Louppe and Mathieu Blondel and Fabian Pedregosa and Andreas Mueller and Olivier Grisel and Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort and Jaques Grobler and Robert Layton and J. Vanderplas and Arnaud Joly and Brian Holt and Ga{\"e}l Varoquaux},
  journal={ArXiv},
  year={2013},
  volume={abs/1309.0238}
}


@article{Asimov1985,
	abstract = { The grand tour is a method for viewing multivariate statistical data via orthogonal projections onto a sequence of two-dimensional subspaces. The sequence of subspaces is chosen so that it is dense in the set of all two-dimensional subspaces. Desirable properties of such sequences of subspaces are considered, and several specific types of sequences are tested for rapidity of becoming dense. Tabulations are provided of the minimum length of a grand tour sequence necessary to achieve various degrees of denseness in dimensions up to 20. },
	author = {Asimov, Daniel},
	doi = {10.1137/0906011},
	eprint = {https://doi.org/10.1137/0906011},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	number = {1},
	pages = {128-143},
	title = {The Grand Tour: A Tool for Viewing Multidimensional Data},
	url = {https://doi.org/10.1137/0906011},
	volume = {6},
	year = {1985},
	bdsk-url-1 = {https://doi.org/10.1137/0906011}}



@article{article29,
	author = {Dianne Cook and Andreas Buja and Javier Cabrera and Catherine Hurley},
	doi = {10.1080/10618600.1995.10474674},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/10618600.1995.10474674},
	journal = {Journal of Computational and Graphical Statistics},
	number = {3},
	pages = {155-172},
	publisher = {Taylor & Francis},
	title = {Grand Tour and Projection Pursuit},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1995.10474674},
	volume = {4},
	year = {1995},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1995.10474674},
	bdsk-url-2 = {https://doi.org/10.1080/10618600.1995.10474674}}

@INPROCEEDINGS{article30,
  author={Lloyd, Errol Lynn},
  booktitle={18th Annual Symposium on Foundations of Computer Science (sfcs 1977)},
  title={On triangulations of a set of points in the plane},
  year={1977},
  volume={},
  number={},
  pages={228-240},
  doi={10.1109/SFCS.1977.21}}


@article{article31,
	abstract = {We present an efficient O(n+1/ε4.5-time algorithm for computing a (1+ε)-approximation of the minimum-volume bounding box of n points in R3. We also present a simpler algorithm whose running time is O(nlogn+n/ε3). We give some experimental results with implementations of various variants of the second algorithm.},
	author = {Barequet, Gill and Har-Peled, Sariel},
	doi = {https://doi.org/10.1006/jagm.2000.1127},
	issn = {0196-6774},
	journal = {Journal of Algorithms},
	keywords = {approximation, bounding box},
	number = {1},
	pages = {91--109},
	title = {Efficiently {Approximating} the {Minimum}-{Volume} {Bounding} {Box} of a {Point} {Set} in {Three} {Dimensions}},
	url = {https://www.sciencedirect.com/science/article/pii/S0196677400911271},
	volume = {38},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0196677400911271},
	bdsk-url-2 = {https://doi.org/10.1006/jagm.2000.1127}}


@article{article32,
	abstract = {A set ofn weighted points in general position in ℝddefines a unique regular triangulation. This paper proves that if the points are added one by one, then flipping in a topological order will succeed in constructing this triangulation. If, in addition, the points are added in a random sequence and the history of the flips is used for locating the next point, then the algorithm takes expected time at mostO(nlogn+n[d/2]). Under the assumption that the points and weights are independently and identically distributed, the expected running time is between proportional to and a factor logn more than the expected size of the regular triangulation. The expectation is over choosing the points and over independent coin-flips performed by the algorithm.},
	author = {Edelsbrunner, H. and Shah, N. R.},
	doi = {10.1007/BF01975867},
	issn = {1432-0541},
	journal = {Algorithmica},
	month = mar,
	number = {3},
	pages = {223--241},
	title = {Incremental topological flipping works for regular triangulations},
	url = {https://doi.org/10.1007/BF01975867},
	volume = {15},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1007/BF01975867}}

@book{article33,
author = {Frey, Pascal Jean and George, Paul-Louis},
title = {Mesh Generation: Application to Finite Elements},
year = {2007},
isbn = {1903398002},
publisher = {ISTE}
}

@article{article57,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}




@article{article34,
author = {Krizhevsky, Alex},
year = {2012},
month = {05},
pages = {},
title = {Learning Multiple Layers of Features from Tiny Images},
journal = {University of Toronto}
}

@article{article35,
title= {Columbia University Image Library (COIL-20)},
journal= {},
author= {S. A. Nene and S. K. Nayar and H. Murase},
year= {1996},
url= {http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php},
abstract= {To database is available in two versions. The first, [unprocessed], consists of images for five of the objects that contain both the object and the background. The second, [processed], contains images for all of the objects in which the background has been discarded (and the images consist of the smallest square that contains the object). For formal documentation look at the corresponding compressed technical report

"Columbia Object Image Library (COIL-20),"
S. A. Nene, S. K. Nayar and H. Murase,
Technical Report CUCS-005-96, February 1996.},
keywords= {},
terms= {},
license= {},
superseded= {}
}

@INPROCEEDINGS{article36,
  author={Samaria, F.S. and Harter, A.C.},
  booktitle={Proceedings of 1994 IEEE Workshop on Applications of Computer Vision},
  title={Parameterisation of a stochastic model for human face identification},
  year={1994},
  volume={},
  number={},
  pages={138-142},
  doi={10.1109/ACV.1994.341300}}

@article{article37,
  title={A global geometric framework for nonlinear dimensionality reduction.},
  author={Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
  journal={Science},
  year={2000},
  volume={290 5500},
  pages={
          2319-23
        }
}

@Book{Torgerson1967,
author = { Torgerson, Warren S. },
title = { Theory and methods of scaling },
publisher = { Wiley New York },
year = { 1967 },
type = { Book },
language = { English },
subjects = { Psychometrics. },
life-dates = { 1967 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn2266997 },
}

@InProceedings{article39,
  author =	{Daniel Engel and Lars H{\"u}ttenberger and Bernd Hamann},
  title =	{{A Survey of Dimension Reduction Methods for High-dimensional Data Analysis and Visualization}},
  booktitle =	{Visualization of Large and Unstructured Data Sets: Applications in Geospatial Planning, Modeling and Engineering - Proceedings of IRTG 1131 Workshop 2011},
  pages =	{135--149},
  series =	{OpenAccess Series in Informatics (OASIcs)},
  ISBN =	{978-3-939897-46-0},
  ISSN =	{2190-6807},
  year =	{2012},
  volume =	{27},
  editor =	{Christoph Garth and Ariane Middel and Hans Hagen},
  publisher =	{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{http://drops.dagstuhl.de/opus/volltexte/2012/3747},
  URN =		{urn:nbn:de:0030-drops-37475},
  doi =		{10.4230/OASIcs.VLUDS.2011.135},
  annote =	{Keywords: high-dimensional, multivariate data, dimension reduction, manifold learning}
}

  @Manual{article40,
    title = {{Rtsne}: T-Distributed Stochastic Neighbor Embedding using
      Barnes-Hut Implementation},
    author = {Jesse H. Krijthe},
    year = {2015},
    note = {R package version 0.16},
    url = {https://github.com/jkrijthe/Rtsne},
  }


@article{article41,
	abstract = { High-dimensional data is ubiquitous in scientific research and industrial production fields. It brings a lot of information to people, at the same time, because of its sparse and redundancy, it also brings great challenges to data mining and pattern recognition. Dimensionality reduction can reduce redundancy and noise, reduce the complexity of learning algorithms, and improve the accuracy of classification, it is an important and key step in pattern recognition system. In this paper, we overview the classical techniques for dimensionality reduction and review their properties, and categorize these techniques according to their implementation process. We deduce each algorithm in detail and intuitively show their underlying mathematical principles. Thereby, the focus is to uncover the optimization process for each technique. We compare the characteristics and limitations of each technique and summarize the scope of application, discussing a number of open problems and a perspective of research trend in future. },
	author = {Huang, Xuan and Wu, Lei and Ye, Yinsong},
	doi = {10.1142/S0218001419500174},
	eprint = {https://doi.org/10.1142/S0218001419500174},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	number = {10},
	pages = {1950017},
	title = {A Review on Dimensionality Reduction Techniques},
	url = {https://doi.org/10.1142/S0218001419500174},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1142/S0218001419500174}}

@inproceedings{article42,
  title={High-dimensional visualizations},
  author={Grinstein, Georges and Trutschl, Marjan and Cvek, Urska},
  booktitle={Proceedings of the Visual Data Mining Workshop, KDD},
  volume={2},
  pages={120},
  year={2001}
}


@article{article43,
	author = {Andreas Buja and Dianne Cook and Deborah F. Swayne Research Scientist},
	doi = {10.1080/10618600.1996.10474696},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/10618600.1996.10474696},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {78-99},
	publisher = {Taylor & Francis},
	title = {Interactive High-Dimensional Data Visualization},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474696},
	volume = {5},
	year = {1996},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474696},
	bdsk-url-2 = {https://doi.org/10.1080/10618600.1996.10474696}}



@article{article44,
	abstract = {Abstract Residual randomization in permutation procedures (RRPP) is an appropriate means of generating empirical sampling distributions for ANOVA statistics and linear model coefficients, using ordinary or generalized least-squares estimation. This is an especially useful approach for high-dimensional (multivariate) data. Here, we present an r package that provides a comprehensive suite of tools for applying RRPP to linear models. Important available features include choices for OLS or GLS coefficient estimation, data or dissimilarity matrix analysis capability, choice among types I, II, or III sums of squares and cross-products, various effect size estimation methods, and an ability to perform mixed-model ANOVA. The lm.rrpp function is similar to the lm function in many regards, but provides coefficient and ANOVA statistics estimates over many random permutations. The S3 generic functions commonly used with lm also work with lm.rrpp. Additionally, a pairwise function provides statistical tests for comparisons of least-squares means or slopes, among designated groups. Users have many options for varying random permutations. Compared to similar available packages and functions, RRPP is extremely fast and yields comprehensive results for downstream analyses and graphics, following model fits with lm.rrpp. The RRPP package facilitates analysis of both univariate and multivariate response data, even when the number of variables exceeds the number of observations.},
	author = {Collyer, Michael L. and Adams, Dean C.},
	doi = {https://doi.org/10.1111/2041-210X.13029},
	eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13029},
	journal = {Methods in Ecology and Evolution},
	keywords = {dissimilarity, generalized least-squares, high-dimensional data, multivariate},
	number = {7},
	pages = {1772-1779},
	title = {RRPP: An r package for fitting linear models to high-dimensional data using residual randomization},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13029},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13029},
	bdsk-url-2 = {https://doi.org/10.1111/2041-210X.13029}}

@ARTICLE{article45,
  author={Friedman, J.H. and Tukey, J.W.},
  journal={IEEE Transactions on Computers},
  title={A Projection Pursuit Algorithm for Exploratory Data Analysis},
  year={1974},
  volume={C-23},
  number={9},
  pages={881-890},
  doi={10.1109/T-C.1974.224051}}

@ARTICLE{article46,
  author={Bertini, Enrico and Tatu, Andrada and Keim, Daniel},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={Quality Metrics in High-Dimensional Data Visualization: An Overview and Systematization},
  year={2011},
  volume={17},
  number={12},
  pages={2203-2212},
  doi={10.1109/TVCG.2011.229}}


@article{Jia2022,
	abstract = {As basic research, it has also received increasing attention from people that the ``curse of dimensionality'' will lead to increase the cost of data storage and computing; it also influences the efficiency and accuracy of dealing with problems. Feature dimensionality reduction as a key link in the process of pattern recognition has become one hot and difficulty spot in the field of pattern recognition, machine learning and data mining. It is one of the most challenging research fields, which has been favored by most of the scholars' attention. How to implement ``low loss'' in the process of feature dimension reduction, keep the nature of the original data, find out the best mapping and get the optimal low dimensional data are the keys aims of the research. In this paper, two-dimensionality reduction methods, feature selection and feature extraction, are introduced; the current mainstream dimensionality reduction algorithms are analyzed, including the method for small sample and method based on deep learning. For each algorithm, examples of their application are given and the advantages and disadvantages of these methods are evaluated.},
	author = {Jia, Weikuan and Sun, Meili and Lian, Jian and Hou, Sujuan},
	doi = {10.1007/s40747-021-00637-x},
	issn = {2198-6053},
	journal = {Complex \& Intelligent Systems},
	month = jun,
	number = {3},
	pages = {2663--2693},
	title = {Feature dimensionality reduction: a review},
	url = {https://doi.org/10.1007/s40747-021-00637-x},
	volume = {8},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s40747-021-00637-x}}


@article{Johnstone2009,
	abstract = { Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue. },
	author = {Johnstone, Iain M. and Titterington, D. Michael},
	doi = {10.1098/rsta.2009.0159},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2009.0159},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	number = {1906},
	pages = {4237-4253},
	title = {Statistical challenges of high-dimensional data},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2009.0159},
	volume = {367},
	year = {2009},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2009.0159},
	bdsk-url-2 = {https://doi.org/10.1098/rsta.2009.0159}}


@article{Guo2023,
	abstract = {The creation of effective visualizations is a fundamental component of data analysis. In biomedical research, new challenges are emerging to visualize multi-dimensional data in a 2D space, but current data visualization tools have limited capabilities. To address this problem, we leverage Gestalt principles to improve the design and interpretability of multi-dimensional data in 2D data visualizations, layering aesthetics to display multiple variables. The proposed visualization can be applied to spatially-resolved transcriptomics data, but also broadly to data visualized in 2D space, such as embedding visualizations. We provide an open source R package escheR, which is built off of the state-of-the-art ggplot2 visualization framework and can be seamlessly integrated into genomics toolboxes and workflows.Availability and implementation The open source R package escheR is freely available on Bioconductor (bioconductor.org/packages/escheR).Competing Interest StatementThe authors have declared no competing interest.PCAprincipal component analysist-SNEt-distributed stochastic neighbor embeddingUMAPuniform manifold approximation and projectionscRNA-seqsingle-cell RNA-sequencingsnRNA-seqsingle-nucleus RNA-sequencingSRTspatially-resolved transcriptomicsEFNA5membrane-bound ligand ephrin A5EPHA5ephrin type-A receptor 5DLPFCdorsolateral prefrontal cortex},
	author = {Guo, Boyi and Huuki-Myers, Louise A. and Grant-Peters, Melissa and Collado-Torres, Leonardo and Hicks, Stephanie C.},
	doi = {article50},
	journal = {bioRxiv},
	note = {Publisher: Cold Spring Harbor Laboratory \_eprint: https://www.biorxiv.org/content/early/2023/06/08/2023.03.18.533302.full.pdf},
	title = {{escheR}: {Unified} multi-dimensional visualizations with {Gestalt} principles},
	url = {https://www.biorxiv.org/content/early/2023/06/08/2023.03.18.533302},
	year = {2023},
	bdsk-url-1 = {https://www.biorxiv.org/content/early/2023/06/08/2023.03.18.533302},
	bdsk-url-2 = {https://doi.org/10.1101/2023.03.18.533302}}

@ARTICLE {article51,
author = {M. Sedlmair and T. Munzner and M. Tory},
journal = {IEEE Transactions on Visualization and Computer Graphics},
title = {Empirical Guidance on Scatterplot and Dimension Reduction Technique Choices},
year = {2013},
volume = {19},
number = {12},
issn = {1941-0506},
pages = {2634-2643},
abstract = {To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often &#x27;good enough&#x27;, that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.},
keywords = {three-dimensional displays;encoding;principal component analysis;data visualization;data analysis},
doi = {10.1109/TVCG.2013.153},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}


@inproceedings{article52,
  title={A behavioral investigation of dimensionality reduction},
  author={Lewis, Joshua and Van der Maaten, Laurens and de Sa, Virginia},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  year={2012}
}



@article{article53,
	author = {Venna, Jarkko and Peltonen, Jaakko and Nybo, Kristian and Aidos, Helena and Kaski, Samuel},
	journal = {Journal of Machine Learning Research},
	number = {13},
	pages = {451--490},
	title = {Information {Retrieval} {Perspective} to {Nonlinear} {Dimensionality} {Reduction} for {Data} {Visualization}},
	url = {http://jmlr.org/papers/v11/venna10a.html},
	volume = {11},
	year = {2010},
	bdsk-url-1 = {http://jmlr.org/papers/v11/venna10a.html}}


@article{article54,
author = {Renka, R. J.},
title = {Algorithm 751: TRIPACK: A Constrained Two-Dimensional Delaunay Triangulation Package},
year = {1996},
issue_date = {March 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/225545.225546},
doi = {10.1145/225545.225546},
abstract = {TRIPACK is a Fortran 77 software package that employs an incremental algorithm to construct a constrained Delaunay traingulation of a set of points in the plane (nodes). The triangulation covers the convex hull of the nodes but may include polygonal constraint regions whose triangles are distinguishable from those in the remainder of the triangulation. This effectively allows for a nonconvex or multiply connected triangulation (the complement of the union of constraint regions) while retaining the efficiency of searching and updating a convex triangulation. The package provides a wide range of capabilities including an efficient means of updating the triangulation with nodal additions or deletions. For N nodes, the storage requirement is 13N integer storage locations in addition to the 2N nodal coordinates.},
journal = {ACM Trans. Math. Softw.},
month = {mar},
pages = {1–8},
numpages = {8},
keywords = {constrained Delaunay triangulation, interpolation}
}

@inproceedings{article55,
  title={A Method for Creating Mosaic Images Using Voronoi Diagrams.},
  author={Dobashi, Yoshinori and Haga, Toshiyuki and Johan, Henry and Nishita, Tomoyuki},
  booktitle={Eurographics (Short Presentations)},
  pages={341--348},
  year={2002}
}

@inproceedings{article56,
  title={Constrained delaunay triangulations},
  author={Chew, L Paul},
  booktitle={Proceedings of the third annual symposium on Computational geometry},
  pages={215--222},
  year={1987}
}



@article{article58,
	abstract = {Abstract Visualization can help in model building, diagnosis, and in developing an understanding about how a model summarizes data. This paper proposes three strategies for visualizing statistical models: (i) display the model in the data space, (ii) look at all members of a collection, and (iii) explore the process of model fitting, not just the end result. Each strategy is accompanied by examples, including manova, classification algorithms, hierarchical clustering, ensembles of linear models, projection pursuit, self-organizing maps, and neural networks.},
	author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike},
	doi = {https://doi.org/10.1002/sam.11271},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11271},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	keywords = {model visualization, exploratory data analysis, data mining, classification, high-dimensional data},
	number = {4},
	pages = {203-225},
	title = {Visualizing statistical models: Removing the blindfold},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11271},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11271},
	bdsk-url-2 = {https://doi.org/10.1002/sam.11271}}


@article{article59,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/1390754},
 abstract = {We propose a rudimentary taxonomy of interactive data visualization based on a triad of data analytic tasks: finding Gestalt, posing queries, and making comparisons. These tasks are supported by three classes of interactive view manipulations: focusing, linking, and arranging views. This discussion extends earlier work on the principles of focusing and linking and sets them on a firmer base. Next, we give a high-level introduction to a particular system for multivariate data visualization--XGobi. This introduction is not comprehensive but emphasizes XGobi tools that are examples of focusing, linking, and arranging views; namely, high-dimensional projections, linked scatterplot brushing, and matrices of conditional plots. Finally, in a series of case studies in data visualization, we show the powers and limitations of particular focusing, linking, and arranging tools. The discussion is dominated by high-dimensional projections that form an extremely well-developed part of XGobi. Of particular interest are the illustration of asymptotic normality of high-dimensional projections (a theorem of Diaconis and Freedman), the use of high-dimensional cubes for visualizing factorial experiments, and a method for interactively generating matrices of conditional plots with high-dimensional projections. Although there is a unifying theme to this article, each section--in particular the case studies--can be read separately.},
 author = {Andreas Buja and Dianne Cook and Deborah F. Swayne},
 journal = {Journal of Computational and Graphical Statistics},
 number = {1},
 pages = {78--99},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Interactive High-Dimensional Data Visualization},
 urldate = {2023-07-22},
 volume = {5},
 year = {1996}
}

@article{article60,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/1390772},
 abstract = {XGobi is a data visualization system with state-of-the-art interactive and dynamic methods for the manipulation of views of data. It implements 2-D displays of projections of points and lines in high-dimensional spaces, as well as parallel coordinate displays and textual views thereof. Projection tools include dotplots of single variables, plots of pairs of variables, 3-D data rotations, various grand tours, and interactive projection pursuit. Views of the data can be reshaped. Points can be labeled and brushed with glyphs and colors. Lines can be edited and colored. Several XGobi processes can be run simultaneously and linked for labeling, brushing, and sharing of projections. Missing data are accommodated and their patterns can be examined; multiple imputations can be given to XGobi for rapid visual diagnostics. XGobi includes an extensive online help facility. XGobi can be integrated in other software systems, as has been done for the data analysis language S, the geographic information system (GIS) ArcView™, and the interactive multidimensional scaling program XGvis. XGobi is implemented in the X Window System™ for portability as well as the ability to run across a network.},
 author = {Deborah F. Swayne and Dianne Cook and Andreas Buja},
 journal = {Journal of Computational and Graphical Statistics},
 number = {1},
 pages = {113--130},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {XGobi: Interactive Dynamic Data Visualization in the X Window System},
 urldate = {2023-07-22},
 volume = {7},
 year = {1998}
}

@Article{article61,
  author = {Hadley Wickham and Dianne Cook and Heike Hofmann and Andreas Buja},
  journal = {Journal of Statistical Software},
  number = {2},
  pages = {1––18},
  title = {tourr: An R package for exploring multivariate data with projections},
  url = {http://www.jstatsoft.org/v40/i02/},
  volume = {40},
  year = {2011},
  bdsk-url-1 = {http://www.jstatsoft.org/v40/i02/},
}

@article{article62,
  title={Nonmetric multidimensional scaling: a numerical method},
  author={Kruskal, Joseph B},
  journal={Psychometrika},
  volume={29},
  number={2},
  pages={115--129},
  year={1964},
  publisher={Springer-Verlag New York}
}

@article{article63,
  title={Global versus local methods in nonlinear dimensionality reduction},
  author={Silva, Vin and Tenenbaum, Joshua},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}

@article{article64,
author = {Coifman, Ronald and Lafon, S and Lee, Ann and Maggioni, Mauro and Nadler, B and Warner, F and Zucker, Steven},
year = {2005},
month = {06},
pages = {7426-31},
title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion Maps},
volume = {102},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.0500334102}
}

@article{article65,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@inproceedings{article66,
  title={Looking at large data sets using binned data plots},
  author={Daniel B. Carr},
  year={1992}
}


@article{article67,
	abstract = { Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori, hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application. },
	author = {Jolliffe, Ian T. and Cadima, Jorge},
	doi = {10.1098/rsta.2015.0202},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2015.0202},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	number = {2065},
	pages = {20150202},
	title = {Principal component analysis: a review and recent developments},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2015.0202},
	volume = {374},
	year = {2016},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2015.0202},
	bdsk-url-2 = {https://doi.org/10.1098/rsta.2015.0202}}

@inproceedings{article68,
author = {Howley, Tom and Madden, Michael and O'Connell, Marie-Louise and Ryder, Alan},
year = {2005},
month = {01},
pages = {209-222},
title = {The Effect of Principal Component Analysis on Machine Learning Accuracy with High Dimensional Spectral Data},
isbn = {978-1-84628-223-2},
doi = {10.1007/1-84628-224-1_16}
}

@article{article69,
  title={Reducing and clustering high dimensional data through principal component analysis},
  author={Indhumathi, R and Sathiyabama, S},
  journal={International Journal of Computer Applications},
  volume={11},
  number={8},
  pages={1--4},
  year={2010},
  publisher={Citeseer}
}

@misc{lee2021review,
      title={A Review of the State-of-the-Art on Tours for Dynamic Visualization of High-dimensional Data},
      author={Stuart Lee and Dianne Cook and Natalia da Silva and Ursula Laa and Earo Wang and Nick Spyrison and H. Sherry Zhang},
      year={2021},
      eprint={2104.08016},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
}

@article{ayesha2020overview,
  title={Overview and comparative study of dimensionality reduction techniques for high dimensional data},
  author={Ayesha, Shaeela and Hanif, Muhammad Kashif and Talib, Ramzan},
  journal={Information Fusion},
  volume={59},
  pages={44--58},
  year={2020},
  publisher={Elsevier}
}

@Manual{Ramnath2023,
  title = {htmlwidgets: HTML Widgets for R},
  author = {Ramnath Vaidyanathan and Yihui Xie and JJ Allaire and Joe Cheng and Carson Sievert and Kenton Russell},
  year = {2023},
  note = {R package version 1.6.2},
  url = {https://CRAN.R-project.org/package=htmlwidgets},
}

@Manual{Rahul2019,
  title = {SeuratData: Install and Manage Seurat Datasets},
  author = {Rahul Satija and Paul Hoffman and Andrew Butler},
  year = {2019},
  note = {http://www.satijalab.org/seurat,
https://github.com/satijalab/seurat-data},
}

@Article{Yuhan2021,
  author = {Yuhan Hao and Stephanie Hao and Erica Andersen-Nissen and William M. Mauck III and Shiwei Zheng and Andrew Butler and Maddie J. Lee and Aaron J. Wilk and Charlotte Darby and Michael Zagar and Paul Hoffman and Marlon Stoeckius and Efthymia Papalexi and Eleni P. Mimitou and Jaison Jain and Avi Srivastava and Tim Stuart and Lamar B. Fleming and Bertrand Yeung and Angela J. Rogers and Juliana M. McElrath and Catherine A. Blish and Raphael Gottardo and Peter Smibert and Rahul Satija},
  title = {Integrated analysis of multimodal single-cell data},
  journal = {Cell},
  year = {2021},
  doi = {10.1016/j.cell.2021.04.048},
  url = {https://doi.org/10.1016/j.cell.2021.04.048},
}


@article{Carr1987,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2289444},
 abstract = {High-performance interaction with scatterplot matrices is a powerful approach to exploratory multivariate data analysis. For a small number of data points, real-time interaction is possible and overplotting is usually not a major problem. When the number of plotted points is large, however, display techniques that deal with overplotting and slow production are important. This article addresses these two problems. Topics include density representation by gray scale or by symbol area, alternatives to brushing, and animation sequences. We also discuss techniques that are generally applicable, including interactive graphical subset selection from any plot in a collection of scatterplots and comparison of scatterplot matrices.},
 author = {D. B. Carr and R. J. Littlefield and W. L. Nicholson and J. S. Littlefield},
 journal = {Journal of the American Statistical Association},
 number = {398},
 pages = {424--436},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Scatterplot Matrix Techniques for Large N},
 urldate = {2024-01-13},
 volume = {82},
 year = {1987}
}


@Manual{Dan2023,
    title = {hexbin: Hexagonal Binning Routines},
    author = {Dan Carr and ported by Nicholas Lewin-Koh and Martin Maechler and contains copies of lattice functions written by Deepayan Sarkar},
    year = {2023},
    note = {R package version 1.28.3},
    url = {https://CRAN.R-project.org/package=hexbin},
  }


@article{Carr2013,
author = {Carr, Daniel and Olsen, Anthony and White, Denis},
year = {2013},
month = {03},
pages = {228-236},
title = {Hexagon Mosaic Maps for Display of Univariate and Bivariate Geographical Data},
volume = {19},
journal = {Cartography and Geographic Information Systems},
doi = {10.1559/152304092783721231}
}



@article{Daniel2010,
	abstract = { Scatter Plots are one of the most powerful and most widely used techniques for visual data exploration. A well-known problem is that scatter plots often have a high degree of overlap, which may occlude a significant portion of the data values shown. In this paper, we propose the generalized scatter plot technique, which allows an overlap-free representation of large data sets to fit entirely into the display. The basic idea is to allow the analyst to optimize the degree of overlap and distortion to generate the best-possible view. To allow an effective usage, we provide the capability to zoom smoothly between the traditional and our generalized scatter plots. We identify an optimization function that takes overlap and distortion of the visualization into acccount. We evaluate the generalized scatter plots according to this optimization function, and show that there usually exists an optimal compromise between overlap and distortion. Our generalized scatter plots have been applied successfully to a number of real-world IT services applications, such as server performance monitoring, telephone service usage analysis and financial data, demonstrating the benefits of the generalized scatter plots over traditional ones. },
	author = {Daniel A. Keim and Ming C. Hao and Umeshwar Dayal and Halldor Janetzko and Peter Bak},
	doi = {10.1057/ivs.2009.34},
	eprint = {https://doi.org/10.1057/ivs.2009.34},
	journal = {Information Visualization},
	number = {4},
	pages = {301-311},
	title = {Generalized Scatter Plots},
	url = {https://doi.org/10.1057/ivs.2009.34},
	volume = {9},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1057/ivs.2009.34}}





