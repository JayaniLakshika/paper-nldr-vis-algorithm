% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Visualising How Non-linear Dimension Reduction Warps Your Data},
  pdfauthor={Jayani P.G. Lakshika; Dianne Cook; Paul Harrison; Michael Lydeamore; Thiyanga S. Talagala},
  pdfkeywords={high-dimensional data, dimension
reduction, triangulation, hexagonal binning, low-dimensional
manifold, manifold learning, tour, data vizualization},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Visualising How Non-linear Dimension Reduction Warps Your
Data}
\author{
Jayani P.G. Lakshika\\
Econometrics \& Business Statistics, Monash University\\
and\\Dianne Cook\\
Econometrics \& Business Statistics, Monash University\\
and\\Paul Harrison\\
MGBP, BDInstitute, Monash University\\
and\\Michael Lydeamore\\
Econometrics \& Business Statistics, Monash University\\
and\\Thiyanga S. Talagala\\
Statistics, University of Sri Jayewardenepura\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
Non-Linear Dimension Reduction (NLDR) techniques have emerged as
powerful tools to visualize high-dimensional data. However, their
complexity and parameter choices may lead to distrustful or misleading
results. To address this challenge, we propose a novel approach that
combines the tour technique with a low-dimensional manifold generated
using NLDR techniques, hexagonal binning, and triangulation. This
integration enables a clear examination of the low-dimensional
representation in the original high-dimensional space. Our approach not
only preserves the advantages of both tours and NLDR but also offers a
more intuitive perception of complex data structures and facilitates
accurate data transformation assessments. The method and example data
sets are available in the \textbf{quollr} R package.
\end{abstract}

\noindent%
{\it Keywords:} high-dimensional data, dimension
reduction, triangulation, hexagonal binning, low-dimensional
manifold, manifold learning, tour, data vizualization
\vfill

\newpage
\spacingset{1} % DON'T change the spacing! (Default 1.9)

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, borderline west={3pt}{0pt}{shadecolor}, breakable, enhanced, interior hidden, boxrule=0pt, frame hidden]}{\end{tcolorbox}}\fi

\hypertarget{sec-intro}{%
\section{Introduction}\label{sec-intro}}

High-dimensional (high-D) data is prevalent across various fields, such
as ecology and bioinformatics \citep{Guo2023}, due to advancements in
data collection technologies \citep{Johnstone2009, ayesha2020overview}.
However, visualization of high-D data introduces significant challenges,
because the complexity of visualizing data beyond two dimensions
\citep{Jia2022}. In recent years, interactive and dynamic graphics
systems like \textbf{liminal} \citep{article21} ---which employs
interactive tools like brushing and linking \citep{article58}---and
software tools such as \textbf{XGobi}, \textbf{GGobi} \citep{article60},
\textbf{tourr} \citep{article61}, \textbf{detourr} \citep{article22},
and \textbf{langevitour} \citep{article09}, involving dynamic methods
like tours \citep{Asimov1985}, have played a key role in visualizing
high-D data (data-vis).

To create low-dimensional representations (typically in 2D) of high-D
data, it is common to apply dimension reduction (DR) techniques.
Approaches for DR involve linear methods such as principal component
analysis (PCA) \citep{Karl1901}, non-linear methods such as
multi-dimensional scaling (MDS) \citep{Torgerson1967}. In the past
decade, many new non-linear dimension reduction (NLDR) techniques have
emerged, such as t-distributed stochastic neighbor embedding (tSNE)
\citep{Laurens2008} and uniform manifold approximation and projection
(UMAP) \citep{Leland2018}, designed to capture the complex and
non-linear relationships present within high-D data
\citep{Johnstone2009}. NLDR techniques are the 2D models of high-D data
in our context.

Visualization of NLDR techniques (m-vis) \citep{article59} for the same
high-D data is particularly important in understanding and finding the
best representation. If we have done so, the 2D models can be quite
different from the data structure shown in high-D space. Therefore,
visualizing the 2D model in high-D space (m-in-dis) is more useful to
answer different types of questions:

\begin{itemize}
\item
  Is there a best representation of high-D data or are they all
  providing equivalent information? Is there a best parameter choice to
  fit the 2D model? How does the model change when it's parameters
  change?
\item
  How well the does the 2D models capture the data structure? Is the
  model fitting able to capture different data structure like
  non-linear, clustering?
\end{itemize}

If we can't easily ask and answer these questions, the ability of
understanding the models are restricted. To find the best 2D model and
the parameter choices, better understanding of the underlying science is
really important.

Also, the importance of m-vis along with data-vis has been recognized
and incorporated into interactive software, \textbf{liminal}
\citep{article21}. But the 2D model and high-D visualize side by side
and interactive like brushing and linking connect the data in the two
panels. To address this challenge, we propose a novel approach by
combining the tour technique with a low-dimensional manifold. This
manifold is created through the synergistic use of NLDR techniques,
hexagonal binning, and triangulation. This integration facilitates a
more understanding of the data structure, how well (or how poorly) NLDR
techniques perform.

The outline of this paper is as follows. The
Section~\ref{sec-background} provides an detailed overview of dimension
reduction methods, and tours. Building upon this foundation, the
Section~\ref{sec-methods} delves into the proposed algorithm, its
implementation details, how to tune the model, model summaries, and a
synthetic example to illustrate the functionality of the algorithm.
Subsequently, Section~\ref{sec-applications} showcases applications of
the algorithm on different data sets, particularly in single-cell
RNA-seq data. These applications reveal insights into the performance
and trustworthiness of NLDR algorithms. We analyze the results to
identify situations where NLDR techniques may lead to misleading
interpretations. Finally, \textbf{?@sec-conclusions} concludes by
summarizing the findings and emphasizing the significance of the
proposed approach in tackling the challenges of high-dimensional data
visualization.

\hypertarget{sec-background}{%
\section{Background}\label{sec-background}}

\hypertarget{dimension-reduction}{%
\subsection{Dimension Reduction}\label{dimension-reduction}}

Consider the high-dimensional data a rectangular matrix \(X\), where
\(X = \begin{bmatrix} \textbf{x}_{1} & \textbf{x}_{2} & \cdots & \textbf{x}_{n}\\ \end{bmatrix}^\top\),
with \(n\) observations in \(p\) dimensions. The objective is to
discover a low-dimensional projection
\(Y = \begin{bmatrix} \textbf{y}_{1} & \textbf{y}_{2} & \cdots & \textbf{y}_{n}\\ \end{bmatrix}^\top\),
represented as an \(n\) × \(d\) matrix, where \(d \ll p\). The reduction
process seeks to remove noise from the original data set while retaining
essential information.

There are two main categories of dimension reduction techniques: linear
and non-linear methods. Linear techniques involve a linear
transformation of the data, with one popular example being PCA. PCA
performs an eigen-decomposition of the sample covariance matrix to
obtain orthogonal principal components that capture the variance of the
data \citep{Karl1901}. However, linear methods may not fully capture
complex non-linear relationships present in the data.

In contrast, NLDR techniques generate the low-dimensional representation
\(Y\) from the high-dimensional data \(X\), often using pre-processing
techniques like \(k\)-nearest neighbors graph or kernel transformations.
Multidimensional Scaling (MDS) is a class of NLDR methods that aims to
construct an embedding \(Y\) in a low-dimensional space, approximating
the pair-wise distances in \(X\) \citep{Torgerson1967}. Variants of MDS
include non-metric scaling \citep{article62} and Isomap, which estimate
geodesic distances to create the low-dimensional representation
\citep{article63}. Other approaches based on diffusion processes, like
diffusion maps \citep{article64} and the PHATE algorithm
\citep{article03}, also fall under NLDR methods.

A challenge with NLDR methods is selecting and tuning appropriate
parameters. One specific technique we focus on is Pairwise Controlled
Manifold Approximation (PaCMAP). Similar considerations apply to related
methods like tSNE \citep{Laurens2008}, UMAP \citep{Leland2018}, and
TrMAP \citep{article02}.

It is important to note that methods like PCA and auto-encoders
\citep{article65} provide a reverse mapping from the low-dimensional
space back to the high-dimensional space, enabling data reconstruction.
However, many non-linear methods, including tSNE, prioritize
visualization and exploration over reconstruction. Their focus is on
capturing complex structures that may not be easily represented in the
original space, making a straightforward reverse mapping challenging.

\hypertarget{non-linear-dimension-reduction-techniques}{%
\subsubsection{Non-linear dimension reduction
techniques}\label{non-linear-dimension-reduction-techniques}}

Non-linear dimension reduction techniques play a crucial role in the
analysis and visualization of high-dimensional data, where the
complexities of relationships among variables may not be adequately
captured by linear methods. Among these techniques, tSNE stands out for
its emphasis on preserving pairwise distances. By minimizing the
divergence between probability distributions in both the high and
low-dimensional spaces, t-SNE effectively reveals intricate structures
and patterns within the data. Its application is widespread in tasks
requiring the visualization of clusters and local relationships, though
it does require careful consideration of the perplexity parameter for
optimal results.

UMAP is another powerful non-linear technique that strikes a balance
between preserving local and global structures. Constructing a fuzzy
topological representation using a weighted k-nearest neighbors graph,
UMAP optimizes the low-dimensional embedding to resemble this
representation. Known for its efficiency and scalability, UMAP is
versatile across various scales of relationships in the data, although
parameter sensitivity, particularly concerning the choice of neighbors,
must be taken into account.

For trajectory data, PHATE provides specialized capabilities. It models
the affinity between data points, simulating a heat diffusion process to
capture developmental processes, particularly in single-cell genomics.
While PHATE excels in revealing trajectory structures and offering
insights into cellular development, it necessitates careful parameter
tuning due to its specialized nature.

TriMAP adopts a unique approach by approximating the data manifold
through the construction of a triangulated graph representation. This
technique efficiently captures both global and local structures by
representing the data as a network of triangles. TriMAP's strength lies
in its ability to efficiently capture complex structures, albeit with
sensitivity to parameter choices, including the number of neighbors.

In contrast, PaCMAP introduces supervised learning to create a
low-dimensional representation while preserving pair-wise relationships.
Constructing a graph based on pair-wise distances, PaCMAP optimizes an
embedding using a customizable loss function. Particularly notable is
PaCMAP's flexibility in incorporating class labels or additional
information to guide the embedding process, offering users a means to
customize its behavior and performance.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-nldervis-1.pdf}

}

\caption{\label{fig-nldervis}2D layouts from different NLDR techniques
applied the same data: (a) tSNE (perplexity = 27), (b) UMAP
(n\_neighbors = 50), (c) PHATE (knn = 5), (d) TriMAP (n\_inliers = 5,
n\_outliers = 4, n\_random = 3), and (e) PaCMAP (n\_neighbors = 10, init
= random, MN\_ratio = 0.9, FP\_ratio = 2). Is there a best
representation of the original data or are they all providing equivalent
information?}

\end{figure}

\hypertarget{linear-overviews-using-tours}{%
\subsection{Linear overviews using
tours}\label{linear-overviews-using-tours}}

A tour is a powerful visualization technique used to explore
high-dimensional data by generating a sequence of projections, typically
into two dimensions. There are two main types of tours: the Grand Tour
and the Guided Tour. The Grand Tour explores the data's shape and global
structure by using random projections \citep{Asimov1985}. In contrast,
the Guided Tour focuses on specific patterns by moving towards
interesting projections defined by a predefined index function
\citep{article29}.

The process begins with a real data matrix \(X\) containing \(n\)
observations in \(p\) dimensions. It generates a sequence of \(p\) ×
\(d\) orthonormal projection matrices (bases), usually 1 or 2
dimensions. For each pair of orthonormal bases \(A_t\) and \(A_{t+1}\),
a geodesic path is interpolated to create smooth animation between
projections.

In the Grand Tour, new orthonormal bases are randomly chosen to explore
the \(d\)-dimensional subspace. The data is often sphered via principal
components to reduce dimensionality. The Guided Tour uses a predefined
index function to generate a sequence of `interesting' projections. The
resulting tour continuously visualizes the projected data \(Y_t\) =
\(XA_t\) as it interpolates between successive bases.

While both tours can be used to visualize data, examples often focus on
using the Grand Tour to observe global structures. However, software
like \textbf{langevitour} can visualize both types of tours, providing
flexibility for exploring high-dimensional data with various objectives.

\hypertarget{sec-methods}{%
\section{Methodology}\label{sec-methods}}

Our algorithm comprises two main phases: (1) generate the model in the
2D space, and (2) generate the model in the high-D space. These two
phases are described in details in this section.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=1\textheight]{figures/workflow.png}

}

\caption{\label{fig-meth}A flow diagram detailing the steps taken to
create the low-dimensional manifold in the high dimensional space. There
are two basic phases, one to generate the model in the 2D space, and
other to generate the model in the high-D space.}

\end{figure}

\hypertarget{preprocessing-steps}{%
\subsection{Preprocessing steps}\label{preprocessing-steps}}

In order to reduce the computational complexity associated with
performing NLDR techniques to high-D data, and as a initial step of
noise reduction of high-D data, PCA \citep[\citet{article68},
\citet{article69}]{article67} is applied as a preprocessing step.
Subsequently, the identified principal components, representing
directions of maximum variance, are used to construct the model.

\hypertarget{sec-construct2d}{%
\subsection{Constructing the 2D model}\label{sec-construct2d}}

\textbf{Step 1: Computing the hexagonal grid configuration}

The hexagonal grid, formed through hexagonal binning
\citep[\citet{article66}]{Carr1987}, serves as a type of bivariate
histogram employed to visualize the structure of high-D data. Hexagons,
being one of only three regular polygons capable of tessellating a plane
\citep{Carr2013}, possess both symmetry of nearest neighbors and the
maximum number of sides for a regular tessellation of the plane. This
unique combination makes hexagons more efficient in covering the plane
compared to other regular tessellations. Additionally, hexagons exhibit
lower visual bias when displaying densities, setting them apart from
other regular tessellations \citep{Dan2023}. In our algorithm, hexagonal
binning is used as the initial step of constructing the 2D model and the
total number of bins (\(b\)) is the crucial parameter that defines the
granularity of the hexagonal grid.

\textbf{(a) Determine the number of bins along the x-axis} (\(b_1\))

First, the number of bins along the x-axis (\(b_1\)) is computed using
the relationship between the diameter (\(h\)) and the area (\(A\)) of
regular hexagons (see Equation~\ref{eq-equation3}).

\begin{equation}\protect\hypertarget{eq-equation3}{}{
 \text{A} = \frac{\sqrt{3}}{2}h^2
}\label{eq-equation3}\end{equation}

To construct regular hexagons, \(A = 1\) (see Figure~\ref{fig-binsize})
use as the default setting. Then, the diameter (\(h\)) of the regular
hexagons is calculated (see Equation~\ref{eq-equation4}).

\begin{equation}\protect\hypertarget{eq-equation4}{}{
  \text{h} = \sqrt{\frac{2}{\sqrt{3}}A}
}\label{eq-equation4}\end{equation}

\citet{Carr2013} mentioned about the relationship between the diameter
(\(h\)) of regular hexagons and the height (\(y\)) of the plotting
region. According to our algorithm, the height (\(y\)) of the plotting
region is the the range of 2D embedding component 1 (\(r_1\)) (see
Equation~\ref{eq-equation6}).

\begin{equation}\protect\hypertarget{eq-equation6}{}{
h = \frac{r_1}{b_1}
}\label{eq-equation6}\end{equation}

After rearranging the Equation~\ref{eq-equation6} as
Equation~\ref{eq-equation5}, \(b_1\) is computed. The \(b_1\) value is
rounded up to the nearest whole number to have an integer value.

\begin{equation}\protect\hypertarget{eq-equation5}{}{
b_1 = \frac{r_1}{h}
}\label{eq-equation5}\end{equation}

\textbf{(b) Determine the shape parameter} (\(s\))

In this step, we determine the shape parameter (\(s\)) for the hexagonal
bins, which significantly influences their shape and arrangement within
the grid. The \(s\) in the hexagonal binning algorithm is defined as the
ratio of the height (\(y\)) to the width (\(x\)) of the plotting region
as defined in Equation~\ref{eq-equation1}.

\begin{equation}\protect\hypertarget{eq-equation1}{}{
s = \frac{y}{x}
}\label{eq-equation1}\end{equation}

The shape parameter (\(s\)) of our algorithm is calculated as the ratio
of the ranges of 2D embedding components, where \(r_1\) and \(r_2\)
represent the ranges of 2D embedding components 1 and component 2,
respectively (see Equation~\ref{eq-equation2}).

\begin{equation}\protect\hypertarget{eq-equation2}{}{
s = \frac{r_2}{r_1}
}\label{eq-equation2}\end{equation}

\textbf{(c) Determine the number of bins along the y-axis} (\(b_2\))

Next, the number of bins along the y-axis is computed based on the
number of bins along the x-axis (\(b_1\)) and the shape parameter
(\(s\)) (see Equation~\ref{eq-equation12}) \citep{Carr2013}.

\begin{equation}\protect\hypertarget{eq-equation12}{}{
b_2 = 2 * \left(\frac{(b_1 \times s)}{\sqrt(3)} + 1.5001 \right)
}\label{eq-equation12}\end{equation}

\textbf{(d) Determine the total number of bins} (\(b\))

The total number of bins is determined by multiplying the number of bins
along the x-axis (\(b_1\)) with the number of bins along the y-axis
(\(b_2\)) (see Equation~\ref{eq-equation13}).

\begin{equation}\protect\hypertarget{eq-equation13}{}{
b = b_1 \times b_2
}\label{eq-equation13}\end{equation}

\begin{figure}[H]

{\centering \includegraphics{paper_files/figure-pdf/fig-binsize-1.pdf}

}

\caption{\label{fig-binsize}Hexbin plots from different number of bins
for the \textbf{UMAP} projections of \textbf{S-curve} data: (a) b = 32
(4, 8), s = 1.643542, (b) b = 264 (12, 22), s = 1.643542, and (c) b =
840 (21, 40), s = 1.643542. The hexbins are colored based on the density
of points, with darker colors indicating higher point density and yellow
color representing lower point density within each bin. Does a value of
number of bins exist to effectively represent the low-dimensional data?}

\end{figure}

\textbf{Step 2: Obtain bin centroids}

As a result of hexagonal binning for high-D data, all the high-D data
are clustered into hexagons. In this step, the bin centroids
(\(C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})\)) (see Figure~\ref{fig-meth}
Step 2) are obtained \citep{Carr2013}.

\textbf{Step 3: Triangulate bin centroids}

In this step, the algorithm proceeds to triangulate the hexagonal bin
centroids (see Figure~\ref{fig-meth} Step 3). Triangulation is a
fundamental process in computational geometry and computer graphics that
involves dividing a set of points in a given space into interconnected
triangles \citep{article30}. One common algorithm used for triangulation
is Delaunay triangulation \citep[\citet{article54}]{article26}, where
points are connected in a way that maximizes the minimum angles of the
resulting triangles, leading to a more regular and well-conditioned
triangulation.

Since we are working with the centroids of regular hexagonal bins, the
resulting mesh will predominantly comprise equal-sized regular
triangles. However, the triangulation also helps span any gaps that may
exist between clusters of points, allowing for a more complete and
interconnected representation of the data.

\hypertarget{lifting-the-model-into-high-dimensions}{%
\subsection{Lifting the model into high
dimensions}\label{lifting-the-model-into-high-dimensions}}

\hypertarget{lifting-the-triangular-mesh-points-into-high-dimensions}{%
\subsubsection{Lifting the triangular mesh points into high
dimensions}\label{lifting-the-triangular-mesh-points-into-high-dimensions}}

\textbf{Step1: Cluster 2D points to hexagons}

Expanding upon the information regarding hexagonal binning discussed in
Step 1 of Section~\ref{sec-construct2d}, the primary objective in this
step is to determine the 2D embedding points associated with each
hexagon. As the initial process of hexagonal binning, the 2D points are
clustered into their respective hexagonal bins. By mapping this
information with the hexagonal bin centroids
(\(C_k^{(2)} \equiv (C_{ky_1}, C_{ky_2})\)) that obtained in Step 2 of
the 2D model building (see Section~\ref{sec-construct2d}), we can find
which 2D points are assigned to which data set (see
Figure~\ref{fig-wkhighD} (a)).

\textbf{Step2: Cluster high dimensional points to hexagons}

Following the step 1, the main focus in this step to determine the
corresponding high dimensional points for each hexagon. Every 2D
embedding point serves as a projection of a data point belonging to high
dimensional space. By using this mapping between the high dimensional
data and their corresponding projections, the high dimensional points
allocated to each hexagons are determined (see video linked in
Figure~\ref{fig-wkhighD}).

\textbf{Step3: Compute the mean within hexagon}

Having identified the high-dimensional points associated with hexagons,
the final step involves computing the mean within each hexagonal bin.
This implies calculating the average of the high-dimensional data points
located within each hex bin. These averaged high-dimensional data
points, denoted as \(C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p})\), serve
as the representative coordinates for the hex bin centroids within the
expansive high-dimensional space (see video linked in
Figure~\ref{fig-wkhighD}).

\hypertarget{lifting-the-2d-triangular-mesh-into-high-dimensions}{%
\subsubsection{Lifting the 2D triangular mesh into high
dimensions}\label{lifting-the-2d-triangular-mesh-into-high-dimensions}}

Based on the insights gained in Step 3 of Section~\ref{sec-construct2d},
where connected edges in the 2D triangular mesh were identified, this
step involves lifting these connections into the high dimensions. Having
the mappings of all 2D triangular mesh points into high dimensions, the
points connected in 2D are also connected in high dimensional space (see
video linked in Figure~\ref{fig-wkhighD}).

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-wkhighD-1.pdf}

}

\caption{\label{fig-wkhighD}How the 2D model lift into high dimensions?
(a) visualize the points and the hexagonal bin centroids related 2nd and
15th hexagons, (b) visualization of the edge connected the 2nd and 15th
hexagons (colored in red) in the triangular mesh. A video of tour
animation is available at
\url{https://www.youtube.com/watch?v=vBvM30Plt24}.}

\end{figure}

\hypertarget{tunning-the-model}{%
\subsection{Tunning the model}\label{tunning-the-model}}

In our model tuning process, we strategically adjust three key
parameters to optimize the performance and accuracy of our approach.
They are (i) the total number of bins (\(b\)), (ii) a benchmark value to
remove low-density hexagons, and (iii) a benchmark value to remove long
edges.

\hypertarget{total-number-of-bins}{%
\subsubsection{Total number of bins}\label{total-number-of-bins}}

Adjusting the parameter \(b_1\) provides control over the total number
of bins \(b\), allowing us to fine-tune the hexagonal grid.

\hypertarget{benchmark-value-to-remove-low-density-hexagons}{%
\subsubsection{Benchmark value to remove low-density
hexagons}\label{benchmark-value-to-remove-low-density-hexagons}}

Addressing low-density hexagons is a systematic process to handle
sparsely represented data in certain regions. For each hex bin, we
identify the six nearest hex bins using an equal 2D distance metric.
Then, we calculate the mean density, as outlined in the equations:

\begin{equation}\protect\hypertarget{eq-equationp2}{}{
\text{standard count} = \frac{\text{count}}{\text{max count}} 
}\label{eq-equationp2}\end{equation}

\begin{equation}\protect\hypertarget{eq-equationp3}{}{
\text{mean density} = \frac{\text{standard count}}{6} 
}\label{eq-equationp3}\end{equation}

The standard count is derived from the number of observations in the hex
bins. By examining the distribution of mean densities and designating
the first quartile as the benchmark value, hex bins with mean densities
below this benchmark are removed. This process ensures the elimination
of regions with insufficient data density, focusing on areas with more
significant data representation and preserving the overall structure in
the low-dimensional space.

\hypertarget{benchmark-value-to-remove-long-edges}{%
\subsubsection{Benchmark value to remove long
edges}\label{benchmark-value-to-remove-long-edges}}

The removal of long edges is a critical step to create a smoother
representation by iteratively eliminating hexagons with excessive
distances between centroids. This process eliminates outliers and noise
while preserving essential local relationships within the data. To
achieve this, distances between vertices are sorted, and unique distance
values are extracted. By setting a threshold based on the largest
difference between consecutive distance values, long edges are
identified and removed. This refinement step contributes to enhancing
the quality of the triangular mesh, ensuring a more accurate
representation of the data structure.

\begin{itemize}
\tightlist
\item
  Compute high-D distances (using averaged high-D data)
\item
  Fit a linear model using response as distance\_high\_D and predictor
  as distance\_2D
\item
  Filter the model residuals only for short edges
\item
  compute error for each edge connection as (residual for that
  point)/(total residual for all edges * number of short edges)
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltScurvelgrm-1.pdf}

}

\caption{\label{fig-diagnosticpltScurvelgrm}Goodness of fit statistics
from different NLDR techniques applied to training S-curve dataset with
different benchmark values to remove the long edges. What is the
effective benchmark value to remove the long edges?}

\end{figure}

\hypertarget{sec-summary}{%
\subsection{Model summaries}\label{sec-summary}}

\hypertarget{predicted-values-and-residuals}{%
\subsubsection{Predicted values and
residuals}\label{predicted-values-and-residuals}}

The approach involves employing the K-nearest neighbors (KNN) algorithm
to identify the nearest hexagonal bin centroid in the 2D space.
Subsequently, the coordinates of this centroid are assigned as the
low-dimensional predicted values for the test data in 2D space. It is
noteworthy that traditional NLDR methods, such as tSNE, often lack a
direct predict function, making our approach valuable for generating
predicted values in the absence of such functionalities.

The concept of ``residuals'' is pivotal in evaluating the accuracy of
representing bin centroids in high dimensions. To quantify this
accuracy, we introduce an error metric, which measures the sum of
squared differences between the high-dimensional data (\(x_{ij}\)) and
the predicted bin centroid data in high-dimensional space (\(C_{x_ij}\))
across all bins and dimensions. Mathematically, this error is expressed
as:

\begin{equation}\protect\hypertarget{eq-equation11}{}{
\text{Error} = \sum_{j = 1}^{n}\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2
}\label{eq-equation11}\end{equation}

Here, \(n\) represents the number of bins, \(p\) represents the
dimensions, \(x_{ij}\) is the actual high-dimensional data, and
\(C_{x_ij}\) is the predicted bin centroid data in high dimensions.

The error metric outlined above provides valuable insights into the
overall accuracy of our predictive model. By quantifying the squared
deviations between the actual and predicted values across all bins and
dimensions, we gain a comprehensive understanding of how well our method
captures and represents the underlying structure of the data in the
reduced 2D space. This assessment is crucial for evaluating the efficacy
of our NLDR technique in preserving the essential information present in
the original high-dimensional data.

\hypertarget{goodness-of-fit-statistics}{%
\subsubsection{Goodness of fit
statistics}\label{goodness-of-fit-statistics}}

Moving on to the assessment of prediction accuracy, we calculate the
Mean Squared Error (MSE). The MSE helps measure the average squared
differences between the actual high-dimensional data (\(x_{ij}\)) and
the predicted bin centroid data in high-D (\(C_{x_ij}\)) values across
all bins. Mathematically, this is expressed as:

\begin{equation}\protect\hypertarget{eq-equation9}{}{
\text{MSE} = \sum_{j = 1}^{n} \frac{\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2}{b}
}\label{eq-equation9}\end{equation}

Here, \(b\) signifies the total number of bins, \(p\) denotes the number
of dimensions in the high-dimensional data, and \(n\) represents the
number of observations.

Additionally, we gauge the model's performance using the Akaike
Information Criterion (AIC), calculated by the formula:

\begin{equation}\protect\hypertarget{eq-equation10}{}{
\text{AIC} = 2bp + np * log(\text{MSE})
}\label{eq-equation10}\end{equation}

These metrics, MSE and AIC, collectively offer valuable insights into
the model's predictive performance, considering both accuracy and
complexity in the predictions.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltScurve-1.pdf}

}

\caption{\label{fig-diagnosticpltScurve}Goodness of fit statistics from
different NLDR techniques applied to training S-curve dataset. What is
the best NLDR technique to represent the original data in 2D?}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-nldervisUMAP-1.pdf}

}

\caption{\label{fig-nldervisUMAP}2D layouts from UMAP applied for the
S-curve data: (a) UMAP (n\_neighbors = 7), (b) UMAP (n\_neighbors = 15),
(c) UMAP (n\_neighbors = 32), (d) UMAP (n\_neighbors = 50). Is there a
best parameter choice in representing UMAP or are they all providing
equivalent information?}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltDiffParam-1.pdf}

}

\caption{\label{fig-diagnosticpltDiffParam}Goodness of fit statistics
from different n\_neighbors parameter of UMAP applied to training
S-curve dataset. What is the best parameter choice in UMAP to represent
the original data in 2D?}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltScurvelwd-1.pdf}

}

\caption{\label{fig-diagnosticpltScurvelwd}Goodness of fit statistics
from different NLDR techniques applied to training S-curve dataset with
different benchmark values to remove the low-density heaxgons. What is
the effective benchmark value to remove the low-density heaxgons?}

\end{figure}

\hypertarget{sec-prediction}{%
\subsection{Prediction}\label{sec-prediction}}

In this context, ``prediction'' denotes the 2D embedding generated for
the NLDR technique. The methodology encompasses identifying the nearest
averaged high-D points in the high-D space for the test data, by
computing high-D Euclidean distances. As the averaged high-D point
corresponds to the lifting of the 2D model, determining its nearest
counterpart allows us to map its hexagonal bin centroid coordinates.
Consequently, these centroid coordinates serve as the assigned
low-dimensional predicted values for the test data in the 2D space.

Some NLDR techniques, such as tSNE, often lack a direct prediction,
making our approach valuable for generating predicted values in the
absence of such functionalities.

\hypertarget{model-interpretation}{%
\subsection{Model interpretation}\label{model-interpretation}}

The visualizations of NLDR techniques for the S-curve dataset shows
different representations in 2D, as showcased in Figure
Figure~\ref{fig-nldervis}. The question at hand is determining the best
NLDR technique to represent the S-curve data. Examining the 2D layouts
in Figure~\ref{fig-nldervis} suggests that TriMAP or PaCMAP may be best
representations. However, the challenge is to decide the best
representation with considerable evidence.

After considering the models constructed from the algorithm for each
NLDR techniques and visualizing them in high-D space with the original
data, shows that all the NLDR techniques capture the non-linearity of
the S-curve (see videos of the models linked in
Figure~\ref{fig-modelScurve}). But thorough examination of each model in
high-D space shows some surprising findings. The model of PHATE is
flattened and lack of capturing the width of S-curve (see videos of the
model with PHATE linked in Figure~\ref{fig-modelScurve}). This nature of
PHATE is also evicted by the considerably higher AIC and MSE values for
PHATE as shown in Figure~\ref{fig-diagnosticpltScurve}. Furthermore,
there are two edges that are not connected to any points in the model of
TriMAP (see see videos of the model with TriMAP linked in
Figure~\ref{fig-modelScurve}). This model shows how the patterns in 2D
representation of TriMAP capture by the model (see
Figure~\ref{fig-nldervis} (d)). As shown in Figure~\ref{fig-nldervis}
(b), UMAP have four clusters in a curvilinear structure. However, the
model reveals that even though UMAP shows a curvilinear in 2D, UMAP
actually capture the structure of the S-curve without any disjoint in
the model (see see videos of the model with UMAP linked in
Figure~\ref{fig-modelScurve}). In the 2D layouts of tSNE and PaCMAP (see
Figure~\ref{fig-nldervis} (a), (e)), there are some sparse areas that
are captured by the models effectively (see see videos of the model with
tSNE and PaCMAP linked in Figure~\ref{fig-modelScurve}).

Different NLDR techniques perform differently on the S-curve data and
tSNE provides the best representation in 2D according to AIC and MSE
values (see Figure~\ref{fig-diagnosticpltScurve}).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-modelScurve-1.pdf}

}

\caption{\label{fig-modelScurve}Is there a best model to represent the
original data in 2D space or are they all providing equivalent
information?, (a) Model in the 2D space with tSNE
(\url{https://youtu.be/uy9SkAo6gAE}), (b) Model in the 2D space with
UMAP (\url{https://youtu.be/0MJDhHrh_Ug}), (c) Model in the 2D space
with PHATE (\url{https://youtu.be/HbVv0uy0QWk}), (d) Model in the 2D
space with TriMAP (\url{https://youtu.be/2OGwipAjzc8}), and (e) Model in
the 2D space with PaCMAP (\url{https://youtu.be/pkokI8d-cBk}).}

\end{figure}

\hypertarget{sec-simpleex}{%
\subsection{Simulated data example}\label{sec-simpleex}}

In this section, we showcase the effectiveness of our methodology using
simulated data. The dataset comprises five spherical Gaussian clusters
in 4-\(d\), with each cluster containing an equal number of points and
consistent within variation.

We \emph{strongly} recommend viewing the linked videos for each study
while reading. Links to the videos are available in the figures for each
example. The videos show the visual appearance of the
\textbf{langevitour} interface with low-dimensional view and how we can
interact with the tour via the controls.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-nldervis5Gau-1.pdf}

}

\caption{\label{fig-nldervis5Gau}2D layouts from different NLDR
techniques applied the same data: (a) tSNE (perplexity = 61), (b) UMAP
(n\_neighbors = 15), (c) PHATE (knn = 5), (d) TriMAP (n\_inliers = 5,
n\_outliers = 4, n\_random = 3), and (e) PaCMAP (n\_neighbors = 10, init
= random, MN\_ratio = 0.9, FP\_ratio = 2). Is there a best
representation of the original data or are they all providing equivalent
information?}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-modelfiveGau-1.pdf}

}

\caption{\label{fig-modelfiveGau}Is there a best model to represent the
original data in 2D space or are they all providing equivalent
information?, (a) Model in the 2D space with tSNE
(\url{https://youtu.be/x5VPB-wOLm4}), (b) Model in the 2D space with
UMAP (\url{https://youtu.be/Cm1dW6iLyHQ}), (c) Model in the 2D space
with PHATE (\url{https://youtu.be/HB0Y3ilz6sI}), (d) Model in the 2D
space with TriMAP (\url{https://youtu.be/iSVkXIrOrSA}), and (e) Model in
the 2D space with PaCMAP (\url{https://youtu.be/O2QBctsc4uM}).}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltGau-1.pdf}

}

\caption{\label{fig-diagnosticpltGau}Goodness of fit statistics from
different NLDR techniques applied to training five spherical Gaussian
cluster dataset. What is the best NLDR technique to represent the
original data in 2D?}

\end{figure}

\hypertarget{sec-applications}{%
\section{Applications}\label{sec-applications}}

\hypertarget{human-peripheral-blood-mononclear-cells-pbmcs-data}{%
\subsection{Human peripheral blood mononclear cells (PBMCs)
data}\label{human-peripheral-blood-mononclear-cells-pbmcs-data}}

\begin{itemize}
\item
  Intro to the data set (This data set contains 13,714 features across
  2,700 samples within a single assay. The active assay is RNA, with
  13,714 features representing different gene expressions.)
\item
  2622 cells
\item
  Evaluate Festem on DEG detection (Festem enabled identification of
  often-missed fine cell types)
\item
  Using the Festem-selected genes, identified 10 clusters in the PBMCsk
  data and annotated them based on the expression of canonical markers
\item
  The 10 clusters included immune cells such as naive CD4 T cells,
  memory CD4 T cells, CD8 T cells, CD14 monocytes, FCGR3A monocytes,
  Natural Killer (NK) cells, B cells and Dendritic Cells (DC).
\item
  In addition to these common cell types, Festem identified a fine cell
  type, CD27− CD4+ memory T cells, which were often missed by other
  methods.
\item
  The CD27. CD4+ memory T cells identified by Festem expressed common
  marker genes (IL7R and S100A4) of memory T cells, but did not express
  CD27. These cells also had downregulated expression of SELL, CCR7, MAL
  and LEF1, and upregulated expression of CCL5 (Fig. 4B) and thus were
  the CD27- CD4+ memory T cells in the literature (36). The CD27- CD4+
  memory T cells were known to be at a more differentiated state and
  have stronger antigen-recall responses than their CD27+ counterparts.
\end{itemize}

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-nldervisPBMCUMAP-1.pdf}

}

\caption{\label{fig-nldervisPBMCUMAP}(a) 2D layout from UMAP
(n\_neighbors = 30, min\_dist = 0.3) applied for the PBMC3k dataset. Is
this a best representation of the original data?, (b) Model in the 2D
space with UMAP (\url{https://youtu.be/Ia3aL3L-fCk})}

\end{figure}

\hypertarget{single-cell-tagged-reverse-transcription-sequencing-data-of-mouse}{%
\subsection{Single-Cell Tagged Reverse Transcription sequencing data of
mouse}\label{single-cell-tagged-reverse-transcription-sequencing-data-of-mouse}}

The Zeisel mouse brain dataset, obtained through Spatial Transcriptomics
(STRT-Seq). Within this dataset, information is collected from a
substantial 2,816 individual mouse brain cells. Each of these cells acts
as a molecular snapshot, capturing the distinctive genetic activity
within various cell types. This diversity spans neurons, glial cells,
and other essential components of the brain, offering a comprehensive
view of the cellular tapestry.

What makes this dataset particularly valuable is its ability to shed
light on the spatial distribution of cells. Researchers can explore how
gene expression patterns vary across different regions of the mouse
brain, unlocking insights into the functional specialization of these
regions and the intricate networks that underpin neural processes.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-nldervis5Mouse-1.pdf}

}

\caption{\label{fig-nldervis5Mouse}2D layouts from different NLDR
techniques applied for the training Zeisel mouse brain dataset: (a) tSNE
(perplexity = 30), (b) UMAP (n\_neighbors = 15), (c) PHATE (knn = 5),
(d) TriMAP (n\_inliers = 5, n\_outliers = 4, n\_random = 3), and (e)
PaCMAP (n\_neighbors = 10, init = random, MN\_ratio = 0.9, FP\_ratio =
2). Is there a best representation of the original data or are they all
providing equivalent information?}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticpltZEI-1.pdf}

}

\caption{\label{fig-diagnosticpltZEI}Goodness of fit statistics from
different NLDR techniques applied to training Zeisel mouse brain
dataset. What is the best NLDR technique to represent the original data
in 2D?}

\end{figure}

\hypertarget{retina-dataset}{%
\subsection{Retina dataset}\label{retina-dataset}}

Human retina:
https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-7316

https://www.cell.com/cms/10.1016/j.cell.2015.05.002/attachment/fd52950c-fd35-401a-9e60-ca7588ef9001/mmc1.pdf

https://www.cell.com/fulltext/S0092-8674(15)00549-8\#app3

32 PCs tSNE (perplexity 30)

\hypertarget{kang-dataset}{%
\subsection{Kang dataset}\label{kang-dataset}}

For the Kang lupus dataset (29), we obtained 16 cell types based on
clustering of the Festem- selected genes (Fig. 4E and Fig. S10, S11),
including 2 fine cell types that were often missed by other methods. The
2 cell types were IFNhi CD14+ monocytes with high expression of type I
interferon-related (IFN) genes, and HSP+ CD4 T cells with up-regulated
heat- shock-protein-related (HSP) genes. The HSP+ CD4 T cells might
result from cryopreservation and thawing of T cells (37). Other than
devianceFS, other available methods could not identify these HSP+ CD4 T
cells (Fig. S11).

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-nlderviskangtSNE-1.pdf}

}

\caption{\label{fig-nlderviskangtSNE}2D layout from tSNE (perplexity =
30) applied for the Kang dataset. Is this a best representation of the
original data?}

\end{figure}

\hypertarget{sec-conclusion}{%
\section{Conclusion}\label{sec-conclusion}}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{bibliography.bib}

\newpage{}




\end{document}
