% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\def\tightlist{}
\usepackage{setspace}
\newcommand\pD{$p\text{-}D$}
\newcommand\kD{$k\text{-}D$}
\newcommand\dD{$d\text{-}D$}
\newcommand\gD{$2\text{-}D$}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Looking at Non-Linear Dimension Reductions as Models in the Data Space},
  pdfauthor={Jayani P.G. Lakshika; Dianne Cook; Paul Harrison; Michael Lydeamore; Thiyanga S. Talagala},
  pdfkeywords={high-dimensional data, dimension reduction, hexagon
binning, low-dimensional representation, tour, data
vizualization, model-in-the-data-space},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}



\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Looking at Non-Linear Dimension Reductions as Models in the
Data Space}
\author{
Jayani P.G. Lakshika\\
Econometrics \& Business Statistics, Monash University\\
and\\Dianne Cook\\
Econometrics \& Business Statistics, Monash University\\
and\\Paul Harrison\\
MGBP, BDInstitute, Monash University\\
and\\Michael Lydeamore\\
Econometrics \& Business Statistics, Monash University\\
and\\Thiyanga S. Talagala\\
Statistics, University of Sri Jayewardenepura\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
Non-linear dimension reduction (NLDR) techniques such as tSNE, and UMAP
provide a low-dimensional representation of high-dimensional
(\(p\text{-}D\)) data using non-linear transformation. The methods and
parameter choices can create wildly different representations, making it
difficult to decide which is best, or whether any or all are accurate or
misleading. NLDR often exaggerates random patterns, sometimes due to the
samples observed. But NLDR views have an important role in data analysis
because, if done well, they provide a concise visual (and conceptual)
summary of \(p\text{-}D\) distributions. To help evaluate the NLDR we
have developed an algorithm to show the \(2\text{-}D\) NLDR model in the
\(p\text{-}D\) space, viewed with a tour. One can see if the model fits
everywhere or better in some subspaces, or completely mismatches the
data. It is used to evaluate which \(2\text{-}D\) layout is the best
representation of the \(p\text{-}D\) distribution and see how different
methods may have similar summaries or quirks.
\end{abstract}

\noindent%
{\it Keywords:} high-dimensional data, dimension reduction, hexagon
binning, low-dimensional representation, tour, data
vizualization, model-in-the-data-space
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!


\spacingset{1.0}

\section{Introduction}\label{introduction}

Non-linear dimension reduction (NLDR) is popular for making a convenient
low-dimensional (\kD{}) representation of high-dimensional (\pD{}) data.
Recently developed methods include t-distributed stochastic neighbor
embedding (tSNE) \citep{laurens2008}, uniform manifold approximation and
projection (UMAP) \citep{leland2018}, potential of heat-diffusion for
affinity-based trajectory embedding (PHATE) algorithm \citep{moon2019},
large-scale dimensionality reduction Using triplets (TriMAP)
\citep{amid2022}, and pairwise controlled manifold approximation
(PaCMAP) \citep{yingfan2021}. However, the representation generated can
vary dramatically from method to method, and with different choices of
parameters or random seeds made using the same method
(Figure~\ref{fig-NLDR-variety}). The dilemma for the analyst is then,
\textbf{which representation to use}. The choice might result in
different procedures used in the downstream analysis, or different
inferential conclusions. The research described here provides new visual
tools to aid with this decision.

\begin{figure}

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-NLDR-variety-1.pdf}

}

\caption{\label{fig-NLDR-variety}Eight different NLDR representations of
the same data. Different techniques and different parameter choices are
used. Researchers may have seen any of these in their analysis of this
data, depending on their choice of method, or typical parameter choice.
Would they make different decisions downstream in the analysis depending
on which version seen? Which is the most accurate representation of the
structure in high dimensions?}

\end{figure}%

The paper is organised as follows. Section~\ref{sec-background} provides
a summary of the literature on NLDR, and high-dimensional data
visualization methods. Section~\ref{sec-method} contains the details of
the new methodology, including simulated data examples. Two applications
illustrating the use of the new methodology for bioinformatics and image
classification are in Section~\ref{sec-applications}. Limitations and
future directions are provided in Section~\ref{sec-discussion}.

\section{Background}\label{sec-background}

Historically, \kD{} representations of \pD{} data have been computed
using multidimensional scaling (MDS) \citep{borg2005}, which includes
principal components analysis (PCA) \citep{jolliffe2011} as a special
case. The \kD{} representation can be considered to be a layout of
points in \kD{} produced by an embedding procedure that maps the data
from \pD{}. In MDS, the \kD{} layout is constructed by minimizing a
stress function that differences distances between points in \pD{} with
potential distances between points in \kD{}. Various formulations of the
stress function result in non-metric scaling \citep{saeed2018} and
isomap \citep{silva2002}. Challenges in working with high-dimensional
data, including visualization, are outlined in \citet{johnstone2009}.

Many new methods for NLDR have emerged in recent years, all designed to
better capture specific structures potentially existing in \pD{}. Here
we focus on five currently popular techniques, tSNE, UMAP, PHATE, TriMAP
and PaCMAP. tNSE and UMAP can be considered to produce the \kD{}
minimizing the divergence between two distributions, where the
distributions are modeling the inter-point distances. PHATE, TriMAP and
PaCMAP are examples of diffusion processes \citep{coifman2005} spreading
to capture geometric shapes, that include both global and local
structure.

The array of layouts in Figure~\ref{fig-NLDR-variety} illustrate what
can emerge from the choices of method and parameters, and the random
seed that initiates the computation. Key structures interpreted from
these views suggest: (1) highly \textbf{separated clusters} (a, b, e, g,
h) with the number ranging from 3-6; (2) \textbf{stringy branches} (f),
and (3) \textbf{barely separated clusters} (c, d) which would
\textbf{contradict} the other representations.

It happens because these methods and parameter choices provide different
lenses on the interpoint distances in the data.

The alternative approach to visualizing the high-dimensional data is to
use linear projections. PCA is the classical approach, resulting in a
set of new variables which are linear combinations of the original
variables. Tours, defined by \citet{lee2021}, broaden the scope by
providing movies of linear projections, that provide views the data from
all directions. \citet{lee2021} provides an review of the main
developments in tours. There are many tour algorithms implemented, with
many available in the R package \texttt{tourr} \citep{wickham2011}, and
versions enabling better interactivity in \texttt{langevitour}
\citep{harisson2024} and \texttt{detourr} \citep{hart2022}. Linear
projections are a safe way to view high-dimensional data, because they
do not warp the space, so they are more faithful representations of the
structure. However, linear projections can be cluttered, and global
patterns can obscure local structure. The simple activity of projecting
data from \pD{} suffers from piling \citep{laa2022}, where data
concentrates in the center of projections. NLDR is designed to escape
these issues, to exaggerate structure so that it can be observed. But as
a result NLDR can hallucinate wildly, to suggest patterns that are not
actually present in the data.

The solution is to use the tour to examine how the NLDR is warping the
space. This approach follows what \citet{wickham2015} describes as
\emph{model-in-the-data-space}. The fitted model should be overlaid on
the data, to examine the fit relative the spread of the observations.
While this is straightforward, and commonly done when data is \gD{}, it
is also possible in \pD{}, for many models, when a tour is used.

\citet{wickham2015} provides several examples of models overlaid on the
data in \pD{}. In hierarchical clustering, a representation of the
dendrogrom using points and lines can be constructed by augmenting the
data with points marking merging of clusters. Showing the movie of
linear projections reveals shows how the algorithm sequentially fitted
the cluster model to the data. For linear discriminant analysis or
model-based clustering the model can be indicated by \((p-1)\text{-}D\)
ellipses. It is possible to see whether the elliptical shapes
appropriately matches the variance of the relevant clusters, and to
compare and contrast different fits. For PCA, one can display the \kD{}
plane of the reduced dimension using wireframes of transformed cubes.
Using a wireframe is the approach we take here, to represent the NLDR
model in \pD{}.

\section{Method}\label{sec-method}

\subsection{What is the NLDR model?}\label{what-is-the-nldr-model}

At first glance, thinking of NLDR as a modeling technique might seem
strange. It is a simplified representation or abstraction of a system,
process, or phenomenon in the real world. The \pD{} observations are the
realization of the phenomenon, and the \kD{} NLDR layout is the
simplified representation. From a statistical perspective we can
consider the distances between points in the \kD{} layout to be variance
that the model explains, and the (relative) difference with their
distances in \pD{} is the error, or unexplained variance. We can also
imagine that the positioning of points in \gD{} represent the fitted
values, that will have some prescribed position in \pD{} that can be
compared with their observed values. This is the conceptual framework
underlying the more formal versions of factor analysis \citep{cfa69} and
multidimensional scaling (MDS) \citep{borg2005}. (Note that, for this
thinking the full \pD{} data needs to be available, not just the
interpoint distances.)

\begin{table}

\centering{

\centering\begingroup\fontsize{12}{14}\selectfont

\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}>{\raggedright\arraybackslash}p{12cm}}
\toprule
\textbf{Notation} & \textbf{Description}\\
\midrule
$n, p, k$ & number of observations, variables, embedding dimension, respectively\\
$\mathbfit{X}, \mathbfit{x}$ & $p$-dimensional data (population, sample)\\
$\mathbfit{y}$ & $k$-dimensional layout\\
$P$ & orthonormal basis, generating a $d\text{-}dimensional$ linear projection of $p$-dimensional data\\
$T$ & true  model\\
\addlinespace
$g$ & functional mapping from \pD{} to \kD{}, especially as prescribed by NLDR\\
$\mathbfit{\theta}$ & (Hyper-) parameters for NLDR method\\
$r$ & ranges of the embedding components\\
$C^{(j)}$ & $j$-dimensional bin centers\\
$(b_1, b_2)$ & number of bins in each direction\\
\addlinespace
$(a_1, a_2)$ & binwidths, distance between centroids in each direction\\
$(s_1, \ s_2)$ & starting coordinates of the hexagonal grid\\
$q$ & buffer to ensure hexgrid covers data, proportion of data range, 0-1\\
$m$ & number of non-empty bins\\
$b$ & number of  hexagons in the grid\\
\addlinespace
$h$ & hexagonal id\\
\bottomrule
\end{tabular}
\endgroup{}

}

\caption{\label{tbl-notation}Summary of notation for describing new
methodology.}

\end{table}%

We define the NLDR as a function
\(g\text{:}~ \mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{n\times k}\),
with (hyper-)parameters \(\mathbfit{\theta}\). The parameters,
\(\mathbfit{\theta}\), depend on the choice of \(g\), and can be
considered part of model fitting in the traditional sense. Common
choices for \(g\) include functions used in tSNE, UMAP, PHATE, TriMAP,
PaCMAP, or MDS, although in theory any function that does this mapping
is suitable.

With our goal being to make a representation of this \gD{} layout that
can be lifted into high-dimensional space, the layout needs to be
augmented to include neighbour information. A simple approach would be
to triangulate the points and add edges. A more stable approach is to
first bin the data, reducing it from \(n\) to \(m\leq n\) observations,
and connect the bin centroids. We recommend using a hexagon grid because
it better reflects the data distribution and has less artifacts than a
rectangular grid. This process serves to reduce some noisiness in the
resulting surface shown in \pD{}. The steps in this process are shown in
Figure~\ref{fig-NLDR-scurve}, and documented below.

To illustrate the method, we use \(7\text{-}D\) simulated data, which we
call the ``S-curve''. It is constructed by simulating \(n=750\)
observations from \(\theta \sim U(-3\pi/2, 3\pi/2)\),
\(X_1 = \sin(\theta)\), \(X_2 \sim U(0, 2)\) (adding thickness to the
S), \(X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)\). The
remaining variables \(X_4, X_5, X_6, X_7\) are all uniform error, with
small variance. We would consider \(T=(X_1, X_2, X_3)\) to be the
geometric structure (true model) that we hope to capture.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-scurve-true-sc-1.pdf}

}

\caption{\label{fig-scurve-true-sc}Two views of the true model (blue
lines) in \(2\text{-}D\) projections from \(7\text{-}D\), for the
S-curve data (black points). The data is spread along the S shape, and
does not vary much from this curve. (The \textbf{langevitour} software
is used to view the data with a tour, and the full video is available at
\url{https://youtu.be/I5GL23vLiw0}).}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-NLDR-scurve-1.pdf}

}

\caption{\label{fig-NLDR-scurve}Key steps for constructing the model on
the UMAP layout (\(k=2\)): (a) data, (b) hexagon bins, (c) bin
centroids, and (d) triangulated centroids. The S-curve data is shown.}

\end{figure}%

\subsection{\texorpdfstring{Algorithm to represent the model in
\gD{}}{Algorithm to represent the model in }}\label{algorithm-to-represent-the-model-in}

\subsubsection{Scale the data}\label{scale-the-data}

Because we are working with distances between points, starting with data
having a standard scale, e.g.~{[}0, 1{]}, is recommended. The default
should take the aspect ratio produced by the NLDR
\((r_1, r_2, ..., r_k)\) into account. When \(k=2\), as in hexagon
binning, the default range is \([0, y_{i,\text{max}}], i=1,2\), where
\(y_{1,\text{max}}=1\) and \(y_{2,\text{max}} = \frac{r_2}{r_1}\)
(Figure~\ref{fig-NLDR-scurve}). If the NLDR aspect ratio is ignored then
set \(y_ {2,\text{max}} = 1\).

\subsubsection{Computing hexagon grid
configuration}\label{computing-hexagon-grid-configuration}

Although there are several implementations of hexagon binning
\citep{carr1987}, and a published paper \citep{dan2023}, surprisingly,
none has sufficient detail or components that produce everything needed
for this project. So we described the process used here.
Figure~\ref{fig-hex-param} illustrates the notation used.

The \gD{} hexagon grid is defined by its bin centroids. Each hexagon,
\(H_h\) (\(h = 1, \dots, b\)) is uniquely described by centroid,
\(C_{h}^{(2)} = (c_{h1}, c_{h2})\). The number of bins in each direction
is denoted as \((b_1, b_2)\), with \(b = b_1 \times b_2\) being the
total number of bins. We expect the user to provide just \(b_1\) and we
calculate \(b_2\) using the NLDR ratio, to compute the grid.

To ensure that the grid covers the range of data values a buffer
parameter (\(q\)) is set as a proportion of the range. By default,
\(q=0.1\). The buffer should be extending a full hexagon width (\(a_1\))
and height (\(a_2\)) beyond the data, in all directions. The lower left
position where the grid starts is defined as \((s_1, s_2)\), and
corresponds to the centroid of the lowest left hexagon,
\(C_{1}^{(2)} = (c_{11}, c_{12})\). This must be smaller than the
minimum data value. Because it is one buffer unit, \(q\) below the
minimum data values, \(s_1 = -q\) and \(s_2 = -qr_2\).

The value for \(b_2\) is computed by fixing \(b_1\). Considering the
upper bound of the first NLDR component, \(a_1 > \frac{1+2q}{b_1 -1}\).
Similarly, for the second NLDR component,
\(a_2 > \frac{r_2 + q(1 + r_2)}{(b_2 - 1)}\). Since
\(a_2 = \frac{\sqrt(3)}{2}a_1\) for regular hexagons,
\(a_1 > \frac{2[r_2 + q(1 + r_2)]}{\sqrt{3}(b_2 - 1)}\). This is a
linear optimization problem. Therefore, the optimal solution must occur
on a vertex. Therefore,
\(b_2 = \Big\lceil1 +\frac{2[r_2 + q(1 + r_2)](b_1 - 1)}{\sqrt{3}(1 + 2q)}\Big\rceil\).

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=0.3\textheight]{paper_files/figure-pdf/fig-hex-param-1.pdf}

}

\caption{\label{fig-hex-param}The components of the hexagon grid
illustrating notation.}

\end{figure}%

\subsubsection{Binning the data}\label{binning-the-data}

Observations are grouped into bins based on their nearest centroid. This
produces a reduction in size of the data from \(n\) to \(m\), where
\(m\leq b\) (total number of bins). This can be defined using the
function
\(u: \mathbb{R}^{n\times 2} \rightarrow \mathbb{R}^{m\times 2}\), where
\(u(i) = \arg\min_{j = 1, \dots, b} \sqrt{(y_{i1} - C^{(2)}_{j1})^2 + (y_{i2} - C^{(2)}_{j2})^2}\),
mapping observation \(i\) into \(H_h = \{i| u(i) = h\}\).

By default, the bin centroid is used for describing a hexagon (as done
in Figure~\ref{fig-NLDR-scurve} (c)), but any measure of center, such as
a mean or weighted mean of the points within each hexagon, could be
used. The bin centers, and the binned data, are the two important
components needed to render the model representation in high dimensions.

\subsubsection{Indicating neighborhood}\label{indicating-neighborhood}

Delaunay triangulation \citep{lee1980, alb2024} is used to connect
points so that edges indicate neighbouring observations, in both the
NLDR layout (Figure~\ref{fig-NLDR-scurve} (d)) and the \pD{} model
representation. When the data has been binned the triangulation connectd
centroids. The edges preserve the neighborhood information when the
model is lifted into \pD{}.

When shapes are non-linear in the NLDR layout, some edges could be long.
It can also happen that distant centroids can be connected, particularly
if clustering is present, which can result in long line segments. In
order to generate a smooth surface in \gD{}, these long line segments
should be removed when tuning the model fit.

\subsection{\texorpdfstring{Rendering the model in
\pD{}}{Rendering the model in }}\label{rendering-the-model-in}

The last step is to lift the \kD{} model into \pD{} by computing \pD{}
vectors that represent bin centroids. We use the \pD{} mean of the
points in \(H_h\) to map the centroid \(C_{h}^{(2)} = (c_{h1}, c_{h2})\)
to a point in \pD{}. Let the \pD{} mean be

\[C_{h}^{(p)} = \frac{1}{n_h}\sum_{i =1}^{n_h} x_i, h = {1, \dots, b; n_h > 0}.\]
Furthermore, line segments that exist in the \kD{} model generate line
segments in \pD{} by connecting the \pD{} means of the corresponding
\kD{} bin centroids. If additional long edges need to be removed,
compute the edges in \pD{} and pruned any detected long edges to improve
the accuracy. Once pruned, re-plot the \gD{} view to ensure it
accurately captures the data.

\begin{figure}[H]

\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/umap_trimesh_layout.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_umap_best_1.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_umap_best_2.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_umap_best_3.png}\end{minipage}%

\caption{\label{fig-scurve-sc}Model in \gD{}, on the UMAP layout, and
three views of the fit in projections from \(7\text{-}D\), for the
S-curve data (\((s_1, \ s_2) = (-0.160, \ -0.263)\),
\(b = 405 \  (15, \ 27)\), \(m = 70\), benchmark value to remove low
density hexagons is \(0.250\), and benchmark value to remove large edges
is \(0.159\)). MSE is \(0.0462\). The model closely fits the shape, but
it doesn't fully fill out the width of the S, which means that it does
not adequately capture the surface (The \textbf{langevitour} software is
used to view the data with a tour, and the full video is available at
\url{https://youtu.be/i7F7xpN1Hz8}).}

\end{figure}%

\subsection{Measuring the fit}\label{sec-summary}

The model here is similar to a confirmatory factor analysis model
\citep{brown2015}, \(\widehat{T}(X_1, X_2, X_3) + \Epsilon\). The
difference between the fitted model and observed values would be
considered to be residuals, and for this problem are \(7\text{-}D\).

Observations are associated with their bin center, \(C_{h}^{(p)}\),
which are also considered to be the \emph{fitted values}. These can also
be denoted as \(\widehat{X}\).

The error is computed by taking the squared \pD{} Euclidean distance,
corresponding to computing the mean squared error (MSE) as:

\begin{equation}\phantomsection\label{eq-equation1}{\frac{1}{n}\sum_{h = 1}^{b}\sum_{i = 1}^{n_h}\sum_{j = 1}^{p} (\mathbfit{x}_{hij} - C^{(p)}_{hj})^2}\end{equation}

where \(n\) is the number of observations, \(b\) is the number of bins,
\(n_h\) is the number of observations in \(h^{th}\) bin, \(p\) is the
number of variables, \(\mathbfit{x}_{hij}\) is the \(j^{th}\)
dimensional data of \(i^{th}\) observation in \(h^{th}\) hexagon.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=0.3\textheight]{paper_files/figure-pdf/fig-p-d-error-in-2d-scurve-1.pdf}

}

\caption{\label{fig-p-d-error-in-2d-scurve}error}

\end{figure}%

\subsection{\texorpdfstring{Prediction into
\gD{}}{Prediction into }}\label{prediction-into}

A new benefit of this fitted model is that it allows us to now predict a
new observation's value in the NLDR, for any method. The steps are to
determine the closest bin centroid in \pD{}, \(C^{(p)}_{h}\) and predict
it to be the centroid of this bin in \gD{}, \(C^{(2)}_{h}\). This can be
written as, let
\(z(i) = \arg\min_{j = 1, \dots, b} \sqrt{\sum_{v=1}^{p}(x_{iv} - C^{(p)}_{jv})^2}\),
then the new observation \(i\) falls in the hexagon,
\(H_h = \{i| z(i) = h\}\) and the corresponding \kD{} bin centroids,
\(C_{h}^{(2)} = (c_{h1}, c_{h2})\).

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-predict-scurve-1.pdf}

}

\caption{\label{fig-predict-scurve}Comparison of prediction generated
using the exiting \texttt{umap} prediction method and our method: (a) A
view of the true model in projections from \(7\text{-}D\), (b) predicted
data from the \texttt{umap} prediction method, (c) predicted data from
our method, and (d) comparison of predictions from both methods. In plot
(d), points representing the same data but predicted by different
methods are connected by lines. Need to add concluded sentence after
changing the data.}

\end{figure}%

\subsection{Tuning}\label{tuning}

The model fitting can be adjusted using these parameters:

\begin{itemize}
\tightlist
\item
  hexagon bin parameters

  \begin{itemize}
  \tightlist
  \item
    bottom left bin position \((s_1, \ s_2)\),
  \item
    the total number of bins (\(b\)),
  \end{itemize}
\item
  bin density cutoff, to remove low-density hexagons, and
\item
  edge length maximum, remove long edges from \gD{} representation.
\end{itemize}

Default values are provided for each of these, but it is expected that
the user will examine the MSE for a range of choices. Choosing these
parameters according to MSE can be automated but it is recommended that
the user examine the resulting model representation by overlaying it on
the data in \pD{}. The next few subsections describe the calculation of
default values, and the effect that different choices have on the model
fit.

\subsubsection{Hexagon bin parameters}\label{hexagon-bin-parameters}

The values \((s_1, \ s_2)\) define the position of the centroid of the
bottom left hexagon. By default, this is at \(s_1 = -q, s_2 = -qr_2\),
where \(q\) is the buffer sound the data. The choice of these values can
have some effect on the distribution of bin counts.
Figure~\ref{fig-param-scurve} (a) illustrates this. The distribution of
bin counts for \(s_1\) varying between \(-0.1-0.0\) is shown. Generally,
a more uniform distribution among these possibilities would indicate
that the bins are reliably capturing the underlying distribution of
observations.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-bins-scurve-1.pdf}

}

\caption{\label{fig-bins-scurve}Hexbin density plots of UMAP layout of
the S-curve data, using three different bin inputs: (a)
\(b = 91 \text{ } (7, \text{ }13)\), (b)
\(b = 220 \text{ } (11, \text{ }20)\), and (c)
\(b = 312 \text{ } (13, \text{ }24)\). Color indicates standardized
counts, dark indicating high count and light indicates low count. At the
smallest bin size the data segregates into two separate groups,
suggesting this is too many bins. Using the MSE of the model fit in
\(p\text{-}D\) helps decide on a useful choice of number of bins.}

\end{figure}%

The default number of bins \(b=b_1\times b_2\) is computed based on the
sample size, by setting \(b_1=n^{1/3}\), consistent with the
Diaconis-Freedman rule \citep{freedman1981}. The value of \(b_2\) is
determined analytically by \(b_1, q, r_2\). Values of \(b_1\) between
\(2\) and \(b_1 = \sqrt{\frac{n}{2}}\) are allowed.
Figure~\ref{fig-param-scurve} (b) shows the effect of different choices
of \(b_1\) on the MSE of the fitted model.

\subsubsection{Removal of low density
bins}\label{removal-of-low-density-bins}

By default, when assessing the choice of \(b_1\), the total number of
bins is measured by the number of \textbf{non-empty} bins. This more
accurately reflects the hexagon grid relative the MSE than the full
number of bins in the grid. It may also be beneficial to remove low
count bins also, in the situation where data is clustered or stringy,
where the observed data is sparse. In order to decide if this is
necessary, you would examine the distribution of bin counts, or the
density which puts the counts on a standard scale. If there is something
of a gap at low values, this would suggest a potential value to use as a
cutoff. Alternatively, one could choose to remove based on a percentile,
the bins with density in the lowest 5\% of all bins, for example.
Figure~\ref{fig-param-scurve} (c) illustrates the effect on the model
representation of removing bins below different percentages. Generally,
we would urge caution in removing low count bins.

The benchmark value for removing low-density hexagons ranges between
\(0\) and \(1\). When analyzing how these benchmark values influence
model performance, it's essential to observe the change in MSE as the
benchmark value increases (Figure~\ref{fig-param-scurve}). The MSE shows
a gradual decrease as the benchmark value goes from \(1\) to \(0\).
Evaluating this rate of increase is important. If the increment is not
considerable, the decision might lean towards retaining low-density
hexagons.

\subsubsection{Removing long edges}\label{removing-long-edges}

Edges define the neighbourhood structure, in order to provide a smooth
\gD{} representation of the fitted model.
Figure~\ref{fig-scurve-true-sc} shows a wire frame of the true model
that was used to generate the S-curve example data. The ideal is that
the representation of the fitted model, at least for this example where
we know the true model, should look similar to this.

The Delaunay triangulation will ensure that all centroids are connected
into a triangular mesh. For some structures, like clustered data, or
highly non-linear shapes, breaks in the mesh are meaningful. When
separated clusters are present the mesh should be broken across the
gaps. For non-linear structures like the S-curve, the mesh should run
unbroken along the S, but there should be no edges connecting the top of
the S directly to the bottom of the S. For these reasons it is necessary
to remove edges from the mesh in some applications.

The decision on edge length removal is made based on the distribution of
edge lengths. In particular, a gap between values, where there a
concentration of small values and then a few larger values, likely
suggests a cutoff for edge removal. Because the triangulation is
typically done on the hexagon centroids, there are particular discrete
edge lengths, based on bin widths. Figure~\ref{fig-NLDR-scurve} (d)
illustrates edge length distributions.

There is an additional step that is needed. When the model is lifted
into \pD{}, if the fit is good all the edges should be relatively small
in this space, too. If this is not the case, then there are several
possible actions: (1) re-do the NLDR to get a more representative
layout; (2) identify the edge and remove it from the model, in \gD{} and
\pD{}; (3) consider different values for the model fit, number of bins,
initial bin position or removing low density bins.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-param-scurve-1.pdf}

}

\caption{\label{fig-param-scurve}Various plots to help assess best
hexagon bin parameters, thresholds to remove low density bins and large
edges. Both (b) and (c) show MSE, against number of bins along the
x-axis and standardised count. A good benchmark value for these
parameters is when the MSE drops and then flattens out. Plot (a) shows
the distribution of stadardised counts of hexagons. Plot (d) shows the
distribution of \(2\text{-}D\) Euclidean distances between bin
centroids, with a good benchmark value for removing large edges would
being the distance that shows the first large decrease.}

\end{figure}%

\section{Best fit}\label{best-fit}

Deciding on the best fit relies on several elements:

\begin{itemize}
\tightlist
\item
  the choice of NLDR method, and the parameters used to create it, and
\item
  model fit parameters: bin size, low density bin removal, long edge
  removal.
\end{itemize}

Comparing the MSE to obtain the best fit is suitable if one starts from
the same NLDR representation. In theory, because the MSE is computed on
\pD{} measuring the fit between model and data it might still be useful
to compare different NLDR representations. A good NLDR representation
should produce a good fit, producing a low MSE if the model fits the
data well. However, it technically might be quite variable.

\begin{figure}[H]

\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/tsne_trimesh_layout.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_tsne_best_1.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_tsne_best_2.png}\end{minipage}%
%
\begin{minipage}{0.25\linewidth}
\includegraphics{figures/scurve/sc_tsne_best_3.png}\end{minipage}%

\caption{\label{fig-scurve-sc-best}Model in \gD{}, on the tSNE layout,
and three views of the fit in projections from \(7\text{-}D\), for the
S-curve data (\((s_1, \ s_2) = (-0.170, \ -0.137)\),
\(b = 256 \  (16, \ 16)\), \(m = 70\), benchmark value to remove low
density hexagons is \(0.250\), and benchmark value to remove large edges
is \(0.180\)). MSE is \(0.0432\). The model closely fits the shape, but
it has some twists (The \textbf{langevitour} software is used to view
the data with a tour, and the full video is available at
\url{https://youtu.be/ZuLdp89qJ6g}).}

\end{figure}%

\section{Linked plots}\label{linked-plots}

It's important to access the \gD{} layout and the generated model
overlaid on data in \pD{} together to understand whether it fits the
points everywhere, fits better in some places, or simply mismatches the
pattern. Interactivity also helps in understanding the quirks that occur
with different NLDR techniques (XXXXRefer the video after finalising the
S-curve).

\section{A curious difference between tSNE, UMAP and PaCMAP
revealer}\label{a-curious-difference-between-tsne-umap-and-pacmap-revealer}

In this section, we will assess how t-SNE, UMAP, and PaCMAP algorithms
preserve the clustering structure. We will use a simulated dataset
consisting of five spherical Gaussian clusters where each cluster
occupies a different corner of the \(4\text{-}D\) space, with each
cluster containing an equal number of points (\(1000\)) and the same
within-cluster variation. The data simulated from
\(\Sigma = \begin{pmatrix}
0.0025 & 0 & 0 & 0 \\
0 & 0.0025 & 0 & 0 \\
0 & 0 & 0.0025 & 0 \\
0 & 0 & 0 & 0.0025 \\
\end{pmatrix}\).

In the t-SNE 2D layout, the distances between clusters appear smaller,
leading to more tightly packed clusters, which is typical of t-SNE's
focus on preserving local structures. This often results in less
emphasis on the global distances between clusters. On the other hand,
UMAP tends to maintain larger separations between clusters, better
reflecting the global structure and preserving the relative distances
between clusters more effectively. PaCMAP, meanwhile, balances these
aspects by maintaining the distance between clusters and positioning
them correctly, effectively capturing both global and local structures.

The t-SNE, UMAP, and PaCMAP layouts each reveal different aspects of the
clustering structure within the dataset. When examining a specific
cluster, which appears more like a filled-out pancake in higher
dimensions, we observe that t-SNE, UMAP, and PaCMAP all effectively
capture global structures. However, there is evidence suggesting that
PaCMAP, with its default hyper-parameter settings, struggles to preserve
the local structure as effectively as t-SNE and UMAP.

The differences in performance can be attributed to several factors. One
key element is the choice of hyper-parameters, particularly the number
of neighbors considered during the dimensionality reduction process.
This parameter significantly impacts how well local structures are
maintained. Additionally, the intrinsic dimensionality of the data, the
balance between global and local structures, and issues related to
optimization and initialization all contribute to the observed outcomes.

It's important to note that these observations are made with specific
hyper-parameter settings. Adjusting these parameters could lead to
different results, potentially improving PaCMAP's ability to capture
local structures or altering how t-SNE and UMAP handle global and local
structures. Thus, while PaCMAP generally balances global and local
preservation, careful tuning of its parameters is crucial for achieving
the best results in any given dataset.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-five-gau-nldr-layouts-1.pdf}

}

\caption{\label{fig-five-gau-nldr-layouts}(a) tSNE, (b) UMAP, and (c)
PaCMAP}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-five-gau-projs-1.pdf}

}

\caption{\label{fig-five-gau-projs}(a) tSNE, (b) UMAP, and (c) PaCMAP}

\end{figure}%

\section{Applications}\label{sec-applications}

\subsection{Single-cell gene
expression}\label{single-cell-gene-expression}

In the field of single-cell studies, a common analytical task involves
clustering to identify groups of cells with similar expression profiles.
NLDR methods are commonly used to display clusters, and help to verify
the results. For example, \citet{chen2023} illustrates the use of UMAP
to identify clusters in Human Peripheral Blood Mononuclear Cells
(PBMC3k). There are \(2622\) single cells. First 9 principal components
are used to generate the UMAP. Figure~\ref{fig-NLDR-variety} (a) is the
reproduction of the published plot. The objective is to assess the
published layout, and if it does not accurately represent the three
clusters with small separations of the PBMC3k dataset
(Figure~\ref{fig-model-pbmc-author-proj} (a2)), then select a reasonable
\gD{} layout.

The Figure~\ref{fig-NLDR-variety} (a) shows three well-separated
clusters with big separations. However, as shown in
Figure~\ref{fig-model-pbmc-author-proj} (a2), there is no big separation
between three clusters in \(9\text{-}D\). Therefore, the suggested UMAP
representation (Figure~\ref{fig-NLDR-variety} (a)) does not accurately
represent the structure of PBMC3k dataset.

As a result, it is necessary to find an appropriate layout for the
dataset. MSE for different binwidths (\(a_1\)) using tSNE, UMAP, PHATE,
PaCMAP, and TriMAP with various (hyper-)parameter settings were computed
(Figure~\ref{fig-pbmc-mse}). Layouts c, d, and e, which show small
separations between clusters, are universally optimal. However, layout d
performs well with smaller binwidths and poorly with larger binwidths.
On the other hand, layout e performs well with larger binwidths. Layout
c was selected for further analysis due to its better (hyper-)parameter
selection for the same method of the published plot.

The visualization of the selected layout in the \(9\text{-}D\) shows
edges between the clusters (Figure~\ref{fig-mnist-tri-proj} (b2)). This
supports the presence of small separations between clusters.
Additionally, the data points are not uniformly distributed across the
clusters, resulting in dense areas (Figure~\ref{fig-mnist-tri-proj}
(b2)). Furthermore, the clusters shows non-linear shapes
(Figure~\ref{fig-mnist-tri-proj} (b3)).

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=1\textheight]{paper_files/figure-pdf/fig-pbmc-mse-1.pdf}

}

\caption{\label{fig-pbmc-mse}Assessing which of the 8 NLDR layouts on
the PBMC3k data (shown in Figure~\ref{fig-NLDR-variety}) is the better
representation using MSE for varying binwidth (\(a_1\)). Colour used for
the lines and points in the left plot and in the scatterplots represents
NLDR layout (a-h). Layout f is universally poor. Layouts a, b, g, h that
show large separations between clusters are universally suboptimal.
Layout d with little separation performs well at tiny binwidth (where
most points are in their own bin) and poorly as binwidth increases. The
choice of best is between layouts c and e, that have small separations
between oddly shaped clusters. Layout e is the best choice.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=0.8\textheight]{paper_files/figure-pdf/fig-model-pbmc-author-proj-1.pdf}

}

\caption{\label{fig-model-pbmc-author-proj}Model in \gD{}, on the
layouts, and two views of the fit in projections from \(9\text{-}D\),
for the PBMC3k data. The \gD{} model shows three well-separated clusters
with large gaps in a1 (\(a_1 = 0.05\), \(b = 676 \  (26,  26)\),
\(m = 92\), benchmark value to remove large edges is \(0.08\)), while a2
(\(a_1 = 0.10\), \(b = 195 \  (13,  15)\), \(m = 90\), benchmark value
to remove large edges is \(0.166\)) shows small separation between the
clusters. Because of the small separation, the three clusters are
connected in \gD{} and appear as long edges in \(9\text{-}D\). The full
video for the model for layout a is available at
\url{https://youtu.be/5Y1hE4i7N2k}. The full video for the model for
layout c is available at \url{https://youtu.be/5Y1hE4i7N2k}.}

\end{figure}%

\subsection{Hand-written digits}\label{hand-written-digits}

The digit 1 of the MNIST dataset consists of \(7877\) grayscale images
of handwritten digits \citep{lecun2010}. Before further analysis, PCA
was used to preprocess the data, where the first \(10\) principal
components, explaining 83\% of the total variation, were selected. The
objective is to select a reasonable \gD{} layout, representing the
non-linear structure of the digit 1 dataset in \(10\text{-}D\)
(Figure~\ref{fig-mnist-tri-proj} (a)).

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=1\textheight]{paper_files/figure-pdf/fig-mnist-mse-1.pdf}

}

\caption{\label{fig-mnist-mse}Assessing which of the 5 NLDR layouts on
the MNIST digit 1 data is the better representation using MSE for
varying binwidth (\(a_1\)). Colour used for the lines and points in the
left plot and in the scatterplots represents NLDR layout (a-e). Layout c
is universally poor. Layouts a that show two close clusters are
universally suboptimal. Layout a is the best choice.}

\end{figure}%

The MSE for different binwidths (\(a_1\)) using tSNE, UMAP, PHATE,
PaCMAP, and TriMAP with default (hyper-)parameter setting
(Figure~\ref{fig-mnist-mse}) were calculated. It is found that tSNE
(Figure~\ref{fig-mnist-mse} (a)) provide the most reasonable
representation for the digit 1 dataset, showing universally best.
However, \gD{} representation shows a big non-linear cluster and a small
cluster with a small gap (Figure~\ref{fig-tsne-best}).

In the case of digit 1 data, there should not be any clusters unless
anomalies exist, indicating different digit 1 patterns. The angle of the
digit 1 images varies along this non-linear clustering structure
(Figure~\ref{fig-tsne-best}), while the small cluster contains the digit
1 images with different patterns of the digit 1, unlike the usual
(Figure~\ref{fig-model-error-mnist} (c)). This provides the evidence for
two close clusters.

By visualizing the model generated for tSNE in \(10\text{-}D\) helps to
assess the \gD{} layout. As shown in Figure~\ref{fig-mnist-tri-proj}
(a), the model provides the evidence for the non-linear structure of the
digit 1 data in \(10\text{-}D\). The model shows some quirks. The
model's twisted pattern provides evidence for the \(10\text{-}D\) data
structure, which is not observed by \gD{} layout
(Figure~\ref{fig-mnist-tri-proj} (b)). Furthermore, the presence of long
edges indicates the existence of the small cluster located closely
together (Figure~\ref{fig-mnist-tri-proj} (c)).

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=0.22\textheight]{paper_files/figure-pdf/fig-tsne-best-1.pdf}

}

\caption{\label{fig-tsne-best}The \(\gD{}\) layout from tSNE applied for
the MNIST digit 1 dataset with defualt (hyper-)parameters. We use our
model in the \(10\text{-}D\) space to assess whether this is an accurate
representation of data structure present in \(10\text{-}D\), or if it is
misleading. The angle of the digit 1 images varies along this structure.
Images at the top-right of the \(\gD{}\) layout show the digit 1 angled
more to the right, while those at the bottom-right show the digit 1
angled more to the left.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-mnist-tri-proj-1.pdf}

}

\caption{\label{fig-mnist-tri-proj}Model in \gD{}, on the tSNE layout
(a), and three views (b, c, d) of the fit in projections from
\(10\text{-}D\), for the digit 1 of MNIST data (\(a_1 = 0.07\),
\(b = 456 \  (19,  24)\), \(m = 226\), and benchmark value to remove
large edges is \(0.114\)). (b) The non-linear structure observed in the
\gD{} layout of tSNE (Figure~\ref{fig-tsne-best}) is also visible when
visualizing the model overlaid on the data space. This indicates that
tSNE accurately captures the non-linear structure of the \(10\text{-}D\)
data. (c) the model shows a twisted pattern within the non-linear
structure in \(10\text{-}D\) space, which is an additional pattern not
visible in the \gD{} representation (Figure~\ref{fig-tsne-best}). (d)
some long edges exist in the \(10\text{-}D\) space that are not
recognized as long edges in the \gD{} representation. The full video is
available at \url{https://youtu.be/zcg_GXBmqjA}.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=0.8\textheight]{paper_files/figure-pdf/fig-model-error-mnist-1.pdf}

}

\caption{\label{fig-model-error-mnist}(a) The \(10\text{-}D\) model
error for tSNE layout in \(\gD{}\) of the MNIST digit 1 dataset. Most
low model errors are distributed along the big non-linear clutser, while
most large model errors are concentrated along the small cluster. (b)
There are certain images that exhibit large errors due to their
deviation from the non-linear cluster, which makes them anomalies. The
images associated with large model errors shows different patterns of
digit 1.}

\end{figure}%

\section{Discussion}\label{sec-discussion}

This study makes several important contributions to the field of NLDR.
We have developed an algorithm to evaluate the most useful NLDR method
and (hyper-)parameter choices for creating a reasonable \gD{} layout of
high-dimensional data. Our objective is to fit a model for the \gD{}
layout that preserves the relationships between neighboring points and
turns it into a high-dimensional wireframe, which can be overlaid on the
data and visualized using a tour. This approach is defined as
\emph{model-in-data-space}. Viewing a model in the data space is an
ideal way to examine the fit.

The effectiveness of this approach is illustrated through various
examples. For instance, the S-curve example demonstrates how the model
accurately fits the points, capturing both local and global structures
in high-dimensional space. Our simulation case study further, five
Gaussian cluster example shows that while all observed NLDR methods
preserve the global structure, only tSNE effectively maintains the local
structure, highlighting the specific strengths and quirks of different
methods.

Human behavior often shows a desire for more certainty and a tendency to
prefer well-separated views. This emphasizes the importance of clear and
distinct clusters. For example, in the UMAP layout of the \textbf{pbmc}
dataset suggested by \citet{chen2023}, three distant, well-separated
clusters are shown. However, our model reveals that these clusters are
actually close to each other in \pD{}. Additionally, the model discovers
non-uniform data distribution and non-linear structures within the
clusters that are not visible in the UMAP layout, demonstrating the
ability of our model in uncovering hidden data characteristics.

Evaluating the error or unexplained variance is important for assessing
how well the model fits the data. By examining the error for different
numbers of bins, we found that tSNE with a perplexity of \(30\) provides
a reasonable representation for the \textbf{pbmc} dataset. Connecting
the closest clusters with line segments in the fitted model further
supports the preservation of neighborhood relationships.

The \textbf{digit: 1} example further illustrates the model's ability to
accurately capture non-linear structures and provide additional
information. Key findings include a twisted pattern that compresses the
structure in some projections and long line segments that detect
anomalies.

Predicting new observations in \kD{} is particularly valuable due to the
limitations of some NLDR methods, like tSNE, which don't provide a
straightforward method for prediction. As a result, our approach offers
a solution that capable of generating predicted \kD{} embedding
regardless of the NLDR method employed, effectively addressing this
functional gap.

In conclusion, while our method effectively captures and represents
high-dimensional data structures, further enhancements could involve
introducing approaches to bind the data, indicate line segments beyond
\gD{}, and diagnose the fitted model. These improvements would help in
creating a more accurate representation of the data when \gD{} layout is
inadequate.

\section{Supplementary Materials}\label{supplementary-materials}

Code, and data for reproducing this paper are available at
\url{https://github.com/JayaniLakshika/paper-nldr-vis-algorithm}.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{bibliography.bib}

\newpage{}





\end{document}
