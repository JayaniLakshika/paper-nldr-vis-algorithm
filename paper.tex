% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{agsm}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Visualising How Non-linear Dimension Reduction Warps Your Data},
  pdfauthor={Jayani P.G. Lakshika; Dianne Cook; Paul Harrison; Michael Lydeamore; Thiyanga S. Talagala},
  pdfkeywords={high-dimensional data, dimension
reduction, triangulation, hexagonal binning, low-dimensional
manifold, manifold learning, tour, data vizualization},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Visualising How Non-linear Dimension Reduction Warps Your
Data}
\author{
Jayani P.G. Lakshika\\
Econometrics \& Business Statistics, Monash University\\
and\\Dianne Cook\\
Econometrics \& Business Statistics, Monash University\\
and\\Paul Harrison\\
MGBP, BDInstitute, Monash University\\
and\\Michael Lydeamore\\
Econometrics \& Business Statistics, Monash University\\
and\\Thiyanga S. Talagala\\
Statistics, University of Sri Jayewardenepura\\
}
\maketitle

\bigskip
\bigskip
\begin{abstract}
Non-Linear Dimension Reduction (NLDR) techniques have emerged as
powerful tools to visualize high-dimensional data. However, their
complexity and parameter choices may lead to distrustful or misleading
results. To address this challenge, we propose a novel approach that
combines the tour technique with a low-dimensional manifold generated
using NLDR techniques, hexagonal binning, and triangulation. This
integration enables a clear examination of the low-dimensional
representation in the original high-dimensional space. Our approach not
only preserves the advantages of both tours and NLDR but also offers a
more intuitive perception of complex data structures and facilitates
accurate data transformation assessments. The method and example data
sets are available in the \textbf{quollr} R package.
\end{abstract}

\noindent%
{\it Keywords:} high-dimensional data, dimension
reduction, triangulation, hexagonal binning, low-dimensional
manifold, manifold learning, tour, data vizualization
\vfill

\newpage
\spacingset{1} % DON'T change the spacing! (Default 1.9)

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, sharp corners, breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced]}{\end{tcolorbox}}\fi

\hypertarget{sec-intro}{%
\section{Introduction}\label{sec-intro}}

High-dimensional (high-D) data is widespread in many fields including
ecology and bioinformatics \citep{Guo2023}, in part because of new data
collection technologies \citep{Johnstone2009, ayesha2020overview}.
Working with high-dimensional data poses considerable challenges due to
the difficulty in visualizing beyond two dimensions \citep{Jia2022}.
High-dimensional data also presents difficulties for model fitting
\citep{Johnstone2009}, both computationally and interpretation, each of
which benefits from visualization.

To create visual representations of high-dimensional data, it is common
to apply dimension reduction techniques. Linear methods such as
principal component analysis (PCA) \citep{Karl1901} have been used for
many years, and remain popular. Non-linear methods such as
multi-dimensional scaling (MDS) \citep{Torgerson1967} have also been
routinely used. In the past decade, there has merged many new techniques
non-linear dimension reduction (NLDR), such as t-distributed stochastic
neighbor embedding (tSNE) \citep{Laurens2008}, uniform manifold
approximation and projection (UMAP) \citep{Leland2018}, designed to
capture the complex and non-linear relationships present within
high-dimensional data \citep{Johnstone2009}.

However, projecting high-dimensional data has limitations, such as
information loss and potential distortion of essential structures and
patterns \citep[\citet{article53}]{Jia2022}. The choice of technique and
parameters further impacts the accuracy of the visualization,
necessitating careful consideration for meaningful interpretation (see
Figure~\ref{fig-nldervis}).

Interactive and dynamic graphics systems have also been developed over
the years to enable visualizing high dimensions. One method, called a
tour \citep{Asimov1985}, shows a sequence of linear projections is shown
as a movie, allowing exploration without warping the space
\citep{lee2021review}. Interactive tools like \textbf{XGobi} and
\textbf{GGobi} have been successful in incorporating tours for exploring
high-dimensional data \citep{article60}. The R package \textbf{tourr}
\citep{article61} further enhances tour visualization within R, although
it may face limitations in frame rate and interactive features compared
to \textbf{GGobi}.

To overcome these limitations, the R package \textbf{detourr}
\citep{article22} has been developed, leveraging a Javascript widget via
htmlwidgets \citep{Ramnath2023} to achieve higher frame rates and
enhanced interactivity. Additionally, the R package \textbf{langevitour}
\citep{article09} utilizes Langevin Dynamics to generate a continuous
path of projections, eliminating the need for interpolation between
projections for animation. The tour technique has proven valuable in
exploring statistical model fits \citep{article58} and factorial
experimental designs \citep{article59}. Augmenting the results of
non-linear dimensional reduction methods with the tour, as demonstrated
in the \textbf{liminal} R package \citep{article21}, further enhances
data exploration.

While tours \citep{Asimov1985} preserve space without warping
\citep{lee2021review}, they require integrating multiple low-dimensional
views mentally to perceive high-dimensional structures. To address this
challenge, we propose a novel approach by combining the tour technique
with a low-dimensional manifold. This manifold is created through the
synergistic use of Non-Linear Dimension Reduction (NLDR) techniques,
hexagonal binning, and triangulation. By merging these techniques, our
approach offers a comprehensive and efficient means to visualize and
explore high-dimensional data while retaining the advantages of both
tours and NLDR. This integration facilitates a more intuitive perception
of complex data structures and empowers analysts with a robust tool for
assessing the accuracy of data transformations. The implementation of
our approach is available as an R package called \textbf{quollr}.

The outline of this paper is as follows. The
Section~\ref{sec-background} provides an detailed overview of dimension
reduction methods, triangulation, and tours. Building upon this
foundation, the Section~\ref{sec-methods} delves into the proposed
algorithm, \textbf{quollr}, and its implementation details. In
\textbf{?@sec-prediction}, discusses the effectiveness of the learned
low-dimensional manifold in accurately representing the complex
high-dimensional data. Following that, Section~\ref{sec-simpleex}
presents simple examples from simulations to illustrate the
functionality of the algorithm. Subsequently,
Section~\ref{sec-applications} showcases real-world applications of
\textbf{quollr} on different data sets, particularly in single-cell
RNA-seq data. These applications reveal insights into the performance
and trustworthiness of NLDR algorithms. We analyze the results to
identify situations where NLDR techniques may lead to misleading
interpretations. Finally, \textbf{?@sec-conclusions} concludes by
summarizing the findings and emphasizing the significance of the
proposed approach in tackling the challenges of high-dimensional data
visualization.

\hypertarget{sec-background}{%
\section{Background}\label{sec-background}}

\hypertarget{dimension-reduction}{%
\subsection{Dimension Reduction}\label{dimension-reduction}}

Consider the high-dimensional data a rectangular matrix \(X\), where
\(X = \begin{bmatrix} \textbf{x}_{1} & \textbf{x}_{2} & \cdots & \textbf{x}_{n}\\ \end{bmatrix}\),
with \(n\) observations in \(p\) dimensions. The objective is to
discover a low-dimensional projection
\(Y = \begin{bmatrix} \textbf{y}_{1} & \textbf{y}_{2} & \cdots & \textbf{y}_{n}\\ \end{bmatrix}\),
represented as an \(n\) × \(d\) matrix, where \(d \ll p\). The reduction
process seeks to remove noise from the original data set while retaining
essential information.

There are two main categories of dimension reduction techniques: linear
and non-linear methods. Linear techniques involve a linear
transformation of the data, with one popular example being PCA. PCA
performs an eigen-decomposition of the sample covariance matrix to
obtain orthogonal principal components that capture the variance of the
data \citep{Karl1901}. However, linear methods may not fully capture
complex non-linear relationships present in the data.

In contrast, NLDR techniques generate the low-dimensional representation
\(Y\) from the high-dimensional data \(X\), often using pre-processing
techniques like \(k\)-nearest neighbors graph or kernel transformations.
Multidimensional Scaling (MDS) is a class of NLDR methods that aims to
construct an embedding \(Y\) in a low-dimensional space, approximating
the pair-wise distances in \(X\) \citep{Torgerson1967}. Variants of MDS
include non-metric scaling \citep{article62} and Isomap, which estimate
geodesic distances to create the low-dimensional representation
\citep{article63}. Other approaches based on diffusion processes, like
diffusion maps \citep{article64} and the PHATE algorithm
\citep{article03}, also fall under NLDR methods.

A challenge with NLDR methods is selecting and tuning appropriate
parameters. One specific technique we focus on is Pairwise Controlled
Manifold Approximation (PaCMAP). Similar considerations apply to related
methods like tSNE \citep{Laurens2008}, UMAP \citep{Leland2018}, and
TrMAP \citep{article02}.

It is important to note that methods like PCA and auto-encoders
\citep{article65} provide a reverse mapping from the low-dimensional
space back to the high-dimensional space, enabling data reconstruction.
However, many non-linear methods, including tSNE, prioritize
visualization and exploration over reconstruction. Their focus is on
capturing complex structures that may not be easily represented in the
original space, making a straightforward reverse mapping challenging.

\hypertarget{introduction-to-umap}{%
\subsubsection{Introduction to UMAP}\label{introduction-to-umap}}

The UMAP algorithm constructs a fuzzy topological representation of the
high-dimensional data and then optimizes the low-dimensional embedding
to be as similar to this representation as possible \citep{Leland2018}.

The UMAP algorithm constructs a weighted k-nearest neighbors graph based
on the local similarity of data points in the original high-dimensional
space. The weighted graph is defined by a conditional probability
distribution \(P_{ij}\), which represents the probability of observing
data point \(x_i\) given data point \(x_j\) as a neighbor:

\[
P_{ij} = \frac{\exp(-\lVert x_i - x_j \rVert^2)}{\sum_{k \neq i} \exp(-\lVert x_i - x_k \rVert^2)}
\]

Similarly, a fuzzy topological representation of the data is constructed
in the low-dimensional space with conditional probability distribution
\(Q_{ij}\):

\[
Q_{ij} = \frac{\exp(-\lVert y_i - y_j \rVert^2)}{\sum_{k \neq i} \exp(-\lVert y_i - y_k \rVert^2)}
\]

where \(y_i\) and \(y_j\) are the corresponding low-dimensional
representations of data points \(x_i\) and \(x_j\).

UMAP minimizes the cross-entropy between the high-dimensional fuzzy
topological representation \(P_{ij}\) and the low-dimensional
representation \(Q_{ij}\) to find an optimal low-dimensional embedding.
The loss function for UMAP is defined as:

\[
L = \sum_{i} \sum_{j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
\]

The algorithm uses stochastic gradient descent to optimize the positions
of the data points in the low-dimensional space to minimize this loss
function.

In R, the \texttt{umap} package is often used to implement UMAP and
generate results.

The ``n\_neighbors'' parameter in UMAP determines the number of nearest
neighbors to consider for each data point during the construction of the
fuzzy topological representation. Higher values of ``n\_neighbors''
capture more global structure, while lower values emphasize local
structure. Choosing an appropriate value for ``n\_neighbors'' is
crucial, as it affects the balance between local and global structure in
the resulting low-dimensional representation.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-nldervis-1.pdf}

}

\caption{\label{fig-nldervis}2D layouts from different NLDR techniques
applied the same data: (a) tSNE (perplexity = 32), (b) UMAP
(n\_neighbors = 50), (c) PHATE (knn = 5), (d) TriMAP (n\_inliers = 5,
n\_outliers = 4, n\_random = 3), and (e) PaCMAP (n\_neighbors = 10, init
= random, MN\_ratio = 0.9, FP\_ratio = 2). Is there a best
representation of the original data or are they all providing equivalent
information?}

\end{figure}

\hypertarget{linear-overviews-using-tours}{%
\subsection{Linear overviews using
tours}\label{linear-overviews-using-tours}}

A tour is a powerful visualization technique used to explore
high-dimensional data by generating a sequence of projections, typically
into two dimensions. There are two main types of tours: the Grand Tour
and the Guided Tour. The Grand Tour explores the data's shape and global
structure by using random projections \citep{Asimov1985}. In contrast,
the Guided Tour focuses on specific patterns by moving towards
interesting projections defined by a predefined index function
\citep{article29}.

The process begins with a real data matrix \(X\) containing \(n\)
observations in \(p\) dimensions. It generates a sequence of \(p\) ×
\(d\) orthonormal projection matrices (bases), usually 1 or 2
dimensions. For each pair of orthonormal bases \(A_t\) and \(A_{t+1}\),
a geodesic path is interpolated to create smooth animation between
projections.

In the Grand Tour, new orthonormal bases are randomly chosen to explore
the \(d\)-dimensional subspace. The data is often sphered via principal
components to reduce dimensionality. The Guided Tour uses a predefined
index function to generate a sequence of `interesting' projections. The
resulting tour continuously visualizes the projected data \(Y_t\) =
\(XA_t\) as it interpolates between successive bases.

While both tours can be used to visualize data, examples often focus on
using the Grand Tour to observe global structures. However, software
like \textbf{langevitour} can visualize both types of tours, providing
flexibility for exploring high-dimensional data with various objectives.

\hypertarget{sec-methods}{%
\section{Methodology}\label{sec-methods}}

Our algorithm comprises two main phases: (1) generate the model in the
2D space, and (2) generate the model in the high-D space. These two
phases are described in details in this section.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=1\textheight]{figures/workflow.png}

}

\caption{\label{fig-meth}A flow diagram detailing the steps taken to
create the low-dimensional manifold in the high dimensional space. There
are two basic phases, one to generate the model in the 2D space, and
other to generate the model in the high-D space.}

\end{figure}

\hypertarget{preprocessing-steps}{%
\subsection{Preprocessing steps}\label{preprocessing-steps}}

To tackle the complexities and noise in high-dimensional data, we apply
PCA as a pre-processing step \citep[\citet{article68},
\citet{article69}]{article67}. This step helps in noise reduction by
identifying principal components that represent directions of maximum
variance, capturing essential patterns in the data.

\hypertarget{constructing-the-2d-model}{%
\subsection{Constructing the 2D model}\label{constructing-the-2d-model}}

\textbf{Step 1: Computing the hexagonal grid configuration}

Hexagonal binning is a data visualization technique that aggregates
high-dimensional data into a two-dimensional representation using
hexagonal regions called bins \citep{article66}. The hexagonal shape is
preferred due to its circular-like appearance, providing smoother
transitions between neighboring bins and efficient data aggregation.
This technique is especially useful for handling large data sets as it
reduces visual clutter while capturing the underlying data distribution
effectively. By considering the data distribution, important features
are captured, and gaps between bins are minimized, leading to a more
accurate representation of the data. In our algorithm, hexagonal binning
is utilized to create a low-dimensional manifold.

\textbf{(a) Determine the number of bins along the x-axis} (\(b_1\))

The first step involves to determine the optimal number of bins (\(b\))
for creating regular hexagons in the hexagonal grid. To achieve this, we
rely on the relationship between the diameter (\(h\)) and the area
(\(A\)) of regular hexagons, as given by Equation~\ref{eq-equation3}.

\begin{equation}\protect\hypertarget{eq-equation3}{}{
 \text{A} = \frac{\sqrt{3}}{2}h^2
}\label{eq-equation3}\end{equation}

To capture the data's structure effectively, we consider the optimal
\(A = 1\) (see Figure~\ref{fig-binsize}). Using
Equation~\ref{eq-equation4}, we then calculate \(h\) of a regular
hexagon.

\begin{equation}\protect\hypertarget{eq-equation4}{}{
 \text{h} = \sqrt{\frac{2}{\sqrt{3}}A}
}\label{eq-equation4}\end{equation}

Next, we refer the \texttt{hexbin} function in R, which provides a
relationship between the diameter of the hexagon (\(h\)) and the number
of bins along the x-axis (\(b_1\)) (Equation~\ref{eq-equation6}).

\begin{equation}\protect\hypertarget{eq-equation6}{}{
 h = \frac{r_1}{b_1}
}\label{eq-equation6}\end{equation}

By utilizing the calculated \(h\) and the range of the non-linear
projection component 1 (\(r_1\)), we determine the \(b_1\)
(Equation~\ref{eq-equation5}). The result is rounded up to the nearest
whole number to ensure an appropriate number of bins that capture the
variability of the data along the x-axis.

\begin{equation}\protect\hypertarget{eq-equation5}{}{
 b_1 = \frac{r_1}{h}
}\label{eq-equation5}\end{equation}

\textbf{(b) Determine the effective shape parameter} (\(s\))

In this step, we determine the optimal shape parameter (\(s\)) for the
hexagonal bins, which significantly influences their shape and
arrangement within the grid. The shape parameter (\(s\)) in the
hexagonal binning algorithm is defined as the ratio of the height
(\(y\)) to the width (\(x\)) of the plotting region, referring to the
\texttt{hexbin} function in R (see Equation~\ref{eq-equation1}). It
determines the shape of the plotting regions and plays a vital role in
generating an appropriate hexagonal grid for the data visualization.

\begin{equation}\protect\hypertarget{eq-equation1}{}{
  s = \frac{y}{x}
}\label{eq-equation1}\end{equation}

To calculate the effective shape parameter (\(s\)) for our algorithm, we
consider the ranges of the non-linear projection components. Denoting
the range of the non-linear projection component 1 as \(r_1\) and the
range of the non-linear projection component 2 as \(r_2\), we find the
shape parameter using Equation~\ref{eq-equation2}.

\begin{equation}\protect\hypertarget{eq-equation2}{}{
s = \frac{r_2}{r_1}
}\label{eq-equation2}\end{equation}

\begin{figure}[H]

{\centering \includegraphics{paper_files/figure-pdf/fig-binsize-1.pdf}

}

\caption{\label{fig-binsize}Hexbin plots from different number of bins
for the same \textbf{s\_curve\_noise\_umap} data: (a) b = (4, 8), s =
2.031, (b) b = (9, 20), s = 2.031, and (c) b = (16, 36), s = 2.031. The
hexbins are colored based on the density of points, with yellow
indicating higher point density and darker colors representing lower
point density within each bin. Does a value of number of bins exist to
effectively represent the low-dimensional data?}

\end{figure}

\textbf{Step 2: Obtain bin centroids}

After computing hexagonal grid configurations, the 2D emdeddings are
binned into hexagon cells. As a result of hexagonal binning , the bin
centroids (\(C_k^{(2)} \equiv (C_{kx}, C_{ky}\))) (see
Figure~\ref{fig-meth} (Step 2)) are obtained.

\textbf{Step 3: Triangulate bin centroids}

In this step, the algorithm proceeds to triangulate the centroids (see
Figure~\ref{fig-meth} (Step 3)) of the hex bins. Triangulation is a
fundamental process in computational geometry and computer graphics that
involves dividing a set of points in a given space into interconnected
triangles \citep{article30}. One common algorithm used for triangulation
is Delaunay triangulation \citep{article26}, where points are connected
in a way that maximizes the minimum angles of the resulting triangles,
leading to a more regular and well-conditioned triangulation. In our
algorithm, triangulation helps us identify geometric relationships and
patterns in the data.

Since we are working with the centroids of regular hexagonal bins, the
resulting mesh will predominantly comprise equal-sized regular
triangles. However, the triangulation also helps span any gaps that may
exist between clusters of points, allowing for a more complete and
interconnected representation of the data.

\hypertarget{lifting-the-model-into-high-dimensions}{%
\subsection{Lifting the model into high
dimensions}\label{lifting-the-model-into-high-dimensions}}

Creating the model in the high-D space involves a two-step process.
Initially, we identify observations within each hexagonal bin. After
this, we calculate the average of the high-dimensional data within each
hex bin. The resulting values represent the averaged high-dimensional
data points (\(C_k^{(p)} \equiv (C_{kx_1}, ..., C_{kx_p}\))).
Essentially, these averaged high-dimensional data points serve as the
representation of the hex bin centroids in the high-D space.

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-wkhighD-1.pdf}

}

\caption{\label{fig-wkhighD}Visualization.}

\end{figure}

\hypertarget{tunning-the-model}{%
\subsection{Tunning the model}\label{tunning-the-model}}

\hypertarget{sec-summary}{%
\subsection{Summaries of the model}\label{sec-summary}}

\hypertarget{predicted-values-and-residuals}{%
\subsubsection{Predicted values and
residuals}\label{predicted-values-and-residuals}}

In this context, the term ``prediction values'' refers to the 2D
coordinates predicted for the NLDR technique. The approach involves
employing the K-nearest neighbors (KNN) algorithm to identify the
nearest hexagonal bin centroid in the 2D space. Subsequently, the
coordinates of this centroid are assigned as the low-dimensional
predicted values for the test data in 2D space. It is noteworthy that
traditional NLDR methods, such as t-SNE, often lack a direct predict
function, making our approach valuable for generating predicted values
in the absence of such functionalities.

The concept of ``residuals'' is pivotal in evaluating the accuracy of
representing bin centroids in high dimensions. To quantify this
accuracy, we introduce an error metric, which measures the sum of
squared differences between the high-dimensional data ((x\_\{ij\})) and
the predicted bin centroid data in high-dimensional space
((C\_\{x\_ij\})) across all bins and dimensions. Mathematically, this
error is expressed as:

\begin{equation}\protect\hypertarget{eq-equation11}{}{
\text{Error} = \sum_{j = 1}^{n}\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2
}\label{eq-equation11}\end{equation}

Here, (n) represents the number of bins, (p) represents the dimensions,
(x\_\{ij\}) is the actual high-dimensional data, and (C\_\{x\_ij\}) is
the predicted bin centroid data in high dimensions.

The error metric outlined above provides valuable insights into the
overall accuracy of our predictive model. By quantifying the squared
deviations between the actual and predicted values across all bins and
dimensions, we gain a comprehensive understanding of how well our method
captures and represents the underlying structure of the data in the
reduced 2D space. This assessment is crucial for evaluating the efficacy
of our NLDR technique in preserving the essential information present in
the original high-dimensional data.

\hypertarget{goodness-of-fit-statistics}{%
\subsubsection{Goodness of fit
statistics}\label{goodness-of-fit-statistics}}

Moving on to the assessment of prediction accuracy, we calculate the
Mean Squared Error (MSE). The MSE helps measure the average squared
differences between the actual high-dimensional data (\(x_{ij}\)) and
the predicted bin centroid data in high-D (\(C_{x_ij}\)) values across
all bins. Mathematically, this is expressed as:

\begin{equation}\protect\hypertarget{eq-equation9}{}{
\text{MSE} = \sum_{j = 1}^{n} \frac{\sum_{i = 1}^{p} (x_{ij} - C_{x_ij})^2}{\text{total number of bins}}
}\label{eq-equation9}\end{equation}

Here, \(b\) signifies the total number of bins, \(p\) denotes the number
of dimensions in the high-dimensional data, and \(n\) represents the
number of observations.

Additionally, we gauge the model's performance using the Akaike
Information Criterion (AIC), calculated by the formula:

\begin{equation}\protect\hypertarget{eq-equation10}{}{
\text{AIC} = 2bp + np * log(\text{MSE})
}\label{eq-equation10}\end{equation}

These metrics, MSE and AIC, collectively offer valuable insights into
the model's predictive performance, considering both accuracy and
complexity in the predictions.

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-predict-1.pdf}

}

\caption{\label{fig-predict}The .}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-diagnosticplt-1.pdf}

}

\caption{\label{fig-diagnosticplt}The .}

\end{figure}

\hypertarget{sec-simpleex}{%
\subsection{Simulated data examples}\label{sec-simpleex}}

In this section, we demonstrate the effectiveness of our approach
through the use of simulation data. These simulations are designed to
have known cluster structures and underlying data geometries. We begin
with a straightforward example where we generate spherical clusters with
a doublet structure. This allows us to assess how well our algorithm can
identify and represent these known structures in the low-dimensional
space.

We then proceed to a more complex example where the data exhibits a more
intricate geometry. This simulation provides a challenging test for our
approach to capture and preserve the underlying data structure
accurately.

We \emph{strongly} recommend viewing the linked videos for each study
while reading. Links to the videos are available in the figures for each
example. The videos show the visual appearance of the
\textbf{langevitour} interface with low-dimensional view and how we can
interact with the tour via the controls.

\textbf{Example 1: Exploring spherical Gaussian clusters with doublet
structure}

The next data set consists of two distinct types of clusters embedded in
a 10-\(d\) space. The first set of clusters includes three Gaussian
clusters with equal variance. Each of these Gaussian clusters is
equidistant from the others, forming a well-separated and symmetrical
arrangement. The second set of clusters consists of three clusters with
a doublet structure. These clusters are also embedded in the 10-\(d\)
space, but each is positioned in the middle of its parent Gaussian
clusters. Moreover, each doublet cluster contains an average number of
points from its corresponding parent Gaussian clusters.

For analysis, we run tSNE on the training data with a perplexity setting
18. Figure~\ref{fig-example3} (a) displays the tSNE results, indicating
that the sub-clusters have been correctly identified. However, the
relative locations of certain clusters to each other appear distorted in
the low-dimensional view, such as the red and pink clusters being far
apart.

Nonetheless, we find no apparent impact on the perception of the data
structure when we examine the tour view and the model (see
Figure~\ref{fig-example3}) through the linked video (see
Figure~\ref{fig-exp1_sc}). The model demonstrates accurate
identification of clusters and captures the internal variations within
each cluster, reflecting the inherent variance present in the data. The
configurations used for the analysis are detailed in
\textbf{?@tbl-table02}. Additionally, the representation of the parent
clusters in the low-dimensional space appears stretched, aligning with
the variance of these clusters. On the other hand, the doublet clusters
are squeezed, reflecting their specific characteristics and relationship
to their parent Gaussian clusters.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-example3-1.pdf}

}

\caption{\label{fig-example3}Visualization and refinement of
low-dimensional representation for sub-clustered data\(\colon\) (a) 2D
layout of tSNE (perplexity = 18), (b) triangular mesh, and (c)
triangular mesh colored by edge type. Edges with lengths less than the
benchmark value of \(2\) are classifiedas small, while edges with
lengths greater than or equal to this value are labeled as long. The
combination of tSNE, hexagonal binning, and triangulation provides the
effectiveness of our approach in visualizing and refining the
low-dimensional representation of the sub-clustered data.}

\end{figure}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_1a.png}

}

}

\subcaption{\label{fig-exp1_sc1}}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_1b.png}

}

}

\subcaption{\label{fig-exp1_sc2}}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_1c.png}

}

}

\subcaption{\label{fig-exp1_sc3}}
\end{minipage}%

\caption{\label{fig-exp1_sc}Screen shots of the \textbf{langevitour}
with the low-dimensional view applied to sub-clustered data set, a video
of the tour animation is available at
(\url{https://drive.google.com/file/d/15nAbmc4UoVlUyFcGtnlnmyxrJEbK39nx/view}).
Combination of the \textbf{langevitour} and low-dimensional
representation of the sub-clustered data provides the effectiveness of
our approach.}

\end{figure}

\textbf{Example 2: Exploring data with piecewise linear structure}

In our next exploration, we delve into simulated noisy tree-structured
data designed to mimic branching trajectories of cell differentiation.
The simulation is crafted to resemble the hierarchical clustering
pattern observed in cell differentiation trajectories, where mature
cells are akin to the tips of branches.

To begin our analysis, we apply principal components to the data and
focus on the first ten principal components, collectively accounting for
approximately 68\% of the variance explained in the data set. Then, we
run TriMAP with the default parameters on the training data (n\_inliers:
5, n\_outliers: 4, and n\_random: 3). Through this process, we create a
low-dimensional manifold using the configurations detailed in
\textbf{?@tbl-table02}. Finally, we present the model along with the
data using the tour view (Figure Figure~\ref{fig-exp2_sc}), allowing us
to visualize and explore the intricate relationships and structures
present in the data's low-dimensional representation.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-example2-1.pdf}

}

\caption{\label{fig-example2}Visualization and refinement of
low-dimensional representation for tree structured data\(\colon\) (a) 2D
layout of TriMAP (n-inliers = 5, n-outliers = 4, n-random = 3), (b)
triangular mesh, and (c) triangular mesh colored by edge type. Edges
with lengths less than the benchmark value of \(2\) are classified as
small, while edges with lengths greater than or equal to this value are
labeled as long. The combination of TriMAP, hexagonal binning, and
triangulation provides the effectiveness of our approach in visualizing
and refining the low-dimensional representation of the tree structured
data.}

\end{figure}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_2a.png}

}

}

\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_2b.png}

}

}

\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/example_2c.png}

}

}

\end{minipage}%

\caption{\label{fig-exp2_sc}Screen shots of the \textbf{langevitour}
with the low-dimensional view applied to tree structured data set, a
video of the tour animation is available at
(\url{https://drive.google.com/file/d/1YQx4UGlzDify1mGqeDtlqTHncgqidDhq/view}).
Combination of the \textbf{langevitour} and low-dimensional
representation of the tree structured data provides the effectiveness of
our approach.}

\end{figure}

\hypertarget{sec-applications}{%
\section{Applications}\label{sec-applications}}

\hypertarget{single-cell-rna-seq-data-of-human}{%
\subsection{Single-cell RNA-seq data of
human}\label{single-cell-rna-seq-data-of-human}}

In the field of single-cell studies, a common analysis task involves
clustering to identify groups of cells with similar expression profiles.
Analysts often turn to NLDR techniques to verify and identify these
clusters and explore developmental trajectories (e.g., example 1). In
clustering workflows, the main objective is to verify the existence of
clusters and subsequently identify them as specific cell types by
examining the expression of ``known'' marker genes. In this context, a
``faithful'' embedding should ideally preserve the topology of the data,
ensuring that cells corresponding to the same cell type are situated
close to the high-dimensional space.

To begin our analysis, we installed the Peripheral Blood Mononuclear
Cells (pbmc) data set obtained from 10x Genomics using the
\texttt{SeuratData} R package \citep{Rahul2019}, which facilitates the
distribution of data sets in the form of Seurat objects
\citep{Yuhan2021}. This data set contains 13,714 features across 2,700
samples within a single assay. The active assay is RNA, with 13,714
features representing different gene expressions. After loading the data
set, we obtained the principal components (PCs) and assessed the
variance explained by each PC. Based on this evaluation, we selected
seven PCs, representing approximately 50\% of the variance in gene
expression, for further analysis.

Next, we employed the UMAP technique with default parameter settings. As
illustrated in Figure~\ref{fig-pbmc}, the cell types B and Platelet are
well-separated in the UMAP layout. Moreover, CD14+ Mono, FCGR3A+ Mono,
and DC form a distinct cluster, while Naive CD4 T, NK, Memory CD4 T, and
CD8 T are grouped together in another cluster. The values utilized to
construct the smooth low-dimensional manifold are presented in
\textbf{?@tbl-table02}. The linked video, demonstrating the tour with
the model, showcases the generation of a smooth surface for this
application, enabling a comprehensive exploration of the data's
structure and relationships (see Figure~\ref{fig-pbmc_sc}).

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{paper_files/figure-pdf/fig-pbmc-1.pdf}

}

\caption{\label{fig-pbmc}Visualization and refinement of low-dimensional
representation for pbmc data\(\colon\) (a) 2D layout of UMAP (n-neigbors
= 15), (b) triangular mesh, and (c) triangular mesh colored by edge
type. Edges with lengths less than the benchmark value of \(2\) are
classified as small, while edges with lengths greater than or equal to
this value are labeled as long. The combination of UMAP, hexagonal
binning, and triangulation provides the effectiveness of our approach in
visualizing and refining the low-dimensional representation of the pbmc
data.}

\end{figure}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/pbmc_a.png}

}

}

\subcaption{\label{fig-pbmc_sc1}}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/pbmc_b.png}

}

}

\subcaption{\label{fig-pbmc_sc2}}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{figures/pbmc_c.png}

}

}

\subcaption{\label{fig-pbmc_sc3}}
\end{minipage}%

\caption{\label{fig-pbmc_sc}Screen shots of the \textbf{langevitour}
with the low-dimensional view applied to pbmc data set, a video of the
tour animation is available at
(\url{https://drive.google.com/file/d/1F7Xb2EIpfUruCxYihPw2rKro4FB5Y2uL/view}).
Combination of the \textbf{langevitour} and low-dimensional
representation of the pbmc data provides the effectiveness of our
approach.}

\end{figure}

\hypertarget{zeisel-mouse-brain-strt-seq}{%
\subsection{Zeisel mouse brain
(STRT-Seq)}\label{zeisel-mouse-brain-strt-seq}}

The Zeisel mouse brain dataset, obtained through Spatial Transcriptomics
(STRT-Seq). Within this dataset, information is collected from a
substantial 2,816 individual mouse brain cells. Each of these cells acts
as a molecular snapshot, capturing the distinctive genetic activity
within various cell types. This diversity spans neurons, glial cells,
and other essential components of the brain, offering a comprehensive
view of the cellular tapestry.

What makes this dataset particularly valuable is its ability to shed
light on the spatial distribution of cells. Researchers can explore how
gene expression patterns vary across different regions of the mouse
brain, unlocking insights into the functional specialization of these
regions and the intricate networks that underpin neural processes.

\hypertarget{sec-discussion}{%
\section{Discussion}\label{sec-discussion}}

Our research introduces a comprehensive framework that leverages tours
for interactive exploration of high-dimensional data coupled with a
low-dimensional manifold, facilitated by the \texttt{quollr} R package.
Regardless of the Non-Linear Dimension Reduction (NLDR) technique in
use, our approach demonstrates effectiveness through simulation
examples, particularly in the iterative removal of long edges for a
smoother representation and capturing cluster variance.

In the example with doublets, our method successfully captures the tweak
within each cluster, indicating the variance present within them.
However, the model may not appear smooth in high-dimensional space due
to considerable noise when the data has a piecewise linear geometry,
such as the tree simulation.

The practical application of our framework, as showcased with the UMAP
view, enables visual inspection of well-separated clusters. Furthermore,
the combined tour and model provide a robust assessment of whether UMAP
preserves the data structure and accurately transforms the data.

The advantages of our approach include its versatility across various
NLDR techniques and the ability to generate interactive visualizations
for detailed exploration. The tour provides an intuitive way to navigate
and comprehend high-dimensional data while assessing the accuracy of
dimensionality reduction.

However, one limitation is that the approach may be less effective in
cases with significant noise, as seen in the tree simulation example.
Additionally, while our method aids in visual verification, quantifying
the accuracy of embeddings might require further evaluation metrics.

In conclusion, our framework presents a powerful tool for researchers
and analysts in single-cell studies to assess their embeddings by
visually inspecting them alongside the original data. By leveraging the
advantages of tours and low-dimensional manifolds, our approach offers
valuable insights into the data transformation process, empowering users
to make informed decisions in analyzing high-dimensional data. Future
work could enhance the method's robustness in the presence of noise and
explore additional evaluation metrics for quantifying embedding
accuracy.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{bibliography.bib}

\newpage{}




\end{document}
